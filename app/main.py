"""
LINE Plant Disease Detection Bot with Google Gemini Vision and Supabase RAG
Production-grade FastAPI implementation with Multi-Agent System
"""

import os
import logging
import time
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from fastapi import FastAPI, Request, HTTPException, Header
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import httpx
import base64
import hashlib
import hmac
import json
from dotenv import load_dotenv
from supabase import create_client, Client
import google.generativeai as genai
from PIL import Image
import io
from sentence_transformers import SentenceTransformer
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="LINE Plant Disease Detection Bot",
    description="AI-powered plant disease detection with Multi-Agent System",
    version="1.0.0"
)

# Initialize Rate Limiter
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# ============================================================================#
# ENVIRONMENT / SERVICES
# ============================================================================#
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

required_env_vars = {
    "LINE_CHANNEL_ACCESS_TOKEN": LINE_CHANNEL_ACCESS_TOKEN,
    "LINE_CHANNEL_SECRET": LINE_CHANNEL_SECRET,
    "GEMINI_API_KEY": GEMINI_API_KEY,
    "SUPABASE_URL": SUPABASE_URL,
    "SUPABASE_KEY": SUPABASE_KEY,
}
for var_name, var_value in required_env_vars.items():
    if not var_value:
        logger.error(f"Missing required environment variable: {var_name}")

# Initialize Gemini (for Vision)
gemini_model = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')
    logger.info("Gemini initialized successfully (gemini-2.5-flash)")

# Initialize E5 model for embeddings (768 dimensions)
e5_model = None
try:
    e5_model = SentenceTransformer('intfloat/multilingual-e5-base')
    logger.info("E5 model initialized successfully (768 dimensions)")
except Exception as e:
    logger.warning(f"E5 model initialization failed: {e}")

# Initialize Supabase (fallback)
supabase_client: Client = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)
        logger.info("Supabase initialized successfully (fallback)")
    except Exception as e:
        logger.error(f"Failed to initialize Supabase: {e}")

# Using Supabase Vector Search with Gemini filtering
logger.info("Using Supabase Vector Search + Gemini Filtering")

# ============================================================================#
# Memory System (Supabase-based) + Caching + Cleanup
# ============================================================================#
# In-memory store for pending image contexts awaiting user symptom input
# Keyed by user_id -> dict with image_bytes and reply_token (optional)
pending_image_contexts: Dict[str, Dict[str, Any]] = {}

# Cache configuration
CACHE_TTL = 3600  # 1 hour
PENDING_CONTEXT_TTL = 300  # 5 minutes
MAX_CACHE_SIZE = 1000  # Maximum cache entries

# Detection cache (image hash -> result)
detection_cache: Dict[str, Dict[str, Any]] = {}

# Product recommendation cache (disease_name -> products)
product_cache: Dict[str, Dict[str, Any]] = {}

# Knowledge cache (query -> knowledge)
knowledge_cache: Dict[str, Dict[str, Any]] = {}

# Rate limiting per user
user_request_counts: Dict[str, List[float]] = {}
USER_RATE_LIMIT = 10  # requests per minute
USER_RATE_WINDOW = 60  # seconds

# Memory configuration
MAX_MEMORY_MESSAGES = 20 # Keep last 20messages for context
MEMORY_CONTEXT_WINDOW = 10 # Use last 10messages for context

# ============================================================================#
# Cache Helper Functions
# ============================================================================#
def get_image_hash(image_bytes: bytes) -> str:
    """Generate hash for image caching"""
    return hashlib.md5(image_bytes).hexdigest()

def get_cache_key(prefix: str, key: str) -> str:
    """Generate cache key with prefix"""
    return f"{prefix}:{key}"

async def get_from_cache(cache_dict: dict, key: str) -> Optional[Any]:
    """Get item from cache if not expired"""
    if key not in cache_dict:
        return None
    
    entry = cache_dict[key]
    if time.time() - entry.get("timestamp", 0) > CACHE_TTL:
        # Expired, remove it
        del cache_dict[key]
        return None
    
    logger.info(f"‚úì Cache hit: {key[:50]}")
    return entry.get("data")

async def set_to_cache(cache_dict: dict, key: str, data: Any):
    """Set item to cache with timestamp"""
    # Check cache size limit
    if len(cache_dict) >= MAX_CACHE_SIZE:
        # Remove oldest entries (simple FIFO)
        oldest_keys = sorted(cache_dict.keys(), key=lambda k: cache_dict[k].get("timestamp", 0))[:100]
        for old_key in oldest_keys:
            del cache_dict[old_key]
        logger.info(f"Cache cleanup: removed {len(oldest_keys)} old entries")
    
    cache_dict[key] = {
        "data": data,
        "timestamp": time.time()
    }
    logger.info(f"‚úì Cache set: {key[:50]}")

async def cleanup_expired_cache():
    """Clean up expired cache entries"""
    current_time = time.time()
    
    # Cleanup detection cache
    expired_keys = [k for k, v in detection_cache.items() if current_time - v.get("timestamp", 0) > CACHE_TTL]
    for key in expired_keys:
        del detection_cache[key]
    
    # Cleanup product cache
    expired_keys = [k for k, v in product_cache.items() if current_time - v.get("timestamp", 0) > CACHE_TTL]
    for key in expired_keys:
        del product_cache[key]
    
    # Cleanup knowledge cache
    expired_keys = [k for k, v in knowledge_cache.items() if current_time - v.get("timestamp", 0) > CACHE_TTL]
    for key in expired_keys:
        del knowledge_cache[key]
    
    # Cleanup pending contexts
    expired_users = [
        user_id for user_id, ctx in pending_image_contexts.items()
        if current_time - ctx.get("timestamp", 0) > PENDING_CONTEXT_TTL
    ]
    for user_id in expired_users:
        del pending_image_contexts[user_id]
    
    if expired_keys or expired_users:
        logger.info(f"Cache cleanup: removed {len(expired_keys)} cache entries, {len(expired_users)} pending contexts")

async def get_cache_stats() -> dict:
    """Get cache statistics"""
    return {
        "detection_cache_size": len(detection_cache),
        "product_cache_size": len(product_cache),
        "knowledge_cache_size": len(knowledge_cache),
        "pending_contexts": len(pending_image_contexts),
        "total_memory_items": len(detection_cache) + len(product_cache) + len(knowledge_cache) + len(pending_image_contexts)
    }

# ============================================================================#
# Rate Limiting Helper Functions
# ============================================================================#
async def check_user_rate_limit(user_id: str) -> bool:
    """Check if user exceeded rate limit"""
    current_time = time.time()
    
    # Initialize user if not exists
    if user_id not in user_request_counts:
        user_request_counts[user_id] = []
    
    # Remove old timestamps outside the window
    user_request_counts[user_id] = [
        ts for ts in user_request_counts[user_id]
        if current_time - ts < USER_RATE_WINDOW
    ]
    
    # Check if exceeded limit
    if len(user_request_counts[user_id]) >= USER_RATE_LIMIT:
        logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
        return False
    
    # Add current request
    user_request_counts[user_id].append(current_time)
    return True

async def cleanup_rate_limit_data():
    """Clean up old rate limit data"""
    current_time = time.time()
    users_to_remove = []
    
    for user_id, timestamps in user_request_counts.items():
        # Remove old timestamps
        user_request_counts[user_id] = [
            ts for ts in timestamps
            if current_time - ts < USER_RATE_WINDOW
        ]
        
        # If no recent requests, remove user
        if not user_request_counts[user_id]:
            users_to_remove.append(user_id)
    
    for user_id in users_to_remove:
        del user_request_counts[user_id]
    
    if users_to_remove:
        logger.info(f"Rate limit cleanup: removed {len(users_to_remove)} inactive users")

# ============================================================================#
# Background Tasks
# ============================================================================#
async def periodic_cleanup():
    """Run periodic cleanup tasks"""
    while True:
        try:
            await asyncio.sleep(300)  # Run every 5 minutes
            logger.info("Running periodic cleanup...")
            await cleanup_expired_cache()
            await cleanup_rate_limit_data()
            
            # Log stats
            stats = await get_cache_stats()
            logger.info(f"Cache stats: {stats}")
            
        except Exception as e:
            logger.error(f"Error in periodic cleanup: {e}")

@app.on_event("startup")
async def startup_event():
    """Start background tasks on startup"""
    logger.info("Starting background tasks...")
    asyncio.create_task(periodic_cleanup())

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("Shutting down gracefully...")
    # Clear all caches
    detection_cache.clear()
    product_cache.clear()
    knowledge_cache.clear()
    pending_image_contexts.clear()
    user_request_counts.clear()
    logger.info("All caches cleared")

async def add_to_memory(user_id: str, role: str, content: str, metadata: dict = None):
    """Add message to conversation memory in Supabase"""
    try:
        if not supabase_client:
            logger.warning("Supabase not available, skipping memory storage")
            return
        
        # Truncate very long messages
        truncated_content = content[:2000] if len(content) > 2000 else content
        
        data = {
            "user_id": user_id,
            "role": role,  # "user" or "assistant"
            "content": truncated_content,
            "metadata": metadata or {}
        }
        
        result = supabase_client.table('conversation_memory').insert(data).execute()
        logger.info(f"‚úì Added to memory: {role} message for user {user_id[:8]}...")
        
        # Clean up old messages (keep last N per user)
        await cleanup_old_memory(user_id)
        
    except Exception as e:
        logger.error(f"Failed to add to memory: {e}")

async def get_conversation_context(user_id: str, limit: int = MEMORY_CONTEXT_WINDOW) -> str:
    """Get conversation history as context string from Supabase"""
    try:
        if not supabase_client:
            return ""
        
        # Get last N messages for this user
        result = supabase_client.table('conversation_memory')\
            .select('role, content, created_at')\
            .eq('user_id', user_id)\
            .order('created_at', desc=True)\
            .limit(limit)\
            .execute()
        
        if not result.data:
            return ""
        
        # Reverse to get chronological order
        messages = list(reversed(result.data))
        
        context_parts = []
        for msg in messages:
            role = "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ" if msg["role"] == "user" else "‡∏â‡∏±‡∏ô"
            content = msg["content"][:150]  # Truncate for context
            context_parts.append(f"{role}: {content}")
        
        logger.info(f"‚úì Retrieved {len(messages)} messages from memory")
        return "\n".join(context_parts)
        
    except Exception as e:
        logger.error(f"Failed to get conversation context: {e}")
        return ""

async def cleanup_old_memory(user_id: str):
    """Keep only last N messages per user"""
    try:
        if not supabase_client:
            return
        
        # Get all message IDs for this user, ordered by created_at desc
        result = supabase_client.table('conversation_memory')\
            .select('id')\
            .eq('user_id', user_id)\
            .order('created_at', desc=True)\
            .execute()
        
        if not result.data or len(result.data) <= MAX_MEMORY_MESSAGES:
            return
        
        # Get IDs to delete (keep only last MAX_MEMORY_MESSAGES)
        ids_to_keep = [msg['id'] for msg in result.data[:MAX_MEMORY_MESSAGES]]
        ids_to_delete = [msg['id'] for msg in result.data[MAX_MEMORY_MESSAGES:]]
        
        if ids_to_delete:
            # Delete old messages
            supabase_client.table('conversation_memory')\
                .delete()\
                .in_('id', ids_to_delete)\
                .execute()
            logger.info(f"‚úì Cleaned up {len(ids_to_delete)} old messages for user {user_id[:8]}...")
            
    except Exception as e:
        logger.error(f"Failed to cleanup old memory: {e}")

async def clear_memory(user_id: str):
    """Clear all conversation memory for user"""
    try:
        if not supabase_client:
            logger.warning("Supabase not available")
            return
        
        result = supabase_client.table('conversation_memory')\
            .delete()\
            .eq('user_id', user_id)\
            .execute()
        
        logger.info(f"‚úì Cleared memory for user {user_id[:8]}...")
        
    except Exception as e:
        logger.error(f"Failed to clear memory: {e}")

async def get_memory_stats(user_id: str) -> dict:
    """Get memory statistics for user"""
    try:
        if not supabase_client:
            return {"total": 0, "user_messages": 0, "assistant_messages": 0}
        
        result = supabase_client.table('conversation_memory')\
            .select('role')\
            .eq('user_id', user_id)\
            .execute()
        
        if not result.data:
            return {"total": 0, "user_messages": 0, "assistant_messages": 0}
        
        user_count = sum(1 for msg in result.data if msg['role'] == 'user')
        assistant_count = sum(1 for msg in result.data if msg['role'] == 'assistant')
        
        return {
            "total": len(result.data),
            "user_messages": user_count,
            "assistant_messages": assistant_count
        }
        
    except Exception as e:
        logger.error(f"Failed to get memory stats: {e}")
        return {"total": 0, "user_messages": 0, "assistant_messages": 0}

# ============================================================================#
# Pydantic Models
# ============================================================================#
class DiseaseDetectionResult(BaseModel):
    disease_name: str
    confidence: str
    symptoms: str
    severity: str
    raw_analysis: str

class ProductRecommendation(BaseModel):
    product_name: str
    active_ingredient: Optional[str] = ""
    target_pest: Optional[str] = ""
    applicable_crops: Optional[str] = ""
    how_to_use: Optional[str] = ""
    usage_period: Optional[str] = ""
    usage_rate: Optional[str] = ""
    score: float = 0.0

# ============================================================================#
# Helpers
# ============================================================================#
def post_process_answer(answer: str) -> str:
    """Post-process Gemini answer for better quality"""
    if not answer:
        return ""
    
    import re
    
    # 1. Remove markdown formatting
    answer = answer.replace("```", "")
    answer = answer.replace("**", "")
    answer = answer.replace("##", "")
    answer = answer.replace("###", "")
    answer = re.sub(r'\*\*([^*]+)\*\*', r'\1', answer)  # **text** ‚Üí text
    answer = re.sub(r'\*([^*]+)\*', r'\1', answer)  # *text* ‚Üí text
    
    # 2. Fix Thai encoding issues
    answer = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥‡∏¥‡∏µ‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πà‡πâ‡πä‡πã])', r'\1\2', answer)
    answer = answer.replace('ƒû', '')
    answer = answer.replace('ÔøΩ', '')
    answer = answer.replace('\x00', '')
    
    # 3. Fix spacing issues
    answer = re.sub(r'\s+', ' ', answer)  # Multiple spaces ‚Üí single space
    answer = answer.replace(' ,', ',')
    answer = answer.replace(' .', '.')
    answer = answer.replace(' :', ':')
    answer = answer.replace('( ', '(')
    answer = answer.replace(' )', ')')
    
    # 4. Fix bullet points (convert markdown to Thai style)
    answer = re.sub(r'^\s*[-*]\s+', '‚Ä¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'\n\s*[-*]\s+', '\n‚Ä¢ ', answer)
    
    # 5. Ensure proper line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)  # Max 2 line breaks
    
    # 6. Remove leading/trailing whitespace
    answer = answer.strip()
    
    # 7. Fix common Thai typos
    answer = answer.replace('‡∏ï‡πâ', '‡∏ï‡πâ')
    answer = answer.replace('‡∏ï', '‡∏ï')
    
    # 8. Ensure emoji spacing
    answer = re.sub(r'([üå±üêõüçÑüíä‚ö†Ô∏è‚úÖüìöüí°üéØüìãüîç])([‡∏Å-‡πôA-Za-z])', r'\1 \2', answer)
    
    return answer

def clean_knowledge_text(text: str) -> str:
    """Clean and format knowledge text for better readability"""
    if not text:
        return ""
    
    import re
    
    # Fix encoding issues - remove corrupted characters
    # Common patterns: ‡∏àƒû‡∏≥, ‡∏•ƒû‡∏≥, ‡∏óƒû‡∏≥, ‡∏ôƒû‡πâ‡∏≥, ‡∏Åƒû‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥])', r'\1\2', text)  # ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏ôƒû‡πâ ‚Üí ‡∏ô‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏¥])', r'\1\2', text)  # ‡∏Åƒû‡∏¥ ‚Üí ‡∏Å‡∏¥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏µ])', r'\1\2', text)  # ‡∏Åƒû‡∏µ ‚Üí ‡∏Å‡∏µ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏∏])', r'\1\2', text)  # ‡∏Åƒû‡∏∏ ‚Üí ‡∏Å‡∏∏
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏π])', r'\1\2', text)  # ‡∏Åƒû‡∏π ‚Üí ‡∏Å‡∏π
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πà])', r'\1\2', text)  # ‡∏Åƒû‡πà ‚Üí ‡∏Å‡πà
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏Åƒû‡πâ ‚Üí ‡∏Å‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πä])', r'\1\2', text)  # ‡∏Åƒû‡πä ‚Üí ‡∏Å‡πä
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πã])', r'\1\2', text)  # ‡∏Åƒû‡πã ‚Üí ‡∏Å‡πã
    text = re.sub(r'ƒû', '', text)  # Remove remaining ƒû
    
    # Fix other corrupted characters
    text = text.replace('‡∏ï‡πâ', '‡∏ï‡πâ')  # Fix tone marks
    text = text.replace('‡∏ï', '‡∏ï')
    text = text.replace('ÔøΩ', '')  # Remove replacement character
    text = text.replace('\x00', '')  # Remove null character
    
    # Fix common Thai encoding issues
    text = text.replace('√†¬∏', '')  # Remove Thai encoding prefix
    text = text.replace('√†¬π', '')  # Remove Thai encoding prefix
    
    # Remove excessive whitespace
    text = ' '.join(text.split())
    
    # Fix common issues
    text = text.replace('  ', ' ')  # Double spaces
    text = text.replace(' ,', ',')  # Space before comma
    text = text.replace(' .', '.')  # Space before period
    text = text.replace('( ', '(')  # Space after opening parenthesis
    text = text.replace(' )', ')')  # Space before closing parenthesis
    text = text.replace(' :', ':')  # Space before colon
    
    # Fix Thai-specific issues (keep important marks)
    # text = text.replace('‡∏∫', '')  # Keep Thai character above
    # text = text.replace('‡πå', '')  # Keep Thai character above
    
    # Remove multiple consecutive spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence spacing
    text = re.sub(r'([.!?])\s*([A-Za-z‡∏Å-‡πô])', r'\1 \2', text)
    
    # Remove leading/trailing whitespace
    text = text.strip()
    
    # Remove lines with only special characters
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if line.strip() and not re.match(r'^[^\w\s]+$', line.strip())]
    text = '\n'.join(cleaned_lines)
    
    return text

def verify_line_signature(body: bytes, signature: str) -> bool:
    if not LINE_CHANNEL_SECRET:
        logger.warning("LINE_CHANNEL_SECRET not set, skipping signature verification")
        return True
    hash_digest = hmac.new(
        LINE_CHANNEL_SECRET.encode('utf-8'),
        body,
        hashlib.sha256
    ).digest()
    expected_signature = base64.b64encode(hash_digest).decode('utf-8')
    return hmac.compare_digest(signature, expected_signature)

async def get_image_content_from_line(message_id: str) -> bytes:
    url = f"https://api-data.line.me/v2/bot/message/{message_id}/content"
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"}
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.get(url, headers=headers)
        response.raise_for_status()
        return response.content



# ============================================================================#
# Core: Detect disease (Gemini Vision)
# ============================================================================#
async def detect_disease(image_bytes: bytes, extra_user_info: Optional[str] = None) -> DiseaseDetectionResult:
    logger.info("Starting pest/disease detection with Gemini Vision")
    
    # Check cache first (only if no extra user info)
    if not extra_user_info:
        image_hash = get_image_hash(image_bytes)
        cached_result = await get_from_cache(detection_cache, image_hash)
        if cached_result:
            logger.info("‚úì Using cached detection result")
            return DiseaseDetectionResult(**cached_result)
    
    try:
        # Convert bytes to PIL Image for Gemini
        image = Image.open(io.BytesIO(image_bytes))
        
        prompt = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡πÑ‡∏ó‡∏¢ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå 20 ‡∏õ‡∏µ

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

üìã **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå**:
1. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ö‡∏ô‡πÉ‡∏ö/‡∏•‡∏≥‡∏ï‡πâ‡∏ô/‡∏ú‡∏• ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏µ ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
3. ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏°‡∏•‡∏á ‡πÑ‡∏Ç‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä
4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢

üîç **‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó**:
- **‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤ (Fungus)**: ‡∏à‡∏∏‡∏î‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•/‡∏î‡∏≥, ‡πÅ‡∏ú‡∏•‡πÄ‡∏õ‡∏µ‡∏¢‡∏Å, ‡∏£‡∏≤‡∏Ç‡∏≤‡∏ß, ‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ
- **‡πÑ‡∏ß‡∏£‡∏±‡∏™ (Virus)**: ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á, ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å, ‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏ö‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡πÅ‡∏Ñ‡∏£‡∏∞‡πÅ‡∏Å‡∏£‡πá‡∏ô
- **‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä (Pest)**: ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á, ‡∏£‡∏≠‡∏¢‡∏Å‡∏±‡∏î, ‡πÉ‡∏ö‡∏°‡πâ‡∏ß‡∏ô, ‡∏°‡∏µ‡πÄ‡∏¢‡∏∑‡πà‡∏≠‡πÉ‡∏¢
- **‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä (Weed)**: ‡∏û‡∏∑‡∏ä‡πÅ‡∏õ‡∏•‡∏Å‡∏õ‡∏•‡∏≠‡∏°‡πÉ‡∏ô‡πÅ‡∏õ‡∏•‡∏á

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏†‡∏≤‡∏û
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏ confidence ‡∏ï‡πà‡∏≥
- ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"

üì§ **‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):

{
  "disease_name": "‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü, ‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™, ‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á",
  "pest_type": "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤/‡πÑ‡∏ß‡∏£‡∏±‡∏™/‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä/‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä",
  "confidence_level_percent": 0-100,
  "confidence": "‡∏™‡∏π‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏ï‡πà‡∏≥",
  "symptoms_in_image": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏û (‡∏™‡∏±‡πâ‡∏ô‡πÜ)",
  "symptoms": "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏ß‡∏°‡∏™‡∏µ ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡∏Ç‡∏ô‡∏≤‡∏î",
  "possible_cause": "‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á",
  "severity_level": "‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢",
  "severity": "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•",
  "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô",
  "affected_area": "‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö",
  "spread_risk": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (‡∏™‡∏π‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏ï‡πà‡∏≥)"
}

‚úÖ ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤: disease_name = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤", confidence = "‡∏™‡∏π‡∏á" """

        # If user provided extra observation text, include it as additional context
        if extra_user_info:
            prompt += f"\n\n‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {extra_user_info}"

        # Call Gemini with image
        response = gemini_model.generate_content([prompt, image])
        raw_text = response.text
        logger.info(f"Gemini raw response: {raw_text}")

        # Extract JSON flexibly
        try:
            json_str = raw_text.strip()
            if json_str.startswith("```"):
                # remove code fences and find JSON part
                parts = json_str.split("```")
                for p in parts:
                    p_s = p.strip()
                    if p_s.startswith("{") and p_s.endswith("}"):
                        json_str = p_s
                        break
            # find first { ... } block if extra text present
            if "{" in json_str and "}" in json_str:
                start = json_str.find("{")
                end = json_str.rfind("}") + 1
                json_str = json_str[start:end]
            data = json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to parse JSON from Gemini response: {e}", exc_info=True)
            data = {"disease_name": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ", "confidence": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "symptoms": "", "severity": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "description": raw_text}

        # Map many possible keys to canonical fields
        disease_name = data.get("disease_name") or data.get("disease") or data.get("‡πÇ‡∏£‡∏Ñ") or "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ"
        # confidence prefer numeric percent if provided
        confidence = ""
        if "confidence_level_percent" in data:
            confidence = str(data.get("confidence_level_percent"))
        elif "confidence" in data:
            confidence = str(data.get("confidence"))
        elif "confidence_percent" in data:
            confidence = str(data.get("confidence_percent"))
        else:
            confidence = "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"
        # symptoms
        symptoms = data.get("symptoms_in_image") or data.get("symptoms") or data.get("‡∏≠‡∏≤‡∏Å‡∏≤‡∏£") or ""
        # severity
        severity = data.get("severity_level") or data.get("severity") or data.get("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á") or "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"
        # description / raw
        description = data.get("description") or data.get("possible_cause") or raw_text

        # Extract pest_type
        pest_type = data.get("pest_type") or "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä"
        
        # Extract additional fields for better analysis
        affected_area = data.get("affected_area") or ""
        spread_risk = data.get("spread_risk") or ""
        
        # Build comprehensive raw_analysis
        raw_parts = [f"{pest_type}: {description}"]
        if affected_area:
            raw_parts.append(f"‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö: {affected_area}")
        if spread_risk:
            raw_parts.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà: {spread_risk}")
        
        result = DiseaseDetectionResult(
            disease_name=str(disease_name),
            confidence=str(confidence),
            symptoms=str(symptoms),
            severity=str(severity),
            raw_analysis=" | ".join(raw_parts)
        )
        
        # Check confidence level and warn if low
        confidence_num = 0
        try:
            if confidence.replace("%", "").replace("‡∏™‡∏π‡∏á", "90").replace("‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "60").replace("‡∏ï‡πà‡∏≥", "30").isdigit():
                confidence_num = int(confidence.replace("%", "").replace("‡∏™‡∏π‡∏á", "90").replace("‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "60").replace("‡∏ï‡πà‡∏≥", "30"))
        except:
            pass
        
        if confidence_num < 50 or "‡∏ï‡πà‡∏≥" in confidence:
            logger.warning(f"Low confidence detection: {result.disease_name} ({confidence})")
        
        logger.info(f"Pest/Disease detected: {result.disease_name} (Type: {pest_type}, Confidence: {confidence})")
        
        # Cache the result (only if no extra user info)
        if not extra_user_info:
            image_hash = get_image_hash(image_bytes)
            await set_to_cache(detection_cache, image_hash, result.dict())
        
        # Log detection for analysis (optional - can be used to improve accuracy)
        try:
            import datetime
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "disease_name": result.disease_name,
                "pest_type": pest_type,
                "confidence": confidence,
                "severity": result.severity,
                "has_user_input": bool(extra_user_info)
            }
            # Could save to file or database for later analysis
            logger.debug(f"Detection log: {log_entry}")
        except Exception as e:
            logger.warning(f"Failed to log detection: {e}")
        
        return result

    except Exception as e:
        logger.error(f"Error in pest/disease detection: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Detection failed: {str(e)}")

# ============================================================================#
# Core: Retrieve product recommendations (Supabase Vector Search)
# ============================================================================#
async def retrieve_product_recommendation(disease_info: DiseaseDetectionResult) -> List[ProductRecommendation]:
    """
    Query products using Vector Search + Gemini filtering
    Returns top 3-5 most relevant products
    """
    try:
        logger.info("üîç Retrieving products with Vector Search + Gemini Filter")

        if not supabase_client:
            logger.warning("Supabase not configured")
            return []

        disease_name = disease_info.disease_name
        logger.info(f"üìù Searching products for: {disease_name}")
        
        # Check cache first
        cache_key = f"products:{disease_name}"
        cached_products = await get_from_cache(product_cache, cache_key)
        if cached_products:
            logger.info("‚úì Using cached product recommendations")
            return [ProductRecommendation(**p) for p in cached_products]
        
        # Strategy 1: Vector search by disease name (most accurate)
        try:
            if e5_model:
                # Generate embedding for disease name
                query_text = f"query: {disease_name}"
                query_embedding = e5_model.encode(query_text, normalize_embeddings=True).tolist()
                logger.info("‚úì Product query embedding generated")
                
                # Vector search in products table
                result = supabase_client.rpc(
                    'match_products',
                    {
                        'query_embedding': query_embedding,
                        'match_threshold': 0.3,  # Lower threshold for more candidates
                        'match_count': 15  # Get more candidates for Gemini filtering
                    }
                ).execute()
                
                if result.data and len(result.data) > 0:
                    logger.info(f"‚úì Found {len(result.data)} product candidates via vector search")
                    
                    # Use Gemini to filter and rank products
                    filtered_products = await filter_products_with_gemini(
                        disease_name,
                        disease_info.raw_analysis,
                        result.data
                    )
                    
                    if filtered_products:
                        logger.info(f"‚úì Gemini filtered {len(filtered_products)} relevant products")
                        return filtered_products
                    else:
                        logger.warning("‚ö†Ô∏è Gemini filtering returned no products, using top vector results")
                        # Fallback: use top vector search results
                        return build_recommendations_from_data(result.data[:6])
                else:
                    logger.info("No products found via vector search, trying keyword search")
            else:
                logger.warning("E5 model not available, using keyword search")
        except Exception as e:
            logger.warning(f"Vector search failed: {e}, trying keyword search")
        
        # Strategy 2: Keyword search fallback
        matches_data = []
        
        # Search in target_pest field
        try:
            result = supabase_client.table('products')\
                .select('*')\
                .ilike('target_pest', f'%{disease_name}%')\
                .limit(10)\
                .execute()
            
            if result.data:
                matches_data.extend(result.data)
                logger.info(f"Found {len(result.data)} products in target_pest")
        except Exception as e:
            logger.warning(f"target_pest search failed: {e}")
        
        # If no results, search by pest type
        if not matches_data:
            try:
                pest_keywords = []
                if "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä"]
                elif "‡πÑ‡∏ß‡∏£‡∏±‡∏™" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÑ‡∏ß‡∏£‡∏±‡∏™"]
                elif "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä" in disease_info.raw_analysis or "‡πÅ‡∏°‡∏•‡∏á" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÅ‡∏°‡∏•‡∏á", "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢"]
                elif "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä" in disease_info.raw_analysis:
                    pest_keywords = ["‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤"]
                
                for keyword in pest_keywords:
                    result = supabase_client.table('products')\
                        .select('*')\
                        .ilike('target_pest', f'%{keyword}%')\
                        .limit(5)\
                        .execute()
                    
                    if result.data:
                        matches_data.extend(result.data)
                        logger.info(f"Found {len(result.data)} products for keyword: {keyword}")
                        break
                        
            except Exception as e:
                logger.warning(f"Keyword search failed: {e}")
        
        if not matches_data:
            logger.warning("No products found with any search strategy")
            return []
        
        logger.info(f"Total products found: {len(matches_data)}")
        recommendations = build_recommendations_from_data(matches_data[:6])
        
        # Cache the results
        if recommendations:
            await set_to_cache(product_cache, cache_key, [r.dict() for r in recommendations])
        
        return recommendations

    except Exception as e:
        logger.error(f"Product search failed: {e}", exc_info=True)
        return []

async def filter_products_with_gemini(disease_name: str, raw_analysis: str, product_candidates: List[Dict]) -> List[ProductRecommendation]:
    """Use Gemini to filter and rank the most relevant products"""
    try:
        if not gemini_model:
            return []
        
        # Build product list for Gemini
        products_text = ""
        for idx, p in enumerate(product_candidates[:10], 1):  # Top 10 candidates
            products_text += f"\n[{idx}] {p.get('product_name', 'N/A')}\n"
            products_text += f"   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient', 'N/A')}\n"
            products_text += f"   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {p.get('target_pest', 'N/A')[:100]}\n"
            products_text += f"   Similarity: {p.get('similarity', 0):.0%}\n"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "{disease_name}"

üìä **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏£‡∏Ñ**:
{raw_analysis}

üì¶ **‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö**:
{products_text}

üìã **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**:
1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏£‡∏Ñ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
3. ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
4. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡∏Ç‡∏≠‡∏á product index ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‚ö†Ô∏è **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å**:
- ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
- ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏£‡∏Ñ (‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤/‡πÅ‡∏°‡∏•‡∏á/‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä)
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):
[1, 3, 5]"""

        response = gemini_model.generate_content(prompt)
        result_text = response.text.strip()
        
        # Parse JSON response
        try:
            # Extract JSON array
            if "[" in result_text and "]" in result_text:
                start = result_text.find("[")
                end = result_text.rfind("]") + 1
                json_str = result_text[start:end]
                selected_indices = json.loads(json_str)
                
                if selected_indices and len(selected_indices) > 0:
                    # Build recommendations from selected products
                    recommendations = []
                    for idx in selected_indices[:5]:  # Max 5
                        if 1 <= idx <= len(product_candidates):
                            product = product_candidates[idx - 1]
                            rec = ProductRecommendation(
                                product_name=product.get('product_name', 'N/A'),
                                active_ingredient=product.get('active_ingredient', ''),
                                target_pest=product.get('target_pest', ''),
                                applicable_crops=product.get('applicable_crops', ''),
                                how_to_use=product.get('how_to_use', ''),
                                usage_period=product.get('usage_period', ''),
                                usage_rate=product.get('usage_rate', ''),
                                score=product.get('similarity', 0.8)
                            )
                            recommendations.append(rec)
                    
                    logger.info(f"‚úì Gemini selected {len(recommendations)} products")
                    return recommendations
                else:
                    logger.warning("Gemini returned empty selection")
                    return []
            else:
                logger.warning(f"Invalid Gemini response format: {result_text[:100]}")
                return []
                
        except Exception as e:
            logger.error(f"Failed to parse Gemini response: {e}")
            return []
        
    except Exception as e:
        logger.error(f"Gemini product filtering failed: {e}")
        return []

def build_recommendations_from_data(products_data: List[Dict]) -> List[ProductRecommendation]:
    """Build ProductRecommendation list from raw data"""
    recommendations = []
    seen_products = set()
    
    for product in products_data:
        pname = product.get("product_name", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠")
        
        if pname in seen_products:
            continue
        seen_products.add(pname)
        
        pest = product.get("target_pest", "")
        if not pest or pest.strip() == "":
            continue
        
        rec = ProductRecommendation(
            product_name=pname,
            active_ingredient=product.get("active_ingredient", ""),
            target_pest=pest,
            applicable_crops=product.get("applicable_crops", ""),
            how_to_use=product.get("how_to_use", ""),
            usage_period=product.get("usage_period", ""),
            usage_rate=product.get("usage_rate", ""),
            score=product.get("similarity", 0.7)
        )
        recommendations.append(rec)
    
    return recommendations



# ============================================================================#
# Core: Intent-based Product Recommendation (NEW)
# ============================================================================#
async def recommend_products_by_intent(question: str, keywords: dict) -> str:
    """‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ï‡∏≤‡∏° intent ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï, ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤, ‡∏Ø‡∏•‡∏Ø)"""
    try:
        intent = keywords.get('intent')
        logger.info(f"üéØ Intent-based recommendation: {intent}")
        logger.info(f"üìù Keywords: crops={keywords.get('crops')}, pests={keywords.get('pests')}")
        
        if not supabase_client:
            logger.error("‚ùå Supabase client not available")
            return await answer_product_question(question, keywords)
        
        if not e5_model:
            logger.error("‚ùå E5 model not available")
            return await answer_product_question(question, keywords)
        
        intent = keywords.get("intent")
        crops = keywords.get("crops", [])
        pests = keywords.get("pests", [])
        
        # Build search query based on intent
        search_queries = []
        
        if intent == "increase_yield":
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï
            if crops:
                for crop in crops[:2]:
                    search_queries.append(f"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï {crop}")
                    search_queries.append(f"‡∏õ‡∏∏‡πã‡∏¢‡∏ö‡∏≥‡∏£‡∏∏‡∏á {crop}")
                    search_queries.append(f"‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô {crop}")
            else:
                search_queries.append("‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï ‡∏õ‡∏∏‡πã‡∏¢ ‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô")
        
        elif intent == "solve_problem":
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä
            if pests and crops:
                for pest in pests[:2]:
                    for crop in crops[:2]:
                        search_queries.append(f"‡∏Å‡∏≥‡∏à‡∏±‡∏î {pest} {crop}")
            elif pests:
                for pest in pests[:2]:
                    search_queries.append(f"‡∏Å‡∏≥‡∏à‡∏±‡∏î {pest}")
            elif crops:
                for crop in crops[:2]:
                    search_queries.append(f"‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ {crop}")
        
        elif intent == "general_care":
            # ‡∏î‡∏π‡πÅ‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            if crops:
                for crop in crops[:2]:
                    search_queries.append(f"‡∏î‡∏π‡πÅ‡∏• {crop}")
                    search_queries.append(f"‡∏ö‡∏≥‡∏£‡∏∏‡∏á {crop}")
        
        else:
            # Default: product inquiry
            if crops:
                search_queries.append(f"‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå {crops[0]}")
            if pests:
                search_queries.append(f"‡∏Å‡∏≥‡∏à‡∏±‡∏î {pests[0]}")
        
        # Vector search for each query
        all_products = []
        logger.info(f"üîç Searching with {len(search_queries)} queries: {search_queries[:3]}")
        
        for query in search_queries[:3]:  # Top 3 queries
            try:
                logger.info(f"   ‚Üí Query: '{query}'")
                query_embedding = e5_model.encode(f"query: {query}", normalize_embeddings=True).tolist()
                
                result = supabase_client.rpc(
                    'match_products',
                    {
                        'query_embedding': query_embedding,
                        'match_threshold': 0.25,  # Lower threshold for more results
                        'match_count': 10
                    }
                ).execute()
                
                if result.data:
                    all_products.extend(result.data)
                    logger.info(f"   ‚úì Found {len(result.data)} products")
                else:
                    logger.warning(f"   ‚ö†Ô∏è No products found")
            except Exception as e:
                logger.error(f"   ‚ùå Vector search failed: {e}", exc_info=True)
        
        # Remove duplicates
        seen = set()
        unique_products = []
        for p in all_products:
            pname = p.get('product_name', '')
            if pname and pname not in seen:
                seen.add(pname)
                unique_products.append(p)
        
        logger.info(f"üì¶ Total products: {len(all_products)}, Unique: {len(unique_products)}")
        
        if not unique_products:
            # Fallback to keyword search
            logger.warning("‚ö†Ô∏è No products from vector search, trying keyword search")
            return await answer_product_question(question, keywords)
        
        # Log product names
        product_names = [p.get('product_name', 'N/A') for p in unique_products[:5]]
        logger.info(f"üìã Top products: {', '.join(product_names)}")
        
        # Use Gemini to filter and create natural response
        products_text = ""
        for idx, p in enumerate(unique_products[:15], 1):  # Top 15 for Gemini
            products_text += f"\n[{idx}] {p.get('product_name', 'N/A')}"
            products_text += f"\n    ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient', 'N/A')}"
            products_text += f"\n    ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {p.get('target_pest', 'N/A')[:100]}"
            products_text += f"\n    ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {p.get('applicable_crops', 'N/A')[:80]}"
            products_text += f"\n    ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period', 'N/A')}"
            products_text += f"\n    ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate', 'N/A')}"
            products_text += f"\n    Similarity: {p.get('similarity', 0):.0%}\n"
        
        # Create intent-specific prompt
        if intent == "increase_yield":
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏û‡∏∑‡∏ä

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:
{products_text}

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:
1. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:
   - ‡∏õ‡∏∏‡πã‡∏¢‡∏ö‡∏≥‡∏£‡∏∏‡∏á (NPK, ‡∏õ‡∏∏‡πã‡∏¢‡∏≠‡∏¥‡∏ô‡∏ó‡∏£‡∏µ‡∏¢‡πå)
   - ‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô‡πÄ‡∏£‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï
   - ‡∏™‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô
   - ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï

2. **‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå**:
   - ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 1: ‡∏õ‡∏∏‡πã‡∏¢‡∏ö‡∏≥‡∏£‡∏∏‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï)
   - ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 2: ‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô/PGR (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï)
   - ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 3: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û)

3. **‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**:
   - ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
   - ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£)
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
   - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á

4. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**:
   - ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
   - ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å‡∏î‡∏≠‡∏Å, ‡∏ï‡∏¥‡∏î‡∏ú‡∏•)
   - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á

5. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏û‡∏£‡πâ‡∏≠‡∏° emoji (üå± üí™ ‚≠ê ‚úÖ)
6. **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown** - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

‚ö†Ô∏è **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°)

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""
        
        elif intent == "solve_problem":
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:
{products_text}

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:
1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤**:
   - ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(pests) if pests else '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'}
   - ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏π‡∏Å: {', '.join(crops) if crops else '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'}

2. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:
   - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ
   - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ
   - ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

3. **‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**:
   - ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
   - ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
   - ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÑ‡∏î‡πâ
   - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•

4. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö**:
   - ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡πà‡∏ô
   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏û‡πà‡∏ô
   - ‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∑‡πâ‡∏≠‡∏¢‡∏≤
   - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á

5. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏û‡∏£‡πâ‡∏≠‡∏° emoji (üíä üêõ ‚ö° ‚úÖ)
6. **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown** - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

‚ö†Ô∏è **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ
- ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏û‡∏∑‡∏ä ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""
        
        else:
            # General product inquiry
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Ç‡∏≠‡∏á ICP Ladda

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:
{products_text}

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:
1. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
2. **‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö** ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
3. **‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
4. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥** ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
5. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏û‡∏£‡πâ‡∏≠‡∏° emoji
6. **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown**

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""
        
        # Check if Gemini is available
        if not gemini_model:
            logger.warning("Gemini model not available, using simple format")
            return await format_product_list_simple(unique_products[:5], question, intent)
        
        try:
            response = gemini_model.generate_content(prompt)
            answer = response.text.strip()
            answer = answer.replace("```", "").replace("**", "").replace("##", "")
            
            # Add footer
            answer += "\n\n" + "="*40
            answer += "\nüìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:"
            answer += "\nüîó https://www.icpladda.com/about/"
            answer += "\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ üòä"
            
            logger.info(f"‚úì Intent-based answer generated ({intent})")
            return answer
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}", exc_info=True)
            # Fallback to simple product list
            return await format_product_list_simple(unique_products[:5], question, intent)
        
    except Exception as e:
        logger.error(f"Error in intent-based recommendation: {e}", exc_info=True)
        return await answer_product_question(question, keywords)

async def format_product_list_simple(products: list, question: str, intent: str) -> str:
    """Format product list as simple fallback"""
    if intent == "increase_yield":
        header = "üå± ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï:\n"
    elif intent == "solve_problem":
        header = "üíä ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä:\n"
    else:
        header = "üì¶ ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n"
    
    response = header
    for idx, p in enumerate(products, 1):
        response += f"\n{idx}. {p.get('product_name', 'N/A')}"
        if p.get('active_ingredient'):
            response += f"\n   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient')}"
        if p.get('target_pest'):
            pest = p.get('target_pest')[:80] + "..." if len(p.get('target_pest', '')) > 80 else p.get('target_pest', '')
            response += f"\n   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {pest}"
        if p.get('applicable_crops'):
            crops = p.get('applicable_crops')[:60] + "..." if len(p.get('applicable_crops', '')) > 60 else p.get('applicable_crops', '')
            response += f"\n   ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops}"
        if p.get('usage_period'):
            response += f"\n   ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period')}"
        if p.get('usage_rate'):
            response += f"\n   ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate')}"
        response += "\n"
        if p.get('active_ingredient'):
            response += f"\n   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient')}"
        if p.get('target_pest'):
            pest = p.get('target_pest')[:80] + "..." if len(p.get('target_pest', '')) > 80 else p.get('target_pest', '')
            response += f"\n   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {pest}"
        if p.get('applicable_crops'):
            crops = p.get('applicable_crops')[:60] + "..." if len(p.get('applicable_crops', '')) > 60 else p.get('applicable_crops', '')
            response += f"\n   ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops}"
        if p.get('usage_period'):
            response += f"\n   ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period')}"
        if p.get('usage_rate'):
            response += f"\n   ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate')}"
        response += "\n"
    
    response += "\nüìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: https://www.icpladda.com/about/"
    return response

# ============================================================================#
# Core: Product-specific Q&A
# ============================================================================#
async def answer_product_question(question: str, keywords: dict) -> str:
    """Answer product-specific questions with high accuracy"""
    try:
        logger.info(f"Product-specific query: {question[:50]}...")
        
        if not supabase_client:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ"
        
        products_data = []
        
        # Search by pest/disease
        if keywords["pests"]:
            for pest in keywords["pests"][:2]:
                result = supabase_client.table('products')\
                    .select('*')\
                    .ilike('target_pest', f'%{pest}%')\
                    .limit(5)\
                    .execute()
                if result.data:
                    products_data.extend(result.data)
        
        # Search by crop
        if keywords["crops"]:
            for crop in keywords["crops"][:2]:
                result = supabase_client.table('products')\
                    .select('*')\
                    .ilike('applicable_crops', f'%{crop}%')\
                    .limit(5)\
                    .execute()
                if result.data:
                    products_data.extend(result.data)
        
        # Search by product name
        if keywords["products"]:
            for prod in keywords["products"]:
                if len(prod) > 3:
                    result = supabase_client.table('products')\
                        .select('*')\
                        .ilike('product_name', f'%{prod}%')\
                        .limit(5)\
                        .execute()
                    if result.data:
                        products_data.extend(result.data)
        
        # If no specific keywords, get general products
        if not products_data:
            result = supabase_client.table('products')\
                .select('*')\
                .limit(10)\
                .execute()
            if result.data:
                products_data = result.data
        
        if not products_data:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏∑‡∏ä‡∏´‡∏£‡∏∑‡∏≠‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡πà‡∏∞ üå±"
        
        # Remove duplicates
        seen = set()
        unique_products = []
        for p in products_data:
            pname = p.get('product_name', '')
            if pname and pname not in seen:
                seen.add(pname)
                unique_products.append(p)
        
        # Use Gemini to filter and format response
        products_text = ""
        for idx, p in enumerate(unique_products[:10], 1):
            products_text += f"\n[{idx}] {p.get('product_name', 'N/A')}"
            products_text += f"\n    ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient', 'N/A')}"
            products_text += f"\n    ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {p.get('target_pest', 'N/A')[:100]}"
            products_text += f"\n    ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {p.get('applicable_crops', 'N/A')[:80]}"
            products_text += f"\n    ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period', 'N/A')}"
            products_text += f"\n    ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate', 'N/A')}"
            products_text += "\n"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á ICP Ladda

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:
{products_text}

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°** - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£
2. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
3. **‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö** - ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
4. **‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**:
   - ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
   - ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
   - ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÑ‡∏î‡πâ
   - ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
   - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠
5. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**:
   - ‡∏≠‡πà‡∏≤‡∏ô‡∏â‡∏•‡∏≤‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ
   - ‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏±‡∏ß
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡πà‡∏≠‡∏ô
6. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏û‡∏£‡πâ‡∏≠‡∏° emoji (üíä üå± ‚úÖ ‚ö†Ô∏è)
7. **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown** - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

‚ö†Ô∏è **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å**:
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‚Üí ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""

        try:
            response = gemini_model.generate_content(prompt)
            answer = response.text.strip()
            answer = answer.replace("```", "").replace("**", "").replace("##", "")
            
            # Add footer
            answer += "\n\n" + "="*40
            answer += "\nüìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:"
            answer += "\nüîó https://www.icpladda.com/about/"
            answer += "\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ üòä"
            
            logger.info("‚úì Product answer generated successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            # Fallback: return top 3 products directly
            response = "üíä ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å ICP Ladda:\n"
            for idx, p in enumerate(unique_products[:3], 1):
                response += f"\n{idx}. {p.get('product_name')}"
                if p.get('active_ingredient'):
                    response += f"\n   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient')}"
                if p.get('target_pest'):
                    pest = p.get('target_pest')[:80] + "..." if len(p.get('target_pest', '')) > 80 else p.get('target_pest', '')
                    response += f"\n   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {pest}"
                if p.get('applicable_crops'):
                    crops = p.get('applicable_crops')[:60] + "..." if len(p.get('applicable_crops', '')) > 60 else p.get('applicable_crops', '')
                    response += f"\n   ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops}"
                if p.get('usage_period'):
                    response += f"\n   ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period')}"
                if p.get('usage_rate'):
                    response += f"\n   ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate')}"
                response += "\n"
            
            response += "\nüìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: https://www.icpladda.com/about/"
            return response
        
    except Exception as e:
        logger.error(f"Error in product Q&A: {e}", exc_info=True)
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏∞ üôè"

# ============================================================================#
# Core: Smart Q&A - Answer questions using Knowledge Base
# ============================================================================#
async def answer_question_with_knowledge(question: str) -> str:
    """Answer user questions using knowledge base and Gemini"""
    try:
        logger.info(f"Answering question: {question[:50]}...")
        
        if not supabase_client or not gemini_model:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ"
        
        # 1. Generate embedding for the question using E5 model (768 dimensions)
        try:
            if e5_model:
                # E5 requires "query: " prefix for queries
                query_text = f"query: {question}"
                query_embedding = e5_model.encode(query_text, normalize_embeddings=True).tolist()
                logger.info("‚úì Question embedding generated (E5, 768 dim)")
            else:
                logger.warning("E5 model not available, using keyword search")
                return await answer_with_keyword_search(question)
        except Exception as e:
            logger.warning(f"Failed to generate E5 embedding: {e}")
            # Fallback to keyword search
            return await answer_with_keyword_search(question)
        
        # 2. Search knowledge base using vector search
        try:
            result = supabase_client.rpc(
                'match_knowledge',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.6,  # Lower threshold for more results
                    'match_count': 10  # Get more candidates
                }
            ).execute()
            
            if result.data:
                logger.info(f"‚úì Found {len(result.data)} relevant knowledge entries")
                # Combine knowledge content
                knowledge_texts = []
                for item in result.data:
                    content = item.get('content', '').strip()
                    similarity = item.get('similarity', 0)
                    # Lower filter threshold to get more results
                    if content and similarity > 0.3:
                        # Clean the text before adding
                        cleaned_content = clean_knowledge_text(content)
                        if cleaned_content:
                            knowledge_texts.append(cleaned_content)
                
                if knowledge_texts:
                    combined_knowledge = "\n\n".join(knowledge_texts[:5])  # Top 5 for better context
                else:
                    # Fallback to keyword search
                    return await answer_with_keyword_search(question)
            else:
                # Fallback to keyword search
                return await answer_with_keyword_search(question)
                
        except Exception as e:
            logger.warning(f"Vector search failed: {e}")
            return await answer_with_keyword_search(question)
        
        # 3. Search for relevant products (Enhanced)
        products_info = ""
        try:
            # Extract keywords for product search
            keywords = extract_keywords_from_question(question)
            products_data = []
            
            # Strategy 1: Search by pest/disease
            if keywords["pests"]:
                for pest in keywords["pests"][:2]:  # Top 2 pests
                    result = supabase_client.table('products')\
                        .select('product_name, active_ingredient, target_pest, applicable_crops, how_to_use, usage_rate')\
                        .ilike('target_pest', f'%{pest}%')\
                        .limit(5)\
                        .execute()
                    if result.data:
                        products_data.extend(result.data)
            
            # Strategy 2: Search by crop
            if keywords["crops"]:
                for crop in keywords["crops"][:2]:  # Top 2 crops
                    result = supabase_client.table('products')\
                        .select('product_name, active_ingredient, target_pest, applicable_crops, how_to_use, usage_rate')\
                        .ilike('applicable_crops', f'%{crop}%')\
                        .limit(5)\
                        .execute()
                    if result.data:
                        products_data.extend(result.data)
            
            # Strategy 3: Search by product name (if asking about specific product)
            if keywords["products"]:
                for prod in keywords["products"][:2]:
                    if len(prod) > 3:  # Skip short words like "‡∏¢‡∏≤", "‡πÉ‡∏ä‡πâ"
                        result = supabase_client.table('products')\
                            .select('product_name, active_ingredient, target_pest, applicable_crops, how_to_use, usage_rate')\
                            .ilike('product_name', f'%{prod}%')\
                            .limit(5)\
                            .execute()
                        if result.data:
                            products_data.extend(result.data)
            
            # Remove duplicates and format
            if products_data:
                seen = set()
                unique_products = []
                for p in products_data:
                    pname = p.get('product_name', '')
                    if pname and pname not in seen:
                        seen.add(pname)
                        unique_products.append(p)
                
                # Limit to top 5 products
                products_list = []
                for p in unique_products[:5]:
                    prod_text = f"\nüì¶ {p.get('product_name')}"
                    if p.get('active_ingredient'):
                        prod_text += f"\n   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient')}"
                    if p.get('target_pest'):
                        pest_short = p.get('target_pest')[:80] + "..." if len(p.get('target_pest', '')) > 80 else p.get('target_pest', '')
                        prod_text += f"\n   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {pest_short}"
                    if p.get('applicable_crops'):
                        crops_short = p.get('applicable_crops')[:60] + "..." if len(p.get('applicable_crops', '')) > 60 else p.get('applicable_crops', '')
                        prod_text += f"\n   ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops_short}"
                    if p.get('usage_period'):
                        prod_text += f"\n   ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {p.get('usage_period')}"
                    if p.get('usage_rate'):
                        prod_text += f"\n   ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {p.get('usage_rate')}"
                    products_list.append(prod_text)
                
                products_info = "\n".join(products_list)
                logger.info(f"Found {len(unique_products)} relevant products")
        except Exception as e:
            logger.warning(f"Product search failed: {e}")
        
        # 4. Use Gemini to generate natural answer
        # Detect if this is a product-focused query
        keywords = extract_keywords_from_question(question)
        is_product_query = keywords["is_product_query"] or any(word in question.lower() for word in ["‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏¢‡∏≤", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£", "‡∏û‡πà‡∏ô‡∏≠‡∏∞‡πÑ‡∏£"])
        
        if is_product_query and products_info:
            # Product-focused response
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á ICP Ladda

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{products_info}

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:
{combined_knowledge if combined_knowledge else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"}

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. **‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å** - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
2. **‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** - ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
3. **‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:
   - ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
   - ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
   - ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÑ‡∏î‡πâ
   - ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
   - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠
4. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥** ‡πÄ‡∏ä‡πà‡∏ô:
   - ‡∏Ñ‡∏ß‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏â‡∏•‡∏≤‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡πà‡∏≠‡∏ô
   - ‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏±‡∏ß
5. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
6. **‡πÄ‡∏û‡∏¥‡πà‡∏° emoji** ‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô (üíä üå± ‚úÖ ‚ö†Ô∏è)
7. **‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown

‚ö†Ô∏è **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: 
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß ‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á) ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ
- ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""
        else:
            # General knowledge response with enhanced filtering
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡πÑ‡∏ó‡∏¢ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå 20 ‡∏õ‡∏µ

üéØ **‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£**: {question}

üìö **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:
{combined_knowledge}

üíä **‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á**:
{products_info if products_info else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå"}

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:

1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:
   - ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
   - ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å
   - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥, ‡∏ï‡πâ ‚Üí ‡∏ï‡πâ)

2. **‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö**:
   - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô)
   - ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô:
     ‚Ä¢ ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞/‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ (‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
     ‚Ä¢ ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ (‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
     ‚Ä¢ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
     ‚Ä¢ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏à‡∏±‡∏î/‡∏£‡∏±‡∏Å‡∏©‡∏≤
     ‚Ä¢ ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

3. **‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á**:
   - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏°
   - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
   - ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á
   - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ

4. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢**:
   - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
   - ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
   - ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ

5. **‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢**:
   - ‡πÉ‡∏ä‡πâ emoji ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (üå± üêõ üçÑ üíä ‚ö†Ô∏è ‚úÖ)
   - ‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
   - ‡πÉ‡∏ä‡πâ bullet points (‚Ä¢) ‡πÅ‡∏ó‡∏ô markdown
   - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown (**, ##, ```)

6. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤**:
   - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
   - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á
   - ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°**:
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ markdown (**, ##, ```)
- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏ô‡∏≠‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)

‚úÖ **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£**:
- ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥
- ‡∏ï‡πâ ‚Üí ‡∏ï‡πâ
- ‡∏•ƒû‡∏≥ ‚Üí ‡∏•‡∏≥
- ‡∏ôƒû‡πâ‡∏≥ ‚Üí ‡∏ô‡πâ‡∏≥

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown):"""

        try:
            response = gemini_model.generate_content(prompt)
            answer = response.text.strip()
            
            # Post-process answer for better quality
            answer = post_process_answer(answer)
            
            logger.info("‚úì Answer generated successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            # Return knowledge directly
            return f"üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n\n{combined_knowledge[:500]}...\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡πà‡∏∞"
        
    except Exception as e:
        logger.error(f"Error in Q&A: {e}", exc_info=True)
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"

async def answer_with_keyword_search(question: str) -> str:
    """Fallback: Answer using keyword search"""
    try:
        # Extract main keywords
        keywords = extract_keywords_from_question(question)
        
        if not keywords["pests"] and not keywords["crops"] and not keywords["products"]:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"
        
        # Search in knowledge table
        search_term = keywords["pests"][0] if keywords["pests"] else (keywords["crops"][0] if keywords["crops"] else keywords["products"][0])
        
        result = supabase_client.table('knowledge')\
            .select('content')\
            .ilike('content', f'%{search_term}%')\
            .limit(2)\
            .execute()
        
        # Also search products
        products_result = None
        if keywords["pests"] or keywords["crops"]:
            if keywords["pests"]:
                products_result = supabase_client.table('products')\
                    .select('product_name, active_ingredient, target_pest, applicable_crops')\
                    .ilike('target_pest', f'%{keywords["pests"][0]}%')\
                    .limit(3)\
                    .execute()
            elif keywords["crops"]:
                products_result = supabase_client.table('products')\
                    .select('product_name, active_ingredient, target_pest, applicable_crops')\
                    .ilike('applicable_crops', f'%{keywords["crops"][0]}%')\
                    .limit(3)\
                    .execute()
        
        response_parts = []
        
        if result.data:
            # Clean and format knowledge
            cleaned_items = []
            for item in result.data:
                content = item.get('content', '')
                cleaned = clean_knowledge_text(content)
                if cleaned:
                    cleaned_items.append(cleaned[:300])
            
            if cleaned_items:
                knowledge = "\n\n".join(cleaned_items)
                response_parts.append(f"üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n\n{knowledge}")
        
        if products_result and products_result.data:
            products_text = "\n\nüíä ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n"
            for idx, p in enumerate(products_result.data, 1):
                products_text += f"\n{idx}. {p.get('product_name')}"
                if p.get('active_ingredient'):
                    products_text += f"\n   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient')}"
                if p.get('applicable_crops'):
                    crops = p.get('applicable_crops')[:60] + "..." if len(p.get('applicable_crops', '')) > 60 else p.get('applicable_crops', '')
                    products_text += f"\n   ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops}"
            response_parts.append(products_text)
        
        if response_parts:
            response = "\n\n".join(response_parts)
            response += "\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡πà‡∏∞"
            return response
        else:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"
            
    except Exception as e:
        logger.error(f"Keyword search failed: {e}")
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏∞"

def extract_keywords_from_question(question: str) -> dict:
    """Extract main keywords from question with categories"""
    question_lower = question.lower()
    
    # Pest/Disease keywords (expanded)
    pest_keywords = [
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏≠‡πà‡∏≠‡∏ô", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢", "‡∏´‡∏ô‡∏≠‡∏ô", "‡πÅ‡∏°‡∏•‡∏á", "‡∏î‡πâ‡∏ß‡∏á‡∏á‡∏ß‡∏á",
        "‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á", "‡∏£‡∏≤‡πÅ‡∏õ‡πâ‡∏á", "‡∏£‡∏≤‡∏™‡∏ô‡∏¥‡∏°", "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡∏£‡∏≤", "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÑ‡∏ß‡∏£‡∏±‡∏™", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å",
        "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤", "‡∏ú‡∏±‡∏Å‡∏ö‡∏∏‡πâ‡∏á", "‡∏´‡∏ç‡πâ‡∏≤‡∏Ñ‡∏≤",
        "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä", "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä", "‡πÑ‡∏£", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÅ‡∏õ‡πâ‡∏á", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏Ç‡πâ‡∏≤‡∏ß",
        "‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡∏´‡∏ô‡∏≠‡∏ô‡πÄ‡∏à‡∏≤‡∏∞", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏≠", "‡∏´‡∏ô‡∏≠‡∏ô‡πÉ‡∏¢", "‡∏î‡πâ‡∏ß‡∏á", "‡∏°‡∏î", "‡∏õ‡∏•‡∏ß‡∏Å",
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡πÅ‡∏°‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ú‡∏•", "‡πÅ‡∏°‡∏•‡∏á‡∏´‡∏ß‡∏µ‡πà‡∏Ç‡∏≤‡∏ß", "‡∏ó‡∏£‡∏¥‡∏õ‡∏™‡πå"
    ]
    
    # Crop keywords (expanded)
    crop_keywords = [
        "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏û‡∏∑‡∏ä‡∏ú‡∏±‡∏Å", "‡∏ú‡∏±‡∏Å", "‡∏ú‡∏•‡πÑ‡∏°‡πâ",
        "‡∏°‡∏∞‡∏ô‡∏≤‡∏ß", "‡∏™‡πâ‡∏°", "‡∏Å‡∏•‡πâ‡∏ß‡∏¢", "‡∏°‡∏∞‡∏û‡∏£‡πâ‡∏≤‡∏ß", "‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤", "‡∏õ‡∏≤‡∏•‡πå‡∏°",
        "‡∏Ç‡πâ‡∏≤‡∏ß‡πÇ‡∏û‡∏î", "‡∏≠‡πâ‡∏≠‡∏¢", "‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á", "‡∏ñ‡∏±‡πà‡∏ß", "‡∏û‡∏£‡∏¥‡∏Å", "‡∏°‡∏∞‡πÄ‡∏Ç‡∏∑‡∏≠‡πÄ‡∏ó‡∏®",
        "‡∏•‡∏≥‡πÑ‡∏¢", "‡∏•‡∏¥‡πâ‡∏ô‡∏à‡∏µ‡πà", "‡πÄ‡∏á‡∏≤‡∏∞", "‡∏°‡∏±‡∏á‡∏Ñ‡∏∏‡∏î", "‡∏ù‡∏£‡∏±‡πà‡∏á", "‡∏ä‡∏°‡∏û‡∏π‡πà"
    ]
    
    # Product-related keywords
    product_keywords = [
        "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏¢‡∏≤", "‡∏™‡∏≤‡∏£", "‡∏õ‡∏∏‡πã‡∏¢",
        "icp", "ladda", "icpl", "‡πÑ‡∏≠‡∏ã‡∏µ‡∏û‡∏µ", "‡∏•‡∏±‡∏î‡∏î‡∏≤",
        "‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏ô", "‡πÑ‡∏î‡∏≠‡∏∞‡∏ã‡∏¥‡∏ô‡∏≠‡∏ô", "‡∏≠‡∏¥‡∏°‡∏¥‡∏î‡∏≤‡πÇ‡∏Ñ‡∏•‡∏û‡∏£‡∏¥‡∏î", "‡πÑ‡∏ã‡πÄ‡∏û‡∏≠‡∏£‡πå‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏ô",
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡πÉ‡∏ä‡πâ", "‡∏û‡πà‡∏ô", "‡∏â‡∏µ‡∏î", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô"
    ]
    
    # Intent keywords (NEW)
    intent_keywords = {
        "increase_yield": ["‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏™‡∏π‡∏á", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏°‡∏≤‡∏Å", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏¢‡∏≠‡∏∞", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏°"],
        "solve_problem": ["‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°"],
        "general_care": ["‡∏î‡∏π‡πÅ‡∏•", "‡∏ö‡∏≥‡∏£‡∏∏‡∏á", "‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á", "‡∏õ‡∏•‡∏π‡∏Å", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢"],
        "product_inquiry": ["‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏°‡∏µ‡πÑ‡∏´‡∏°", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ã‡∏∑‡πâ‡∏≠"]
    }
    
    found = {
        "pests": [],
        "crops": [],
        "products": [],
        "intent": None,  # NEW: detect user intent
        "is_product_query": False
    }
    
    # Extract pests
    for keyword in pest_keywords:
        if keyword in question_lower:
            found["pests"].append(keyword)
    
    # Extract crops
    for keyword in crop_keywords:
        if keyword in question_lower:
            found["crops"].append(keyword)
    
    # Extract product-related
    for keyword in product_keywords:
        if keyword in question_lower:
            found["products"].append(keyword)
            found["is_product_query"] = True
    
    # Detect intent (NEW)
    for intent, keywords in intent_keywords.items():
        for keyword in keywords:
            if keyword in question_lower:
                found["intent"] = intent
                found["is_product_query"] = True
                break
        if found["intent"]:
            break
    
    return found

# ============================================================================#
# Core: Retrieve knowledge from knowledge table (Vector Search)
# ============================================================================#
async def retrieve_knowledge_from_knowledge_table(disease_info: DiseaseDetectionResult) -> str:
    """Query knowledge table using vector search + Gemini filtering for disease information"""
    try:
        if not supabase_client:
            return ""
        
        logger.info(f"üîç Searching knowledge for: {disease_info.disease_name}")
        
        # Strategy 1: Search by exact disease name first (most accurate)
        query_text = disease_info.disease_name
        logger.info(f"üìù Primary query: {query_text}")
        
        # Generate embedding using E5 model (768 dimensions)
        try:
            if e5_model:
                query_with_prefix = f"query: {query_text}"
                query_embedding = e5_model.encode(query_with_prefix, normalize_embeddings=True).tolist()
                logger.info("‚úì Embedding generated (E5, 768 dim)")
            else:
                logger.warning("E5 model not available, using keyword search")
                return await retrieve_knowledge_keyword_search(disease_info)
        except Exception as e:
            logger.warning(f"Failed to generate E5 embedding: {e}")
            return await retrieve_knowledge_keyword_search(disease_info)
        
        # Vector similarity search
        try:
            result = supabase_client.rpc(
                'match_knowledge',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.4,  # Lower threshold to get more candidates
                    'match_count': 10  # Get more results for Gemini to filter
                }
            ).execute()
            
            if result.data and len(result.data) > 0:
                logger.info(f"‚úì Found {len(result.data)} knowledge candidates")
                
                # Collect all knowledge content for Gemini filtering
                knowledge_candidates = []
                for idx, item in enumerate(result.data, 1):
                    content = item.get('content', '').strip()
                    similarity = item.get('similarity', 0)
                    if content and len(content) > 20:
                        cleaned_content = clean_knowledge_text(content)
                        knowledge_candidates.append({
                            'index': idx,
                            'content': cleaned_content,
                            'similarity': similarity
                        })
                
                if knowledge_candidates:
                    # Use Gemini to filter and synthesize the most relevant knowledge
                    filtered_knowledge = await filter_knowledge_with_gemini(
                        disease_info.disease_name,
                        knowledge_candidates
                    )
                    
                    if filtered_knowledge:
                        logger.info(f"‚úì Gemini filtered knowledge successfully")
                        return filtered_knowledge
                    else:
                        # Fallback: return top 2 by similarity
                        logger.info("‚ö†Ô∏è Gemini filtering failed, using top results")
                        top_results = sorted(knowledge_candidates, key=lambda x: x['similarity'], reverse=True)[:2]
                        return "\n\n".join([k['content'][:300] + "..." if len(k['content']) > 300 else k['content'] for k in top_results])
            else:
                logger.info("No knowledge found via vector search, trying keyword search")
                return await retrieve_knowledge_keyword_search(disease_info)
                
        except Exception as e:
            logger.warning(f"Vector search failed: {e}, trying keyword search")
            return await retrieve_knowledge_keyword_search(disease_info)
        
        return ""
        
    except Exception as e:
        logger.warning(f"Failed to retrieve knowledge: {e}")
        return ""

async def filter_knowledge_with_gemini(disease_name: str, knowledge_candidates: List[Dict]) -> str:
    """Use Gemini to filter and synthesize the most relevant knowledge"""
    try:
        if not gemini_model:
            return ""
        
        # Build prompt with all candidates
        candidates_text = ""
        for k in knowledge_candidates[:5]:  # Top 5 candidates
            candidates_text += f"\n[{k['index']}] (Similarity: {k['similarity']:.0%})\n{k['content'][:400]}\n"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö "{disease_name}"

üìö **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏û‡∏ö**:
{candidates_text}

üìã **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**:
1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö "{disease_name}"
3. ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 250 ‡∏Ñ‡∏≥)
4. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞, ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏à‡∏±‡∏î
5. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
6. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ markdown ‡∏´‡∏£‡∏∑‡∏≠ bullet points

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:"""

        response = gemini_model.generate_content(prompt)
        filtered_text = response.text.strip()
        
        # Check if Gemini found relevant info
        if "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in filtered_text or "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠" in filtered_text or len(filtered_text) < 50:
            logger.warning("Gemini: No relevant knowledge found")
            return ""
        
        # Clean up markdown if any
        filtered_text = filtered_text.replace("```", "").replace("**", "").replace("##", "")
        
        logger.info(f"‚úì Gemini filtered knowledge: {len(filtered_text)} chars")
        return filtered_text
        
    except Exception as e:
        logger.error(f"Gemini filtering failed: {e}")
        return ""

async def retrieve_knowledge_keyword_search(disease_info: DiseaseDetectionResult) -> str:
    """Fallback: keyword search in knowledge table"""
    try:
        result = supabase_client.table('knowledge')\
            .select('content')\
            .ilike('content', f'%{disease_info.disease_name}%')\
            .limit(2)\
            .execute()
        
        if result.data:
            logger.info(f"‚úì Found {len(result.data)} knowledge entries via keyword search")
            knowledge_parts = []
            for item in result.data:
                content = item.get('content', '').strip()
                if content and len(content) > 20:
                    # Clean the text first
                    cleaned_content = clean_knowledge_text(content)
                    preview = cleaned_content[:250] + "..." if len(cleaned_content) > 250 else cleaned_content
                    knowledge_parts.append(preview)
            
            if knowledge_parts:
                return "\n\n".join(knowledge_parts)
        
        return ""
    except Exception as e:
        logger.warning(f"Keyword search failed: {e}")
        return ""

# ============================================================================#
# Core: Generate final response (single long text block, friendly Thai)
# ============================================================================#
async def generate_final_response(
    disease_info: DiseaseDetectionResult,
    recommendations: List[ProductRecommendation]
) -> str:
    try:
        logger.info("Generating final response (Thai friendly minimal RAG)")

        # Header: disease summary with confidence warning
        header = f"üîç ‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û: {disease_info.disease_name}\n\n"
        
        # Add confidence indicator
        confidence_str = str(disease_info.confidence)
        confidence_emoji = "üü¢" if "‡∏™‡∏π‡∏á" in confidence_str or any(str(x) in confidence_str for x in range(70, 101)) else \
                          "üü°" if "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á" in confidence_str or any(str(x) in confidence_str for x in range(50, 70)) else "üî¥"
        
        header += f"{confidence_emoji} ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {disease_info.confidence}\n"
        
        # Add warning for low confidence
        if "‡∏ï‡πà‡∏≥" in confidence_str or confidence_emoji == "üî¥":
            header += "‚ö†Ô∏è **‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç\n\n"
        
        header += f"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á: {disease_info.severity}\n\n"
        header += f"üìù ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô: {disease_info.symptoms}\n\n"
        
        # Retrieve additional knowledge from knowledge table (Vector Search)
        knowledge = await retrieve_knowledge_from_knowledge_table(disease_info)
        if knowledge:
            header += f"üìö ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n{knowledge}\n\n"

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
        disease_name_lower = disease_info.disease_name.lower()
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤/‡πÇ‡∏£‡∏Ñ
        if "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤" in disease_name_lower or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏£‡∏Ñ" in disease_name_lower:
            body = "‚úÖ **‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡∏µ!** ‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏î‡∏µ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏Å‡∏±‡∏á‡∏ß‡∏•\n\n"
            body += "ÔøΩ ‡∏î**‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•**:\n"
            body += "‚Ä¢ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô‡πÉ‡∏ô‡∏î‡∏¥‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n"
            body += "‚Ä¢ ‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πã‡∏¢‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ß‡∏•‡∏≤\n"
            body += "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏∑‡∏ä‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥\n"
            body += "‚Ä¢ ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®\n\n"
            body += "üå± **‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô**:\n"
            body += "‚Ä¢ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏£‡∏≠‡∏ö‡πÅ‡∏õ‡∏•‡∏á\n"
            body += "‚Ä¢ ‡∏ï‡∏±‡∏î‡πÉ‡∏ö‡πÅ‡∏Å‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å\n"
            body += "‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏µ\n"
            body += "‚Ä¢ ‡∏´‡∏°‡∏±‡πà‡∏ô‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥\n\n"
            body += "üì∏ ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ üòä"
            return header + body
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ / ‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î / ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏∑‡∏ä
        if any(keyword in disease_name_lower for keyword in [
            "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
            "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏∑‡∏ä",
            "‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î",
            "‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏"
        ]):
            body = "‚ö†Ô∏è **‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ**\n\n"
            body += "üì∏ **‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤**:\n\n"
            body += "1Ô∏è‚É£ **‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**\n"
            body += "   ‚Ä¢ ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏á‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÅ‡∏™‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\n"
            body += "   ‚Ä¢ ‡πÑ‡∏°‡πà‡πÄ‡∏ö‡∏•‡∏≠ ‡πÑ‡∏°‡πà‡∏™‡∏±‡πà‡∏ô\n"
            body += "   ‚Ä¢ ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n\n"
            body += "2Ô∏è‚É£ **‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡∏û‡∏≠**\n"
            body += "   ‚Ä¢ ‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö/‡∏•‡∏≥‡∏ï‡πâ‡∏ô/‡∏ú‡∏•\n"
            body += "   ‚Ä¢ ‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡∏à‡∏∏‡∏î, ‡πÅ‡∏ú‡∏•, ‡∏™‡∏µ)\n"
            body += "   ‚Ä¢ ‡∏£‡∏∞‡∏¢‡∏∞ 20-50 ‡∏ã‡∏°. ‡∏à‡∏≤‡∏Å‡∏û‡∏∑‡∏ä\n\n"
            body += "3Ô∏è‚É£ **‡∏ñ‡πà‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤**\n"
            body += "   ‚Ä¢ ‡πÉ‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\n"
            body += "   ‚Ä¢ ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏°‡∏•‡∏á\n"
            body += "   ‚Ä¢ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏µ\n\n"
            body += "4Ô∏è‚É£ **‡∏ñ‡πà‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏°** (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ)\n"
            body += "   ‚Ä¢ ‡∏°‡∏∏‡∏°‡πÉ‡∏Å‡∏•‡πâ (Close-up)\n"
            body += "   ‚Ä¢ ‡∏°‡∏∏‡∏°‡∏Å‡∏•‡∏≤‡∏á (‡πÉ‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ö)\n"
            body += "   ‚Ä¢ ‡∏°‡∏∏‡∏°‡πÑ‡∏Å‡∏• (‡∏ï‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô)\n\n"
            body += "üí° **‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö**:\n"
            body += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 8-10 ‡πÇ‡∏°‡∏á‡πÄ‡∏ä‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ 3-5 ‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô\n"
            body += "‚Ä¢ ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏™‡∏á‡πÅ‡∏î‡∏î‡∏à‡πâ‡∏≤‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n"
            body += "‚Ä¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏°‡∏•‡∏á ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á‡∏î‡πâ‡∏ß‡∏¢\n\n"
            body += "üì§ **‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß?** ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞ üòä"
            return header + body
        
        if not recommendations:
            body = "‚ö†Ô∏è ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n\n"
            body += "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î\n\n"
            body += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà:\n"
            body += "https://www.icpladda.com/about/\n\n"
            body += "‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏ñ‡πà‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö üòä"
            return header + body

        # Build product blocks (minimal fields)
        body = "üíä ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n"
        for idx, rec in enumerate(recommendations, 1):
            body += f"\n{idx}. {rec.product_name}\n"
            if rec.active_ingredient:
                body += f"   ‚Ä¢ ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {rec.active_ingredient}\n"
            if rec.usage_rate:
                body += f"   ‚Ä¢ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {rec.usage_rate}\n"
            if rec.target_pest:
                # Truncate long text
                pest_text = rec.target_pest[:80] + "..." if len(rec.target_pest) > 80 else rec.target_pest
                body += f"   ‚Ä¢ ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {pest_text}\n"
            if rec.applicable_crops:
                crops_text = rec.applicable_crops[:60] + "..." if len(rec.applicable_crops) > 60 else rec.applicable_crops
                body += f"   ‚Ä¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops_text}\n"
            # friendly short how-to: if long, take first line
            if rec.how_to_use:
                short_how = rec.how_to_use.split("\n")[0].strip()
                if len(short_how) > 80:
                    short_how = short_how[:80] + "..."
                body += f"   ‚Ä¢ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ: {short_how}\n"
            body += f"   ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {int(rec.score * 100)}%\n"

        footer = "\n" + "="*40 + "\n"
        footer += "üìã **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:\n"
        footer += "‚Ä¢ ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡∏£‡∏≤/‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ï‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏Å‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n"
        footer += "‚Ä¢ ‚úÖ ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ\n"
        footer += "‚Ä¢ ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏û‡πà‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏õ‡∏•‡∏á\n\n"
        
        footer += "ÔøΩ  **‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô**:\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏Å‡∏•‡πâ‡πÜ ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏° (‡πÉ‡∏ö ‡∏•‡∏≥‡∏ï‡πâ‡∏ô ‡∏ú‡∏•)\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏á‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠\n"
        footer += "‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏ô‡∏¥‡∏î‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n\n"
        
        footer += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n"
        footer += "üîó https://www.icpladda.com/about/\n\n"
        footer += "üí¨ ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ üòä"
        
        return header + body + footer

    except Exception as e:
        logger.error(f"Error generating final response: {e}", exc_info=True)
        # fallback simple template
        products_text = "\n".join([f"‚Ä¢ {p.product_name}" for p in (recommendations or [])[:3]])
        fallback = f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {disease_info.disease_name}\n\n"
        fallback += f"‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n{products_text}\n\n"
        fallback += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: https://www.icpladda.com/about/\n"
        fallback += "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏â‡∏•‡∏≤‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ"
        return fallback

# ============================================================================#
# LINE reply helper
# ============================================================================#
async def handle_natural_conversation(user_id: str, text: str, reply_token: str) -> None:
    """Handle natural conversation using Gemini AI"""
    try:
        logger.info(f"Natural conversation: {text[:50]}...")
        
        # Get conversation context
        context = await get_conversation_context(user_id)
        
        # Build prompt for Gemini
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡∏ä‡∏∑‡πà‡∏≠ "ICP LADDA"

üéØ **‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó**:
- ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
- ‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡πÄ‡∏û‡∏¥‡πà‡∏° emoji ‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å üå± üêõ üçÑ üíä

üìù **‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤**:
{context if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà)"}

üí¨ **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ**: {text}

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:
1. **‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏à‡∏ï‡∏ô‡∏≤**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£
   - ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢ ‚Üí ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö
   - ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‚Üí ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
   - ‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‚Üí ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
   - ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‚Üí ‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥

2. **‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö**: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3-4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß)

3. **‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£**: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡πà‡∏∞", "‡∏ô‡∏∞‡∏Ñ‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö" ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó

4. **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**: ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ:
   - ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
   - ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
   - ‡∏û‡∏¥‡∏°‡∏û‡πå "‡∏ä‡πà‡∏ß‡∏¢" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

5. **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ markdown**: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°**:
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏©‡∏ï‡∏£/‡∏û‡∏∑‡∏ä
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à"

‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):"""

        # Call Gemini
        response = gemini_model.generate_content(prompt)
        answer = response.text.strip()
        
        # Clean up markdown if any
        answer = answer.replace("```", "").replace("**", "").replace("##", "")
        
        # Add to memory
        await add_to_memory(user_id, "user", text)
        await add_to_memory(user_id, "assistant", answer)
        
        # Reply
        await reply_line(reply_token, answer)
        
    except Exception as e:
        logger.error(f"Natural conversation error: {e}", exc_info=True)
        fallback = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° üòÖ\n\nüí° ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏ä‡πà‡∏ß‡∏¢' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô üå±"
        await reply_line(reply_token, fallback)

async def reply_line(reply_token: str, message: str, with_sticker: bool = False) -> None:
    """Reply to LINE with text message and optionally a sticker"""
    try:
        logger.info(f"Replying to LINE token: {reply_token[:10]}...")
        url = "https://api.line.me/v2/bot/message/reply"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"
        }
        
        # Build messages array
        messages = [{"type": "text", "text": message}]
        
        # Add sticker if requested
        if with_sticker:
            # Use LINE's free sticker packages
            # Package 446: Brown & Cony's Friendly Stickers
            sticker_message = {
                "type": "sticker",
                "packageId": "446",
                "stickerId": "1988"  # Thumbs up sticker
            }
            messages.append(sticker_message)
        
        payload = {"replyToken": reply_token, "messages": messages}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
        logger.info("Reply sent to LINE")
    except Exception as e:
        logger.error(f"Error sending LINE reply: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to send LINE reply: {str(e)}")

# ============================================================================#
# API endpoints (unchanged flow)
# ============================================================================#
@app.get("/")
async def root():
    return {"status": "ok", "service": "LINE Plant Disease Detection Bot", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    cache_stats = await get_cache_stats()
    return {
        "status": "healthy",
        "services": {
            "gemini": "ok" if GEMINI_API_KEY else "not_configured",
            "supabase": "ok" if supabase_client else "not_configured",
            "line": "ok" if LINE_CHANNEL_ACCESS_TOKEN else "not_configured"
        },
        "cache": cache_stats,
        "rate_limiting": {
            "active_users": len(user_request_counts),
            "user_limit": f"{USER_RATE_LIMIT} requests per {USER_RATE_WINDOW}s"
        }
    }

@app.get("/cache/stats")
async def cache_stats():
    """Get detailed cache statistics"""
    stats = await get_cache_stats()
    return {
        "cache_stats": stats,
        "cache_config": {
            "ttl_seconds": CACHE_TTL,
            "max_size": MAX_CACHE_SIZE,
            "pending_context_ttl": PENDING_CONTEXT_TTL
        },
        "rate_limiting": {
            "active_users": len(user_request_counts),
            "user_limit": USER_RATE_LIMIT,
            "window_seconds": USER_RATE_WINDOW
        }
    }

@app.post("/cache/clear")
async def clear_cache():
    """Clear all caches (admin endpoint - should be protected in production)"""
    detection_cache.clear()
    product_cache.clear()
    knowledge_cache.clear()
    pending_image_contexts.clear()
    logger.info("All caches cleared manually")
    return {"status": "success", "message": "All caches cleared"}

@app.post("/webhook")
@limiter.limit("30/minute")  # Global rate limit: 30 requests per minute per IP
async def webhook(
    request: Request,
    x_line_signature: str = Header(None, alias="X-Line-Signature")
):
    try:
        body = await request.body()
        if x_line_signature and not verify_line_signature(body, x_line_signature):
            logger.warning("Invalid LINE signature")
            raise HTTPException(status_code=403, detail="Invalid signature")
        webhook_data = json.loads(body.decode("utf-8"))
        events = webhook_data.get("events", [])
        logger.info(f"Received {len(events)} events from LINE")
        for event in events:
            event_type = event.get("type")
            reply_token = event.get("replyToken")
            
            # Get user ID for rate limiting
            user_id = event.get("source", {}).get("userId")
            
            # Check user-specific rate limit
            if user_id and not await check_user_rate_limit(user_id):
                rate_limit_message = (
                    "‚ö†Ô∏è ‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Ñ‡πà‡∏∞\n\n"
                    f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ {USER_RATE_WINDOW} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞ üôè\n\n"
                    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏î‡∏µ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ:\n"
                    "‚Ä¢ ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô 1 ‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n"
                    "‚Ä¢ ‡∏£‡∏≠‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏°‡πà\n"
                    "‚Ä¢ ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á"
                )
                await reply_line(reply_token, rate_limit_message)
                continue
            
            if event_type == "message":
                message = event.get("message", {})
                message_type = message.get("type")
                if message_type == "image":
                    # When receiving an image, store it and ask the user for additional symptoms
                    message_id = message.get("id")
                    try:
                        image_bytes = await get_image_content_from_line(message_id)
                        user_id = event.get("source", {}).get("userId") or event.get("source", {}).get("userId")
                        if user_id:
                            # store pending context with timestamp
                            pending_image_contexts[user_id] = {
                                "image_bytes": image_bytes,
                                "reply_token": reply_token,
                                "timestamp": time.time()
                            }
                        ask_message = (
                            "‚úÖ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n"
                            "üìù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:\n\n"
                            "1Ô∏è‚É£ **‡∏ä‡∏ô‡∏¥‡∏î‡∏û‡∏∑‡∏ä**: ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á/‡∏Ç‡πâ‡∏≤‡∏ß/‡∏≠‡∏∑‡πà‡∏ô‡πÜ?\n"
                            "2Ô∏è‚É£ **‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£**: ‡πÉ‡∏ö/‡∏•‡∏≥‡∏ï‡πâ‡∏ô/‡∏ú‡∏•/‡∏£‡∏≤‡∏Å?\n"
                            "3Ô∏è‚É£ **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£**:\n"
                            "   ‚Ä¢ ‡∏™‡∏µ‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î/‡πÅ‡∏ú‡∏• (‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•/‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á/‡∏î‡∏≥/‡∏Ç‡∏≤‡∏ß)\n"
                            "   ‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏°‡∏≤‡∏Å)\n"
                            "   ‚Ä¢ ‡∏°‡∏µ‡πÅ‡∏°‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î)\n\n"
                            "4Ô∏è‚É£ **‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤**: ‡πÄ‡∏Å‡∏¥‡∏î‡∏°‡∏≤‡∏ô‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n\n"
                            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö:\n"
                            "\"‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÉ‡∏ö‡∏°‡πâ‡∏ß‡∏ô ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏• ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏µ‡∏î‡∏≥ ‡πÄ‡∏Å‡∏¥‡∏î‡∏°‡∏≤ 3 ‡∏ß‡∏±‡∏ô\"\n\n"
                            "‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å ‡∏¢‡∏¥‡πà‡∏á‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ñ‡πà‡∏∞ üéØ"
                        )
                        await reply_line(reply_token, ask_message)
                    except Exception as e:
                        logger.error(f"Error fetching image content: {e}", exc_info=True)
                        error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å LINE ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
                        await reply_line(reply_token, error_message)
                elif message_type == "text":
                    text = message.get("text", "").strip()
                    user_id = event.get("source", {}).get("userId")
                    # If user has a pending image, treat this text as symptom input
                    if user_id and user_id in pending_image_contexts:
                        ctx = pending_image_contexts.pop(user_id)
                        image_bytes = ctx.get("image_bytes")
                        try:
                            # Run detection with extra user-provided observations
                            disease_result = await detect_disease(image_bytes, extra_user_info=text)
                            
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                            disease_name_lower = disease_result.disease_name.lower()
                            
                            # ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ
                            skip_product_recommendation = any(keyword in disease_name_lower for keyword in [
                                "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
                                "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏£‡∏Ñ",
                                "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
                                "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏∑‡∏ä",
                                "‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î",
                                "‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
                                "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏"
                            ])
                            
                            if skip_product_recommendation:
                                recommendations = []
                            else:
                                recommendations = await retrieve_product_recommendation(disease_result)
                            
                            final_message = await generate_final_response(disease_result, recommendations)
                            await reply_line(reply_token, final_message)
                        except Exception as e:
                            logger.error(f"Error processing combined image+text: {e}", exc_info=True)
                            error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏∞ üôè"
                            await reply_line(reply_token, error_message)
                    else:
                        # Natural conversation with memory
                        text_lower = text.lower()
                        
                        # Check for memory clear command
                        if any(keyword in text_lower for keyword in ["‡∏•‡∏∑‡∏°", "‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà", "clear", "reset"]):
                            await clear_memory(user_id)
                            clear_message = "‚úÖ ‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ üå±"
                            await reply_line(reply_token, clear_message)
                        
                        # Check for specific commands that need exact responses
                        elif any(keyword in text_lower for keyword in ["‡∏ä‡πà‡∏ß‡∏¢", "help", "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á"]):
                            help_message = (
                                "üå± ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Plant Disease Bot\n\n"
                                "ÔøΩ ‡∏ï‡∏£‡∏ß‡∏≥‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä:\n"
                                "1. ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n"
                                "2. ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô\n"
                                "3. ‡∏£‡∏≠ 5-10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n"
                                "4. ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå\n\n"
                                "ÔøΩ ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤/‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô:\n"
                                "‚Ä¢ \"‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏¢‡∏±‡∏á‡πÑ‡∏á?\"\n"
                                "‚Ä¢ \"‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏≠‡∏∞‡πÑ‡∏£?\"\n"
                                "‚Ä¢ \"‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏ô 50 ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?\"\n\n"
                                "üéØ ‡∏â‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:\n"
                                "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä/‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä\n"
                                "‚Ä¢ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä\n"
                                "‚Ä¢ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ\n"
                                "‚Ä¢ ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥\n\n"
                                "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞! üòä"
                            )
                            await add_to_memory(user_id, "user", text)
                            await add_to_memory(user_id, "assistant", help_message)
                            await reply_line(reply_token, help_message)
                        
                        elif any(keyword in text_lower for keyword in ["‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "product"]):
                            help_message = (
                                "üì¶ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n\n"
                                "‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä 43 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:\n\n"
                                "üêõ ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÅ‡∏°‡∏•‡∏á (Insecticide)\n"
                                "üçÑ ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤ (Fungicide)\n"
                                "üåø ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä (Herbicide)\n"
                                "üå± ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï (PGR)\n\n"
                                "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà:\n"
                                "üîó https://www.icpladda.com/about/\n\n"
                                "üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:\n"
                                "‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏∞ üòä"
                            )
                            await add_to_memory(user_id, "user", text)
                            await add_to_memory(user_id, "assistant", help_message)
                            await reply_line(reply_token, help_message)
                        
                        # Check if it's a specific question that needs knowledge base
                        elif any(q in text_lower for q in ["?", "‡∏¢‡∏±‡∏á‡πÑ‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏Ñ‡∏∑‡∏≠", "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á", "‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°", "‡πÉ‡∏ä‡πâ", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡πÄ‡∏û‡∏¥‡πà‡∏°", "‡πÅ‡∏Å‡πâ"]):
                            # This is a question - check if it's product-focused
                            logger.info(f"Processing Q&A: {text[:50]}...")
                            try:
                                # Extract keywords to determine query type
                                keywords = extract_keywords_from_question(text)
                                
                                # Check intent
                                intent = keywords.get("intent")
                                
                                # Check if this is a product-focused query
                                is_product_query = (
                                    keywords["is_product_query"] or 
                                    intent in ["increase_yield", "solve_problem", "general_care", "product_inquiry"] or
                                    any(word in text_lower for word in [
                                        "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏¢‡∏≤", "‡∏™‡∏≤‡∏£", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", 
                                        "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£", "‡∏û‡πà‡∏ô‡∏≠‡∏∞‡πÑ‡∏£", "‡∏â‡∏µ‡∏î‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ã‡∏∑‡πâ‡∏≠",
                                        "icp", "ladda", "icpl", "‡πÑ‡∏≠‡∏ã‡∏µ‡∏û‡∏µ", "‡∏•‡∏±‡∏î‡∏î‡∏≤",
                                        "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏°‡∏µ‡πÑ‡∏´‡∏°", "‡∏Ç‡∏≤‡∏¢",
                                        "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"
                                    ]) or
                                    (keywords["crops"] and any(word in text_lower for word in ["‡πÉ‡∏ä‡πâ", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏î‡∏µ", "‡πÄ‡∏´‡∏°‡∏≤‡∏∞", "‡πÄ‡∏û‡∏¥‡πà‡∏°", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï"]))
                                )
                                
                                if is_product_query:
                                    # Check if it's intent-based query (increase yield, solve problem)
                                    if intent in ["increase_yield", "solve_problem", "general_care"]:
                                        logger.info(f"‚Üí Intent-based query detected: {intent}")
                                        answer = await recommend_products_by_intent(text, keywords)
                                    else:
                                        # Use product-specific Q&A
                                        logger.info("‚Üí Product-focused query detected")
                                        answer = await answer_product_question(text, keywords)
                                else:
                                    # Use general knowledge Q&A
                                    logger.info("‚Üí General knowledge query")
                                    answer = await answer_question_with_knowledge(text)
                                
                                await add_to_memory(user_id, "user", text)
                                await add_to_memory(user_id, "assistant", answer)
                                await reply_line(reply_token, answer)
                            except Exception as e:
                                logger.error(f"Q&A error: {e}", exc_info=True)
                                # Fallback to natural conversation
                                await handle_natural_conversation(user_id, text, reply_token)
                        
                        else:
                            # Natural conversation for everything else
                            await handle_natural_conversation(user_id, text, reply_token)
                
                elif message_type == "sticker":
                    # Handle sticker messages
                    sticker_id = message.get("stickerId")
                    package_id = message.get("packageId")
                    logger.info(f"Received sticker: packageId={package_id}, stickerId={sticker_id}")
                    
                    # Reply with a friendly sticker response
                    sticker_response = (
                        "üòä ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞!\n\n"
                        "üå± ‡∏â‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n"
                        "üì∏ ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô\n"
                        "‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏∞\n\n"
                        "üí° ‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏ä‡πà‡∏ß‡∏¢' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
                    )
                    # Reply with text and sticker
                    await reply_line(reply_token, sticker_response, with_sticker=True)
                
                else:
                    # Handle other message types (video, audio, location, etc.)
                    logger.info(f"Received unsupported message type: {message_type}")
                    unsupported_message = (
                        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞:\n\n"
                        "üì∏ ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä\n"
                        "üí¨ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
                        "üòä ‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢\n\n"
                        "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏Ñ‡πà‡∏∞ üå±"
                    )
                    await reply_line(reply_token, unsupported_message)
        
        return JSONResponse(content={"status": "ok"})
    except Exception as e:
        logger.error(f"Webhook error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================#
# Startup
# ============================================================================#
@app.on_event("startup")
async def startup_event():
    logger.info("=" * 60)
    logger.info("Starting LINE Plant Pest & Disease Detection Bot")
    logger.info(f"Gemini API: {'‚úì' if GEMINI_API_KEY else '‚úó'}")
    logger.info(f"Supabase: {'‚úì' if supabase_client else '‚úó'}")
    logger.info(f"LINE Bot: {'‚úì' if LINE_CHANNEL_ACCESS_TOKEN else '‚úó'}")
    logger.info("Vision: Google Gemini 2.5 Flash")
    logger.info("RAG Method: Keyword Search (Fast & Reliable)")
    logger.info("=" * 60)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
