import re
from typing import List, Dict

# Allowed emojis: ðŸ˜Š (U+1F60A) and ðŸŒ± (U+1F331)
_ALLOWED_EMOJIS = {'\U0001F60A', '\U0001F331'}

# Unicode ranges covering most emoji (simplified but effective)
_EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # Emoticons
    "\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
    "\U0001F680-\U0001F6FF"  # Transport and Map
    "\U0001F1E0-\U0001F1FF"  # Flags
    "\U0001F900-\U0001F9FF"  # Supplemental Symbols
    "\U0001FA00-\U0001FA6F"  # Chess Symbols
    "\U0001FA70-\U0001FAFF"  # Symbols Extended-A
    "\U00002702-\U000027B0"  # Dingbats
    "\U000024C2-\U0001F251"  # Enclosed characters
    "\U0000FE0F"             # Variation Selector
    "\U00002600-\U000026FF"  # Misc Symbols (â˜€ï¸âš ï¸âš¡ etc.)
    "\U00002700-\U000027BF"  # Dingbats
    "\U0000200D"             # ZWJ
    "\U00002B50"             # Star
    "\U0000203C\U00002049"   # Exclamation marks
    "\U000023E9-\U000023F3"  # Media symbols
    "\U000023F8-\U000023FA"  # Media symbols
    "]+",
    flags=re.UNICODE
)

def _strip_banned_emojis(text: str) -> str:
    """Remove all emojis except ðŸ˜Š and ðŸŒ±"""
    def _replace(match):
        chars = match.group()
        # Keep only allowed emojis from the matched span
        kept = ''.join(c for c in chars if c in _ALLOWED_EMOJIS)
        return kept
    return _EMOJI_PATTERN.sub(_replace, text)


def post_process_answer(answer: str) -> str:
    """Post-process Gemini answer for better quality"""
    if not answer:
        return ""
    
    # 1. Remove markdown formatting
    answer = answer.replace("```", "")
    answer = answer.replace("**", "")
    answer = answer.replace("##", "")
    answer = answer.replace("###", "")
    answer = re.sub(r'\*\*([^*]+)\*\*', r'\1', answer)  # **text** â†’ text
    answer = re.sub(r'\*([^*]+)\*', r'\1', answer)  # *text* â†’ text
    
    # 2. Fix Thai encoding issues
    answer = re.sub(r'([à¸-à¸®])Äž([à¸³à¸´à¸µà¸¸à¸¹à¹€à¹à¹‚à¹ƒà¹„à¹ˆà¹‰à¹Šà¹‹])', r'\1\2', answer)
    answer = answer.replace('Äž', '')
    answer = answer.replace('', '')
    answer = answer.replace('\x00', '')
    
    # 3. Fix spacing issues (preserve newlines!)
    # Only collapse multiple spaces within lines, preserve newlines
    answer = re.sub(r'[ \t]+', ' ', answer)  # Multiple spaces/tabs â†’ single space (preserve \n)
    answer = answer.replace(' ,', ',')
    answer = answer.replace(' .', '.')
    answer = answer.replace(' :', ':')
    answer = answer.replace('( ', '(')
    answer = answer.replace(' )', ')')
    
    # 4. Fix bullet points (convert markdown to Thai style)
    answer = re.sub(r'^\s*[-*]\s+', 'â€¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'\n\s*[-*]\s+', '\nâ€¢ ', answer)
    
    # 5. Ensure proper line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)  # Max 2 line breaks
    
    # 6. Remove leading/trailing whitespace
    answer = answer.strip()
    
    # 7. Fix common Thai typos
    answer = answer.replace('à¸•à¹‰', 'à¸•à¹‰')
    answer = answer.replace('à¸•', 'à¸•')
    
    # 8. Strip all emojis except ðŸ˜Š and ðŸŒ±
    answer = _strip_banned_emojis(answer)

    # 9. Normalize dividers to standard format
    answer = re.sub(r'^[-=â”€]{3,}$', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”', answer, flags=re.MULTILINE)

    # 10. Remove old [à¸«à¸±à¸§à¸‚à¹‰à¸­] bracket format (fallback cleanup)
    answer = re.sub(r'^\[([^\]]+)\]\s*$', r'\1', answer, flags=re.MULTILINE)

    return answer

def clean_knowledge_text(text: str) -> str:
    """Clean and format knowledge text for better readability"""
    if not text:
        return ""
    
    # Fix encoding issues - remove corrupted characters
    # Common patterns: à¸ˆÄžà¸³, à¸¥Äžà¸³, à¸—Äžà¸³, à¸™Äžà¹‰à¸³, à¸Äžà¸³
    text = re.sub(r'([à¸-à¸®])Äž([à¸³])', r'\1\2', text)  # à¸ˆÄžà¸³ â†’ à¸ˆà¸³
    text = re.sub(r'([à¸-à¸®])Äž([à¹‰])', r'\1\2', text)  # à¸™Äžà¹‰ â†’ à¸™à¹‰
    text = re.sub(r'([à¸-à¸®])Äž([à¸´])', r'\1\2', text)  # à¸Äžà¸´ â†’ à¸à¸´
    text = re.sub(r'([à¸-à¸®])Äž([à¸µ])', r'\1\2', text)  # à¸Äžà¸µ â†’ à¸à¸µ
    text = re.sub(r'([à¸-à¸®])Äž([à¸¸])', r'\1\2', text)  # à¸Äžà¸¸ â†’ à¸à¸¸
    text = re.sub(r'([à¸-à¸®])Äž([à¸¹])', r'\1\2', text)  # à¸Äžà¸¹ â†’ à¸à¸¹
    text = re.sub(r'([à¸-à¸®])Äž([à¹ˆ])', r'\1\2', text)  # à¸Äžà¹ˆ â†’ à¸à¹ˆ
    text = re.sub(r'([à¸-à¸®])Äž([à¹‰])', r'\1\2', text)  # à¸Äžà¹‰ â†’ à¸à¹‰
    text = re.sub(r'([à¸-à¸®])Äž([à¹Š])', r'\1\2', text)  # à¸Äžà¹Š â†’ à¸à¹Š
    text = re.sub(r'([à¸-à¸®])Äž([à¹‹])', r'\1\2', text)  # à¸Äžà¹‹ â†’ à¸à¹‹
    text = re.sub(r'Äž', '', text)  # Remove remaining Äž
    
    # Fix other corrupted characters
    text = text.replace('à¸•à¹‰', 'à¸•à¹‰')  # Fix tone marks
    text = text.replace('à¸•', 'à¸•')
    text = text.replace('', '')  # Remove replacement character
    text = text.replace('\x00', '')  # Remove null character
    
    # Fix common Thai encoding issues
    text = text.replace('Ã Â¸', '')  # Remove Thai encoding prefix
    text = text.replace('Ã Â¹', '')  # Remove Thai encoding prefix
    
    # Remove excessive whitespace
    text = ' '.join(text.split())
    
    # Fix common issues
    text = text.replace('  ', ' ')  # Double spaces
    text = text.replace(' ,', ',')  # Space before comma
    text = text.replace(' .', '.')  # Space before period
    text = text.replace('( ', '(')  # Space after opening parenthesis
    text = text.replace(' )', ')')  # Space before closing parenthesis
    text = text.replace(' :', ':')  # Space before colon
    
    # Fix Thai-specific issues (keep important marks)
    # text = text.replace('à¸º', '')  # Keep Thai character above
    # text = text.replace('à¹Œ', '')  # Keep Thai character above
    
    # Remove multiple consecutive spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence spacing
    text = re.sub(r'([.!?])\s*([A-Za-zà¸-à¹™])', r'\1 \2', text)
    
    # Remove leading/trailing whitespace
    text = text.strip()
    
    # Remove lines with only special characters
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if line.strip() and not re.match(r'^[^\w\s]+$', line.strip())]
    text = '\n'.join(cleaned_lines)
    
    return text

def extract_keywords_from_question(question: str) -> dict:
    """Extract main keywords from question with categories"""
    question_lower = question.lower()
    # Normalize simple punctuation to spaces for better matching
    question_norm = re.sub(r'[\.,\?\!\:\;\(\)\/]',' ', question_lower)
    
    # Pest/Disease keywords (expanded)
    pest_keywords = [
        # Thai - à¹à¸¡à¸¥à¸‡
        "à¹€à¸žà¸¥à¸µà¹‰à¸¢à¹„à¸Ÿ", "à¹€à¸žà¸¥à¸µà¹‰à¸¢à¸­à¹ˆà¸­à¸™", "à¹€à¸žà¸¥à¸µà¹‰à¸¢", "à¸«à¸™à¸­à¸™", "à¹à¸¡à¸¥à¸‡", "à¸”à¹‰à¸§à¸‡à¸‡à¸§à¸‡",
        "à¸ˆà¸±à¸à¸ˆà¸±à¹ˆà¸™", "à¸«à¸™à¸­à¸™à¹€à¸ˆà¸²à¸°", "à¸«à¸™à¸­à¸™à¸à¸­", "à¸«à¸™à¸­à¸™à¹ƒà¸¢", "à¸”à¹‰à¸§à¸‡", "à¸¡à¸”", "à¸›à¸¥à¸§à¸",
        "à¹€à¸žà¸¥à¸µà¹‰à¸¢à¸ˆà¸±à¸à¸ˆà¸±à¹ˆà¸™", "à¹à¸¡à¸¥à¸‡à¸§à¸±à¸™à¸œà¸¥", "à¹à¸¡à¸¥à¸‡à¸«à¸§à¸µà¹ˆà¸‚à¸²à¸§", "à¸—à¸£à¸´à¸›à¸ªà¹Œ",
        "à¸¨à¸±à¸•à¸£à¸¹à¸žà¸·à¸Š", "à¹„à¸£", "à¹€à¸žà¸¥à¸µà¹‰à¸¢à¹à¸›à¹‰à¸‡", "à¸«à¸™à¸­à¸™à¸à¸£à¸°à¸—à¸¹à¹‰à¸‚à¹‰à¸²à¸§",
        # Thai - à¹‚à¸£à¸„à¹€à¸Šà¸·à¹‰à¸­à¸£à¸²
        "à¸£à¸²à¸™à¹‰à¸³à¸„à¹‰à¸²à¸‡", "à¸£à¸²à¹à¸›à¹‰à¸‡", "à¸£à¸²à¸ªà¸™à¸´à¸¡", "à¹€à¸Šà¸·à¹‰à¸­à¸£à¸²", "à¸£à¸²", "à¹à¸­à¸™à¹à¸—à¸£à¸„à¹‚à¸™à¸ª",
        "à¹‚à¸£à¸„à¸œà¸¥à¹€à¸™à¹ˆà¸²", "à¸œà¸¥à¹€à¸™à¹ˆà¸²", "à¹‚à¸£à¸„à¹ƒà¸šà¹„à¸«à¸¡à¹‰", "à¹ƒà¸šà¹„à¸«à¸¡à¹‰", "à¹‚à¸£à¸„à¸£à¸²à¸”à¸³", "à¸£à¸²à¸”à¸³",
        "à¹‚à¸£à¸„à¹ƒà¸šà¸ˆà¸¸à¸”", "à¹ƒà¸šà¸ˆà¸¸à¸”", "à¹‚à¸£à¸„à¸à¸´à¹ˆà¸‡à¹à¸«à¹‰à¸‡", "à¸à¸´à¹ˆà¸‡à¹à¸«à¹‰à¸‡", "à¹‚à¸£à¸„à¸£à¸²à¸à¹€à¸™à¹ˆà¸²", "à¸£à¸²à¸à¹€à¸™à¹ˆà¸²",
        "à¹‚à¸£à¸„à¸¥à¸³à¸•à¹‰à¸™à¹€à¸™à¹ˆà¸²", "à¸¥à¸³à¸•à¹‰à¸™à¹€à¸™à¹ˆà¸²", "à¹‚à¸£à¸„à¹‚à¸„à¸™à¹€à¸™à¹ˆà¸²", "à¹‚à¸„à¸™à¹€à¸™à¹ˆà¸²",
        # Thai - à¹„à¸§à¸£à¸±à¸ª
        "à¹„à¸§à¸£à¸±à¸ª", "à¹‚à¸£à¸„à¹ƒà¸šà¸”à¹ˆà¸²à¸‡", "à¹‚à¸£à¸„à¹ƒà¸šà¸«à¸‡à¸´à¸", "à¹‚à¸£à¸„à¸ˆà¸¹à¹‹", "à¹‚à¸£à¸„à¹€à¸«à¸¥à¸·à¸­à¸‡",
        # Thai - à¸§à¸±à¸Šà¸žà¸·à¸Š
        "à¸§à¸±à¸Šà¸žà¸·à¸Š", "à¸«à¸à¹‰à¸²", "à¸œà¸±à¸à¸šà¸¸à¹‰à¸‡", "à¸«à¸à¹‰à¸²à¸„à¸²",
        # Thai - à¸—à¸±à¹ˆà¸§à¹„à¸›
        "à¹‚à¸£à¸„à¸žà¸·à¸Š", "à¹‚à¸£à¸„",
        # English
        "aphid", "thrips", "whitefly", "moth", "caterpillar", "worm", "beetle",
        "mildew", "powdery mildew", "rust", "fungus", "fungal", "anthracnose",
        "virus", "viral", "disease", "weed", "grass", "mite", "borer", "leaf miner",
        "insect", "pest", "armyworm", "thrips", "fruit rot", "root rot", "stem rot"
    ]
    
    # Crop keywords (expanded)
    crop_keywords = [
        # Thai
        "à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™", "à¸¡à¸°à¸¡à¹ˆà¸§à¸‡", "à¸‚à¹‰à¸²à¸§", "à¸žà¸·à¸Šà¸œà¸±à¸", "à¸œà¸±à¸", "à¸œà¸¥à¹„à¸¡à¹‰",
        "à¸¡à¸°à¸™à¸²à¸§", "à¸ªà¹‰à¸¡", "à¸à¸¥à¹‰à¸§à¸¢", "à¸¡à¸°à¸žà¸£à¹‰à¸²à¸§", "à¸¢à¸²à¸‡à¸žà¸²à¸£à¸²", "à¸›à¸²à¸¥à¹Œà¸¡",
        "à¸‚à¹‰à¸²à¸§à¹‚à¸žà¸”", "à¸­à¹‰à¸­à¸¢", "à¸¡à¸±à¸™à¸ªà¸³à¸›à¸°à¸«à¸¥à¸±à¸‡", "à¸–à¸±à¹ˆà¸§", "à¸žà¸£à¸´à¸", "à¸¡à¸°à¹€à¸‚à¸·à¸­à¹€à¸—à¸¨",
        "à¸¥à¸³à¹„à¸¢", "à¸¥à¸´à¹‰à¸™à¸ˆà¸µà¹ˆ", "à¹€à¸‡à¸²à¸°", "à¸¡à¸±à¸‡à¸„à¸¸à¸”", "à¸à¸£à¸±à¹ˆà¸‡", "à¸Šà¸¡à¸žà¸¹à¹ˆ",
        # English
        "durian", "mango", "rice", "vegetable", "vegetables", "fruit",
        "lime", "orange", "banana", "coconut", "rubber", "palm",
        "corn", "sugarcane", "cassava", "peanut", "chilli", "tomato",
        "longan", "lychee", "rambutan", "mangosteen", "guava"
    ]
    
    # Product-related keywords
    product_keywords = [
        # Thai
        "à¸œà¸¥à¸´à¸•à¸ à¸±à¸“à¸‘à¹Œ", "à¸ªà¸´à¸™à¸„à¹‰à¸²", "à¸¢à¸²", "à¸ªà¸²à¸£", "à¸›à¸¸à¹‹à¸¢",
        "icp", "ladda", "icpl", "à¹„à¸­à¸‹à¸µà¸žà¸µ", "à¸¥à¸±à¸”à¸”à¸²",
        "à¹‚à¸¡à¹€à¸”à¸´à¸™", "à¹„à¸”à¸­à¸°à¸‹à¸´à¸™à¸­à¸™", "à¸­à¸´à¸¡à¸´à¸”à¸²à¹‚à¸„à¸¥à¸žà¸£à¸´à¸”", "à¹„à¸‹à¹€à¸žà¸­à¸£à¹Œà¹€à¸¡à¸—à¸£à¸´à¸™",
        "à¹à¸™à¸°à¸™à¸³", "à¹ƒà¸Šà¹‰", "à¸žà¹ˆà¸™", "à¸‰à¸µà¸”", "à¸à¸³à¸ˆà¸±à¸”", "à¸›à¹‰à¸­à¸‡à¸à¸±à¸™",
        # English
        "product", "products", "fertilizer", "pesticide", "insecticide", "fungicide", "recommend"
    ]

    # Fertilizer-specific keywords (NEW)
    fertilizer_keywords = [
        # à¸›à¸£à¸°à¹€à¸ à¸—à¸›à¸¸à¹‹à¸¢/à¸ªà¸²à¸£à¸šà¸³à¸£à¸¸à¸‡
        "à¸›à¸¸à¹‹à¸¢", "à¸ªà¸²à¸£à¸šà¸³à¸£à¸¸à¸‡", "à¸˜à¸²à¸•à¸¸à¸­à¸²à¸«à¸²à¸£", "à¸®à¸­à¸£à¹Œà¹‚à¸¡à¸™", "à¸ªà¸²à¸£à¹€à¸£à¹ˆà¸‡",
        "à¸›à¸¸à¹‹à¸¢à¹€à¸„à¸¡à¸µ", "à¸›à¸¸à¹‹à¸¢à¸­à¸´à¸™à¸—à¸£à¸µà¸¢à¹Œ", "à¸›à¸¸à¹‹à¸¢à¸Šà¸µà¸§à¸ à¸²à¸ž",
        # à¸Šà¸·à¹ˆà¸­à¸ªà¸´à¸™à¸„à¹‰à¸²à¸›à¸¸à¹‹à¸¢ ICP (à¸ˆà¸²à¸ CSV)
        "à¸à¸£à¸°à¸£à¸±à¸•", "à¸à¸²à¸£à¸¹à¸à¹‰à¸²", "à¸„à¸­à¸™à¸—à¸²à¸Ÿ", "à¸‹à¸­à¸¢à¸šà¸­à¸¡", "à¸‹à¸µà¹€à¸­à¹‡à¸¡à¸ˆà¸µ",
        "à¸—à¹Šà¸­à¸›à¸à¸±à¸™", "à¸žà¸²à¸™à¸²à¸ª", "à¸£à¸²à¹€à¸‹à¸­à¸£à¹Œ", "à¹€à¸§à¸„à¹€à¸•à¸­à¸£à¹Œ", "à¸£à¸µà¹‚à¸™à¹€à¸§à¸—",
        "à¸­à¸´à¸¡à¸´à¸”à¸²à¹‚à¸à¸¥à¸”à¹Œ", "à¹€à¸à¸£à¸„", "à¹€à¸—à¸­à¸£à¸²à¹‚à¸™", "à¹€à¸¡à¸—à¸²à¸¡à¸­à¸£à¹Œà¸›", "à¹à¸¡à¸ªà¸Ÿà¸­à¸£à¹Œà¸”",
        "à¹à¸­à¸™à¸”à¸²à¹à¸¡à¹‡à¸à¸‹à¹Œ", "à¹à¸­à¹‡à¸„à¸™à¸²à¸§", "à¹‚à¸®à¸›", "à¹„à¸à¸¥à¹‚à¸Ÿà¹€à¸ªà¸—", "à¸­à¸±à¸žà¸”à¸²à¸§",
        "à¹„à¸”à¸žà¸´à¸¡", "à¹„à¸®à¸‹à¸µà¸ª", "à¸šà¸­à¸¡à¸ªà¹Œ", "à¸žà¸£à¸µà¸”à¸´à¸„à¸—à¹Œ", "à¸§à¸­à¹à¸£à¸™à¸”à¹Œ",
        "à¸­à¸´à¸™à¹€à¸™à¸­à¸£à¹Œ", "à¹€à¸šà¸™à¸‹à¸²à¸™à¹ˆà¸²", "à¹à¸ˆà¹Šà¸ª", "à¸™à¸²à¹à¸”à¸™", "à¸„à¸²à¸‹à¹ˆà¸²",
        "à¸‹à¸´à¸¡à¹€à¸¡à¸­à¸£à¹Œ", "à¸­à¸²à¸—à¸£à¸²à¸‹à¸µà¸™", "à¸„à¸²à¸£à¸´à¸ªà¸¡à¸²",
        # à¸›à¸£à¸°à¹€à¸ à¸—à¸ªà¸²à¸£à¹€à¸„à¸¡à¸µ
        "à¸ªà¸²à¸£à¸à¸³à¸ˆà¸±à¸”à¹à¸¡à¸¥à¸‡", "à¸ªà¸²à¸£à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¹‚à¸£à¸„", "à¸ªà¸²à¸£à¸à¸³à¸ˆà¸±à¸”à¸§à¸±à¸Šà¸žà¸·à¸Š",
        "à¸¢à¸²à¸†à¹ˆà¸²à¹à¸¡à¸¥à¸‡", "à¸¢à¸²à¸†à¹ˆà¸²à¸«à¸à¹‰à¸²", "à¸¢à¸²à¸†à¹ˆà¸²à¹€à¸Šà¸·à¹‰à¸­à¸£à¸²",
        # à¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸›à¸¸à¹‹à¸¢
        "à¸­à¸±à¸•à¸£à¸²à¹ƒà¸Šà¹‰", "à¸­à¸±à¸•à¸£à¸²à¸œà¸ªà¸¡", "à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰à¸›à¸¸à¹‹à¸¢", "à¹ƒà¸ªà¹ˆà¸›à¸¸à¹‹à¸¢",
        # English
        "fertilizer", "nutrient", "hormone", "chemical"
    ]
    
    # Intent keywords (NEW)
    intent_keywords = {
        "increase_yield": [
            # Thai
            "à¹€à¸žà¸´à¹ˆà¸¡à¸œà¸¥à¸œà¸¥à¸´à¸•", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¸ªà¸¹à¸‡", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¸¡à¸²à¸", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¸”à¸µ", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸¢à¸­à¸°", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¸‚à¸¶à¹‰à¸™", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¸”à¸µà¸‚à¸¶à¹‰à¸™", "à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸žà¸´à¹ˆà¸¡",
            # English
            "increase yield", "higher yield", "more yield", "increase production", "boost yield", "increase harvest"
        ],
        "solve_problem": [
            # Thai
            "à¹à¸à¹‰à¸›à¸±à¸à¸«à¸²", "à¹à¸à¹‰à¹„à¸‚", "à¸£à¸±à¸à¸©à¸²", "à¸à¸³à¸ˆà¸±à¸”", "à¸›à¹‰à¸­à¸‡à¸à¸±à¸™", "à¸„à¸§à¸šà¸„à¸¸à¸¡", "à¹à¸à¹‰à¹‚à¸£à¸„", "à¹à¸à¹‰",
            # English
            "solve problem", "control", "kill", "manage pest", "prevent", "control pest", "treat"
        ],
        "general_care": [
            # Thai
            "à¸”à¸¹à¹à¸¥", "à¸šà¸³à¸£à¸¸à¸‡", "à¹€à¸¥à¸µà¹‰à¸¢à¸‡", "à¸›à¸¥à¸¹à¸", "à¹ƒà¸ªà¹ˆà¸›à¸¸à¹‹à¸¢",
            # English
            "care", "fertilize", "general care", "maintenance", "nurture"
        ],
        "product_inquiry": [
            # Thai - à¹€à¸žà¸´à¹ˆà¸¡ patterns à¸ªà¸³à¸«à¸£à¸±à¸šà¸–à¸²à¸¡à¸«à¸²à¸ªà¸´à¸™à¸„à¹‰à¸²
            "à¸¡à¸µà¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡", "à¸¡à¸µà¹„à¸«à¸¡", "à¹à¸™à¸°à¸™à¸³", "à¸„à¸§à¸£à¹ƒà¸Šà¹‰", "à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£", "à¸‹à¸·à¹‰à¸­",
            "à¸•à¸±à¸§à¹„à¸«à¸™", "à¸¢à¸²à¸­à¸°à¹„à¸£", "à¹ƒà¸Šà¹‰à¸•à¸±à¸§à¹„à¸«à¸™", "à¹ƒà¸Šà¹‰à¸¢à¸²à¸­à¸°à¹„à¸£", "à¸žà¹ˆà¸™à¸­à¸°à¹„à¸£", "à¸‰à¸µà¸”à¸­à¸°à¹„à¸£",
            "à¸ªà¸²à¸£à¸­à¸°à¹„à¸£", "à¹ƒà¸Šà¹‰à¸ªà¸²à¸£à¸­à¸°à¹„à¸£", "à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£à¸”à¸µ", "à¸•à¸±à¸§à¹„à¸«à¸™à¸”à¸µ", "à¸¢à¸²à¸•à¸±à¸§à¹„à¸«à¸™",
            "à¸¡à¸µà¸¢à¸²à¸­à¸°à¹„à¸£", "à¸¡à¸µà¸•à¸±à¸§à¹„à¸«à¸™", "à¹„à¸”à¹‰à¸šà¹‰à¸²à¸‡", "à¸šà¹‰à¸²à¸‡",
            # English
            "what products", "what is available", "recommend product", "recommend", "what to use", "is there"
        ]
    }
    
    found = {
        "pests": [],
        "crops": [],
        "products": [],
        "fertilizers": [],  # NEW: fertilizer keywords
        "intent": None,  # NEW: detect user intent
        "is_product_query": False,
        "is_fertilizer_query": False  # NEW: flag for fertilizer questions
    }
    
    # Extract pests
    for keyword in pest_keywords:
        if keyword in question_norm:
            found["pests"].append(keyword)
    
    # Extract crops
    for keyword in crop_keywords:
        if keyword in question_norm:
            found["crops"].append(keyword)
    
    # Extract product-related
    for keyword in product_keywords:
        if keyword in question_norm:
            found["products"].append(keyword)
            found["is_product_query"] = True

    # Extract fertilizer-related (NEW)
    for keyword in fertilizer_keywords:
        if keyword in question_norm:
            found["fertilizers"].append(keyword)
            found["is_fertilizer_query"] = True
            found["is_product_query"] = True

    # Detect intent (NEW)
    # Detect intent (MATCH ON NORMALIZED TEXT)
    for intent, keywords in intent_keywords.items():
        for keyword in keywords:
            if keyword in question_norm:
                found["intent"] = intent
                found["is_product_query"] = True
                break
        if found["intent"]:
            break
    
    return found


def generate_thai_disease_variants(disease_name: str) -> List[str]:
    """
    Generate Thai disease name variants for fuzzy matching.

    Thai farmers commonly drop "à¸ªà¸µ" (color) from disease names:
      à¸£à¸²à¸ªà¸µà¸Šà¸¡à¸žà¸¹ â†’ à¸£à¸²à¸Šà¸¡à¸žà¸¹, à¸ˆà¸¸à¸”à¸ªà¸µà¸™à¹‰à¸³à¸•à¸²à¸¥ â†’ à¸ˆà¸¸à¸”à¸™à¹‰à¸³à¸•à¸²à¸¥

    Also handles à¹‚à¸£à¸„ prefix add/remove.

    Returns list of variants including the original name.
    """
    variants = {disease_name}

    # Strip à¹‚à¸£à¸„ prefix for matching, but also keep variant with it
    bare = disease_name
    if bare.startswith("à¹‚à¸£à¸„"):
        bare = bare[len("à¹‚à¸£à¸„"):]
        variants.add(bare)
    else:
        variants.add("à¹‚à¸£à¸„" + bare)

    _COLORS = ["à¸Šà¸¡à¸žà¸¹", "à¸™à¹‰à¸³à¸•à¸²à¸¥", "à¹€à¸—à¸²", "à¸‚à¸²à¸§", "à¸”à¸³", "à¸¡à¹ˆà¸§à¸‡", "à¹€à¸«à¸¥à¸·à¸­à¸‡", "à¸ªà¹‰à¸¡"]

    for color in _COLORS:
        # à¸£à¸²+à¸ªà¸µ+color â†” à¸£à¸²+color
        with_si = f"à¸£à¸²à¸ªà¸µ{color}"
        without_si = f"à¸£à¸²{color}"
        if with_si in bare or without_si in bare:
            variants.add(bare.replace(with_si, without_si))
            variants.add(bare.replace(without_si, with_si))
            variants.add("à¹‚à¸£à¸„" + bare.replace(with_si, without_si))
            variants.add("à¹‚à¸£à¸„" + bare.replace(without_si, with_si))

        # à¸ˆà¸¸à¸”+à¸ªà¸µ+color â†” à¸ˆà¸¸à¸”+color
        jud_with_si = f"à¸ˆà¸¸à¸”à¸ªà¸µ{color}"
        jud_without_si = f"à¸ˆà¸¸à¸”{color}"
        if jud_with_si in bare or jud_without_si in bare:
            variants.add(bare.replace(jud_with_si, jud_without_si))
            variants.add(bare.replace(jud_without_si, jud_with_si))
            variants.add("à¹‚à¸£à¸„" + bare.replace(jud_with_si, jud_without_si))
            variants.add("à¹‚à¸£à¸„" + bare.replace(jud_without_si, jud_with_si))

    return list(variants)
