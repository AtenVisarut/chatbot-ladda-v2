import re
from typing import List, Dict

# Allowed emojis: üòä (U+1F60A) and üå± (U+1F331)
_ALLOWED_EMOJIS = {'\U0001F60A', '\U0001F331'}

# Unicode ranges covering most emoji (simplified but effective)
_EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # Emoticons
    "\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
    "\U0001F680-\U0001F6FF"  # Transport and Map
    "\U0001F1E0-\U0001F1FF"  # Flags
    "\U0001F900-\U0001F9FF"  # Supplemental Symbols
    "\U0001FA00-\U0001FA6F"  # Chess Symbols
    "\U0001FA70-\U0001FAFF"  # Symbols Extended-A
    "\U00002702-\U000027B0"  # Dingbats
    "\U000024C2-\U0001F251"  # Enclosed characters
    "\U0000FE0F"             # Variation Selector
    "\U00002600-\U000026FF"  # Misc Symbols (‚òÄÔ∏è‚ö†Ô∏è‚ö° etc.)
    "\U00002700-\U000027BF"  # Dingbats
    "\U0000200D"             # ZWJ
    "\U00002B50"             # Star
    "\U0000203C\U00002049"   # Exclamation marks
    "\U000023E9-\U000023F3"  # Media symbols
    "\U000023F8-\U000023FA"  # Media symbols
    "]+",
    flags=re.UNICODE
)

def _strip_banned_emojis(text: str) -> str:
    """Remove all emojis except üòä and üå±"""
    def _replace(match):
        chars = match.group()
        # Keep only allowed emojis from the matched span
        kept = ''.join(c for c in chars if c in _ALLOWED_EMOJIS)
        return kept
    return _EMOJI_PATTERN.sub(_replace, text)


def post_process_answer(answer: str) -> str:
    """Post-process Gemini answer for better quality"""
    if not answer:
        return ""
    
    # 1. Remove markdown formatting
    answer = answer.replace("```", "")
    answer = answer.replace("**", "")
    answer = answer.replace("##", "")
    answer = answer.replace("###", "")
    answer = re.sub(r'\*\*([^*]+)\*\*', r'\1', answer)  # **text** ‚Üí text
    answer = re.sub(r'\*([^*]+)\*', r'\1', answer)  # *text* ‚Üí text
    
    # 2. Fix Thai encoding issues
    answer = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥‡∏¥‡∏µ‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πà‡πâ‡πä‡πã])', r'\1\2', answer)
    answer = answer.replace('ƒû', '')
    answer = answer.replace('', '')
    answer = answer.replace('\x00', '')
    
    # 3. Fix spacing issues (preserve newlines!)
    # Only collapse multiple spaces within lines, preserve newlines
    answer = re.sub(r'[ \t]+', ' ', answer)  # Multiple spaces/tabs ‚Üí single space (preserve \n)
    answer = answer.replace(' ,', ',')
    answer = answer.replace(' .', '.')
    answer = answer.replace(' :', ':')
    answer = answer.replace('( ', '(')
    answer = answer.replace(' )', ')')
    
    # 4. Fix bullet points (convert markdown to Thai style)
    answer = re.sub(r'^\s*[-*]\s+', '‚Ä¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'\n\s*[-*]\s+', '\n‚Ä¢ ', answer)
    
    # 5. Ensure proper line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)  # Max 2 line breaks
    
    # 6. Remove leading/trailing whitespace
    answer = answer.strip()
    
    # 7. Fix common Thai typos
    answer = answer.replace('‡∏ï‡πâ', '‡∏ï‡πâ')
    answer = answer.replace('‡∏ï', '‡∏ï')
    
    # 8. Strip all emojis except üòä and üå±
    answer = _strip_banned_emojis(answer)

    # 9. Normalize dividers to standard format
    answer = re.sub(r'^[-=‚îÄ]{3,}$', '‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ', answer, flags=re.MULTILINE)

    # 10. Remove old [‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠] bracket format (fallback cleanup)
    answer = re.sub(r'^\[([^\]]+)\]\s*$', r'\1', answer, flags=re.MULTILINE)

    return answer

def clean_knowledge_text(text: str) -> str:
    """Clean and format knowledge text for better readability"""
    if not text:
        return ""
    
    # Fix encoding issues - remove corrupted characters
    # Common patterns: ‡∏àƒû‡∏≥, ‡∏•ƒû‡∏≥, ‡∏óƒû‡∏≥, ‡∏ôƒû‡πâ‡∏≥, ‡∏Åƒû‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥])', r'\1\2', text)  # ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏ôƒû‡πâ ‚Üí ‡∏ô‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏¥])', r'\1\2', text)  # ‡∏Åƒû‡∏¥ ‚Üí ‡∏Å‡∏¥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏µ])', r'\1\2', text)  # ‡∏Åƒû‡∏µ ‚Üí ‡∏Å‡∏µ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏∏])', r'\1\2', text)  # ‡∏Åƒû‡∏∏ ‚Üí ‡∏Å‡∏∏
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏π])', r'\1\2', text)  # ‡∏Åƒû‡∏π ‚Üí ‡∏Å‡∏π
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πà])', r'\1\2', text)  # ‡∏Åƒû‡πà ‚Üí ‡∏Å‡πà
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏Åƒû‡πâ ‚Üí ‡∏Å‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πä])', r'\1\2', text)  # ‡∏Åƒû‡πä ‚Üí ‡∏Å‡πä
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πã])', r'\1\2', text)  # ‡∏Åƒû‡πã ‚Üí ‡∏Å‡πã
    text = re.sub(r'ƒû', '', text)  # Remove remaining ƒû
    
    # Fix other corrupted characters
    text = text.replace('‡∏ï‡πâ', '‡∏ï‡πâ')  # Fix tone marks
    text = text.replace('‡∏ï', '‡∏ï')
    text = text.replace('', '')  # Remove replacement character
    text = text.replace('\x00', '')  # Remove null character
    
    # Fix common Thai encoding issues
    text = text.replace('√†¬∏', '')  # Remove Thai encoding prefix
    text = text.replace('√†¬π', '')  # Remove Thai encoding prefix
    
    # Remove excessive whitespace
    text = ' '.join(text.split())
    
    # Fix common issues
    text = text.replace('  ', ' ')  # Double spaces
    text = text.replace(' ,', ',')  # Space before comma
    text = text.replace(' .', '.')  # Space before period
    text = text.replace('( ', '(')  # Space after opening parenthesis
    text = text.replace(' )', ')')  # Space before closing parenthesis
    text = text.replace(' :', ':')  # Space before colon
    
    # Fix Thai-specific issues (keep important marks)
    # text = text.replace('‡∏∫', '')  # Keep Thai character above
    # text = text.replace('‡πå', '')  # Keep Thai character above
    
    # Remove multiple consecutive spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence spacing
    text = re.sub(r'([.!?])\s*([A-Za-z‡∏Å-‡πô])', r'\1 \2', text)
    
    # Remove leading/trailing whitespace
    text = text.strip()
    
    # Remove lines with only special characters
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if line.strip() and not re.match(r'^[^\w\s]+$', line.strip())]
    text = '\n'.join(cleaned_lines)
    
    return text

def extract_keywords_from_question(question: str) -> dict:
    """Extract main keywords from question with categories"""
    question_lower = question.lower()
    # Normalize simple punctuation to spaces for better matching
    question_norm = re.sub(r'[\.,\?\!\:\;\(\)\/]',' ', question_lower)
    
    # Pest/Disease keywords (expanded)
    pest_keywords = [
        # Thai - ‡πÅ‡∏°‡∏•‡∏á
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏≠‡πà‡∏≠‡∏ô", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢", "‡∏´‡∏ô‡∏≠‡∏ô", "‡πÅ‡∏°‡∏•‡∏á", "‡∏î‡πâ‡∏ß‡∏á‡∏á‡∏ß‡∏á",
        "‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡∏´‡∏ô‡∏≠‡∏ô‡πÄ‡∏à‡∏≤‡∏∞", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏≠", "‡∏´‡∏ô‡∏≠‡∏ô‡πÉ‡∏¢", "‡∏î‡πâ‡∏ß‡∏á", "‡∏°‡∏î", "‡∏õ‡∏•‡∏ß‡∏Å",
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡πÅ‡∏°‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ú‡∏•", "‡πÅ‡∏°‡∏•‡∏á‡∏´‡∏ß‡∏µ‡πà‡∏Ç‡∏≤‡∏ß", "‡∏ó‡∏£‡∏¥‡∏õ‡∏™‡πå",
        "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä", "‡πÑ‡∏£", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÅ‡∏õ‡πâ‡∏á", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏Ç‡πâ‡∏≤‡∏ß",
        # Thai - ‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤
        "‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á", "‡∏£‡∏≤‡πÅ‡∏õ‡πâ‡∏á", "‡∏£‡∏≤‡∏™‡∏ô‡∏¥‡∏°", "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡∏£‡∏≤", "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÇ‡∏£‡∏Ñ‡∏ú‡∏•‡πÄ‡∏ô‡πà‡∏≤", "‡∏ú‡∏•‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ", "‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ", "‡πÇ‡∏£‡∏Ñ‡∏£‡∏≤‡∏î‡∏≥", "‡∏£‡∏≤‡∏î‡∏≥",
        "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏à‡∏∏‡∏î", "‡πÉ‡∏ö‡∏à‡∏∏‡∏î", "‡πÇ‡∏£‡∏Ñ‡∏Å‡∏¥‡πà‡∏á‡πÅ‡∏´‡πâ‡∏á", "‡∏Å‡∏¥‡πà‡∏á‡πÅ‡∏´‡πâ‡∏á", "‡πÇ‡∏£‡∏Ñ‡∏£‡∏≤‡∏Å‡πÄ‡∏ô‡πà‡∏≤", "‡∏£‡∏≤‡∏Å‡πÄ‡∏ô‡πà‡∏≤",
        "‡πÇ‡∏£‡∏Ñ‡∏•‡∏≥‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡∏•‡∏≥‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏£‡∏Ñ‡πÇ‡∏Ñ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏Ñ‡∏ô‡πÄ‡∏ô‡πà‡∏≤",
        # Thai - ‡πÑ‡∏ß‡∏£‡∏±‡∏™
        "‡πÑ‡∏ß‡∏£‡∏±‡∏™", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å", "‡πÇ‡∏£‡∏Ñ‡∏à‡∏π‡πã", "‡πÇ‡∏£‡∏Ñ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á",
        # Thai - ‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä
        "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤", "‡∏ú‡∏±‡∏Å‡∏ö‡∏∏‡πâ‡∏á", "‡∏´‡∏ç‡πâ‡∏≤‡∏Ñ‡∏≤",
        # Thai - ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä", "‡πÇ‡∏£‡∏Ñ",
        # English
        "aphid", "thrips", "whitefly", "moth", "caterpillar", "worm", "beetle",
        "mildew", "powdery mildew", "rust", "fungus", "fungal", "anthracnose",
        "virus", "viral", "disease", "weed", "grass", "mite", "borer", "leaf miner",
        "insect", "pest", "armyworm", "thrips", "fruit rot", "root rot", "stem rot"
    ]
    
    # Crop keywords (expanded)
    crop_keywords = [
        # Thai
        "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏û‡∏∑‡∏ä‡∏ú‡∏±‡∏Å", "‡∏ú‡∏±‡∏Å", "‡∏ú‡∏•‡πÑ‡∏°‡πâ",
        "‡∏°‡∏∞‡∏ô‡∏≤‡∏ß", "‡∏™‡πâ‡∏°", "‡∏Å‡∏•‡πâ‡∏ß‡∏¢", "‡∏°‡∏∞‡∏û‡∏£‡πâ‡∏≤‡∏ß", "‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤", "‡∏õ‡∏≤‡∏•‡πå‡∏°",
        "‡∏Ç‡πâ‡∏≤‡∏ß‡πÇ‡∏û‡∏î", "‡∏≠‡πâ‡∏≠‡∏¢", "‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á", "‡∏ñ‡∏±‡πà‡∏ß", "‡∏û‡∏£‡∏¥‡∏Å", "‡∏°‡∏∞‡πÄ‡∏Ç‡∏∑‡∏≠‡πÄ‡∏ó‡∏®",
        "‡∏•‡∏≥‡πÑ‡∏¢", "‡∏•‡∏¥‡πâ‡∏ô‡∏à‡∏µ‡πà", "‡πÄ‡∏á‡∏≤‡∏∞", "‡∏°‡∏±‡∏á‡∏Ñ‡∏∏‡∏î", "‡∏ù‡∏£‡∏±‡πà‡∏á", "‡∏ä‡∏°‡∏û‡∏π‡πà",
        # English
        "durian", "mango", "rice", "vegetable", "vegetables", "fruit",
        "lime", "orange", "banana", "coconut", "rubber", "palm",
        "corn", "sugarcane", "cassava", "peanut", "chilli", "tomato",
        "longan", "lychee", "rambutan", "mangosteen", "guava"
    ]
    
    # Product-related keywords
    product_keywords = [
        # Thai
        "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏¢‡∏≤", "‡∏™‡∏≤‡∏£", "‡∏õ‡∏∏‡πã‡∏¢",
        "icp", "ladda", "icpl", "‡πÑ‡∏≠‡∏ã‡∏µ‡∏û‡∏µ", "‡∏•‡∏±‡∏î‡∏î‡∏≤",
        "‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏ô", "‡πÑ‡∏î‡∏≠‡∏∞‡∏ã‡∏¥‡∏ô‡∏≠‡∏ô", "‡∏≠‡∏¥‡∏°‡∏¥‡∏î‡∏≤‡πÇ‡∏Ñ‡∏•‡∏û‡∏£‡∏¥‡∏î", "‡πÑ‡∏ã‡πÄ‡∏û‡∏≠‡∏£‡πå‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏ô",
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡πÉ‡∏ä‡πâ", "‡∏û‡πà‡∏ô", "‡∏â‡∏µ‡∏î", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô",
        # English
        "product", "products", "fertilizer", "pesticide", "insecticide", "fungicide", "recommend"
    ]

    # Fertilizer-specific keywords (NEW)
    fertilizer_keywords = [
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏∏‡πã‡∏¢/‡∏™‡∏≤‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á
        "‡∏õ‡∏∏‡πã‡∏¢", "‡∏™‡∏≤‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á", "‡∏ò‡∏≤‡∏ï‡∏∏‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô", "‡∏™‡∏≤‡∏£‡πÄ‡∏£‡πà‡∏á",
        "‡∏õ‡∏∏‡πã‡∏¢‡πÄ‡∏Ñ‡∏°‡∏µ", "‡∏õ‡∏∏‡πã‡∏¢‡∏≠‡∏¥‡∏ô‡∏ó‡∏£‡∏µ‡∏¢‡πå", "‡∏õ‡∏∏‡πã‡∏¢‡∏ä‡∏µ‡∏ß‡∏†‡∏≤‡∏û",
        # ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏õ‡∏∏‡πã‡∏¢ ICP (‡∏à‡∏≤‡∏Å CSV)
        "‡∏Å‡∏£‡∏∞‡∏£‡∏±‡∏ï", "‡∏Å‡∏≤‡∏£‡∏π‡∏Å‡πâ‡∏≤", "‡∏Ñ‡∏≠‡∏ô‡∏ó‡∏≤‡∏ü", "‡∏ã‡∏≠‡∏¢‡∏ö‡∏≠‡∏°", "‡∏ã‡∏µ‡πÄ‡∏≠‡πá‡∏°‡∏à‡∏µ",
        "‡∏ó‡πä‡∏≠‡∏õ‡∏Å‡∏±‡∏ô", "‡∏û‡∏≤‡∏ô‡∏≤‡∏™", "‡∏£‡∏≤‡πÄ‡∏ã‡∏≠‡∏£‡πå", "‡πÄ‡∏ß‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏£‡∏µ‡πÇ‡∏ô‡πÄ‡∏ß‡∏ó",
        "‡∏≠‡∏¥‡∏°‡∏¥‡∏î‡∏≤‡πÇ‡∏Å‡∏•‡∏î‡πå", "‡πÄ‡∏Å‡∏£‡∏Ñ", "‡πÄ‡∏ó‡∏≠‡∏£‡∏≤‡πÇ‡∏ô", "‡πÄ‡∏°‡∏ó‡∏≤‡∏°‡∏≠‡∏£‡πå‡∏õ", "‡πÅ‡∏°‡∏™‡∏ü‡∏≠‡∏£‡πå‡∏î",
        "‡πÅ‡∏≠‡∏ô‡∏î‡∏≤‡πÅ‡∏°‡πá‡∏Å‡∏ã‡πå", "‡πÅ‡∏≠‡πá‡∏Ñ‡∏ô‡∏≤‡∏ß", "‡πÇ‡∏Æ‡∏õ", "‡πÑ‡∏Å‡∏•‡πÇ‡∏ü‡πÄ‡∏™‡∏ó", "‡∏≠‡∏±‡∏û‡∏î‡∏≤‡∏ß",
        "‡πÑ‡∏î‡∏û‡∏¥‡∏°", "‡πÑ‡∏Æ‡∏ã‡∏µ‡∏™", "‡∏ö‡∏≠‡∏°‡∏™‡πå", "‡∏û‡∏£‡∏µ‡∏î‡∏¥‡∏Ñ‡∏ó‡πå", "‡∏ß‡∏≠‡πÅ‡∏£‡∏ô‡∏î‡πå",
        "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå", "‡πÄ‡∏ö‡∏ô‡∏ã‡∏≤‡∏ô‡πà‡∏≤", "‡πÅ‡∏à‡πä‡∏™", "‡∏ô‡∏≤‡πÅ‡∏î‡∏ô", "‡∏Ñ‡∏≤‡∏ã‡πà‡∏≤",
        "‡∏ã‡∏¥‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå", "‡∏≠‡∏≤‡∏ó‡∏£‡∏≤‡∏ã‡∏µ‡∏ô", "‡∏Ñ‡∏≤‡∏£‡∏¥‡∏™‡∏°‡∏≤",
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏≤‡∏£‡πÄ‡∏Ñ‡∏°‡∏µ
        "‡∏™‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÅ‡∏°‡∏•‡∏á", "‡∏™‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ", "‡∏™‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä",
        "‡∏¢‡∏≤‡∏Ü‡πà‡∏≤‡πÅ‡∏°‡∏•‡∏á", "‡∏¢‡∏≤‡∏Ü‡πà‡∏≤‡∏´‡∏ç‡πâ‡∏≤", "‡∏¢‡∏≤‡∏Ü‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤",
        # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏∏‡πã‡∏¢
        "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ", "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏™‡∏°", "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏õ‡∏∏‡πã‡∏¢", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢",
        # English
        "fertilizer", "nutrient", "hormone", "chemical"
    ]
    
    # Intent keywords (NEW)
    intent_keywords = {
        "increase_yield": [
            # Thai
            "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏™‡∏π‡∏á", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏°‡∏≤‡∏Å", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏¢‡∏≠‡∏∞", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏°",
            # English
            "increase yield", "higher yield", "more yield", "increase production", "boost yield", "increase harvest"
        ],
        "solve_problem": [
            # Thai
            "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°", "‡πÅ‡∏Å‡πâ‡πÇ‡∏£‡∏Ñ", "‡πÅ‡∏Å‡πâ",
            # English
            "solve problem", "control", "kill", "manage pest", "prevent", "control pest", "treat"
        ],
        "general_care": [
            # Thai
            "‡∏î‡∏π‡πÅ‡∏•", "‡∏ö‡∏≥‡∏£‡∏∏‡∏á", "‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á", "‡∏õ‡∏•‡∏π‡∏Å", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢",
            # English
            "care", "fertilize", "general care", "maintenance", "nurture"
        ],
        "product_inquiry": [
            # Thai - ‡πÄ‡∏û‡∏¥‡πà‡∏° patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
            "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏°‡∏µ‡πÑ‡∏´‡∏°", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ã‡∏∑‡πâ‡∏≠",
            "‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡∏û‡πà‡∏ô‡∏≠‡∏∞‡πÑ‡∏£", "‡∏â‡∏µ‡∏î‡∏≠‡∏∞‡πÑ‡∏£",
            "‡∏™‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ", "‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡∏î‡∏µ", "‡∏¢‡∏≤‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô",
            "‡∏°‡∏µ‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á", "‡∏ö‡πâ‡∏≤‡∏á",
            # English
            "what products", "what is available", "recommend product", "recommend", "what to use", "is there"
        ]
    }
    
    found = {
        "pests": [],
        "crops": [],
        "products": [],
        "fertilizers": [],  # NEW: fertilizer keywords
        "intent": None,  # NEW: detect user intent
        "is_product_query": False,
        "is_fertilizer_query": False  # NEW: flag for fertilizer questions
    }
    
    # Extract pests
    for keyword in pest_keywords:
        if keyword in question_norm:
            found["pests"].append(keyword)
    
    # Extract crops
    for keyword in crop_keywords:
        if keyword in question_norm:
            found["crops"].append(keyword)
    
    # Extract product-related
    for keyword in product_keywords:
        if keyword in question_norm:
            found["products"].append(keyword)
            found["is_product_query"] = True

    # Extract fertilizer-related (NEW)
    for keyword in fertilizer_keywords:
        if keyword in question_norm:
            found["fertilizers"].append(keyword)
            found["is_fertilizer_query"] = True
            found["is_product_query"] = True

    # Detect intent (NEW)
    # Detect intent (MATCH ON NORMALIZED TEXT)
    for intent, keywords in intent_keywords.items():
        for keyword in keywords:
            if keyword in question_norm:
                found["intent"] = intent
                found["is_product_query"] = True
                break
        if found["intent"]:
            break
    
    return found


def generate_thai_disease_variants(disease_name: str) -> List[str]:
    """
    Generate Thai disease name variants for fuzzy matching.

    Handles:
    1. ‡πÇ‡∏£‡∏Ñ prefix add/remove
    2. ‡∏™‡∏µ prefix in color names (‡∏£‡∏≤‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π ‚Üî ‡∏£‡∏≤‡∏ä‡∏°‡∏û‡∏π)
    3. Common transliteration variants (‡πÅ‡∏≠‡∏Ñ‡πÅ‡∏ó‡∏Ñ‡πÇ‡∏ô‡∏™ ‚Üî ‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™)
    4. N-gram substrings for fuzzy matching

    Returns list of variants including the original name.
    """
    variants = {disease_name}

    # Strip ‡πÇ‡∏£‡∏Ñ prefix for matching, but also keep variant with it
    bare = disease_name
    if bare.startswith("‡πÇ‡∏£‡∏Ñ"):
        bare = bare[len("‡πÇ‡∏£‡∏Ñ"):]
        variants.add(bare)
    else:
        variants.add("‡πÇ‡∏£‡∏Ñ" + bare)

    # Known transliteration variants (common misspellings by Thai farmers)
    _SPELLING_VARIANTS = {
        "‡πÅ‡∏≠‡∏Ñ‡πÅ‡∏ó‡∏Ñ‡πÇ‡∏ô‡∏™": "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™": "‡πÅ‡∏≠‡∏Ñ‡πÅ‡∏ó‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Å‡πÇ‡∏ô‡∏™": "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÅ‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™": "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏Ñ‡πÇ‡∏ô‡∏™": "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÑ‡∏ü‡∏ó‡πá‡∏≠‡∏õ‡πÇ‡∏ó‡∏£‡πà‡∏≤": "‡πÑ‡∏ü‡∏ó‡∏≠‡∏õ‡∏ò‡∏≠‡∏£‡πà‡∏≤",
        "‡πÑ‡∏ü‡∏ó‡∏≠‡∏õ‡∏ò‡∏≠‡∏£‡πà‡∏≤": "‡πÑ‡∏ü‡∏ó‡πá‡∏≠‡∏õ‡πÇ‡∏ó‡∏£‡πà‡∏≤",
        "‡πÑ‡∏ü‡∏ò‡∏≠‡∏õ‡∏ó‡∏≠‡∏£‡πà‡∏≤": "‡πÑ‡∏ü‡∏ó‡∏≠‡∏õ‡∏ò‡∏≠‡∏£‡πà‡∏≤",
        "‡∏ü‡∏¥‡∏ß‡∏ã‡∏≤‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏°": "‡∏ü‡∏¥‡∏ß‡∏ã‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°",
        "‡∏ü‡∏¥‡∏ß‡πÄ‡∏ã‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏°": "‡∏ü‡∏¥‡∏ß‡∏ã‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°",
        "‡∏ü‡∏π‡∏ã‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°": "‡∏ü‡∏¥‡∏ß‡∏ã‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°",
        "‡∏ü‡∏¥‡∏ß‡∏™‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°": "‡∏ü‡∏¥‡∏ß‡∏ã‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏°",
        "‡∏î‡∏≤‡∏ß‡∏ô‡∏µ‡πà‡∏°‡∏¥‡∏•‡∏î‡∏¥‡∏ß": "‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á",
        "‡∏û‡∏≤‡∏ß‡πÄ‡∏î‡∏≠‡∏£‡∏µ‡πà‡∏°‡∏¥‡∏•‡∏î‡∏¥‡∏ß": "‡∏£‡∏≤‡πÅ‡∏õ‡πâ‡∏á",
    }

    # Add spelling variants
    for variant_from, variant_to in _SPELLING_VARIANTS.items():
        if variant_from in bare:
            new_bare = bare.replace(variant_from, variant_to)
            variants.add(new_bare)
            variants.add("‡πÇ‡∏£‡∏Ñ" + new_bare)

    _COLORS = ["‡∏ä‡∏°‡∏û‡∏π", "‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•", "‡πÄ‡∏ó‡∏≤", "‡∏Ç‡∏≤‡∏ß", "‡∏î‡∏≥", "‡∏°‡πà‡∏ß‡∏á", "‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á", "‡∏™‡πâ‡∏°"]

    for color in _COLORS:
        # ‡∏£‡∏≤+‡∏™‡∏µ+color ‚Üî ‡∏£‡∏≤+color
        with_si = f"‡∏£‡∏≤‡∏™‡∏µ{color}"
        without_si = f"‡∏£‡∏≤{color}"
        if with_si in bare or without_si in bare:
            variants.add(bare.replace(with_si, without_si))
            variants.add(bare.replace(without_si, with_si))
            variants.add("‡πÇ‡∏£‡∏Ñ" + bare.replace(with_si, without_si))
            variants.add("‡πÇ‡∏£‡∏Ñ" + bare.replace(without_si, with_si))

        # ‡∏à‡∏∏‡∏î+‡∏™‡∏µ+color ‚Üî ‡∏à‡∏∏‡∏î+color
        jud_with_si = f"‡∏à‡∏∏‡∏î‡∏™‡∏µ{color}"
        jud_without_si = f"‡∏à‡∏∏‡∏î{color}"
        if jud_with_si in bare or jud_without_si in bare:
            variants.add(bare.replace(jud_with_si, jud_without_si))
            variants.add(bare.replace(jud_without_si, jud_with_si))
            variants.add("‡πÇ‡∏£‡∏Ñ" + bare.replace(jud_with_si, jud_without_si))
            variants.add("‡πÇ‡∏£‡∏Ñ" + bare.replace(jud_without_si, jud_with_si))

    return list(variants)
