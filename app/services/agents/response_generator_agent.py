"""
Response Generator Agent

Responsibilities:
- Synthesize final answer from grounded/verified product data
- Generate formatted response using LLM with "‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤" persona
- Format citations for readability
- Add confidence indicators when needed
- Handle fallback responses when no data found
"""

import logging
import json

from app.services.agents import (
    QueryAnalysis,
    RetrievalResult,
    GroundingResult,
    AgenticRAGResponse,
    IntentType
)
from app.utils.text_processing import post_process_answer

logger = logging.getLogger(__name__)

# Configuration
LOW_CONFIDENCE_THRESHOLD = 0.5


class ResponseGeneratorAgent:
    """
    Agent 4: Response Generation
    Creates the final user-facing response using LLM with verified product data
    """

    def __init__(self, openai_client=None):
        self.openai_client = openai_client

    async def generate(
        self,
        query_analysis: QueryAnalysis,
        retrieval_result: RetrievalResult,
        grounding_result: GroundingResult
    ) -> AgenticRAGResponse:
        """
        Generate final response from pipeline results using LLM

        Returns:
            AgenticRAGResponse with answer, citations, and metadata
        """
        try:
            logger.info(f"ResponseGeneratorAgent: Generating response")
            logger.info(f"  - Grounded: {grounding_result.is_grounded}")
            logger.info(f"  - Confidence: {grounding_result.confidence:.2f}")

            # Handle special intents
            if query_analysis.intent == IntentType.GREETING:
                return self._generate_greeting_response(query_analysis)

            # Handle no data case
            if not grounding_result.is_grounded and not retrieval_result.documents:
                return self._generate_no_data_response(query_analysis)

            # Generate answer from verified product data using LLM
            answer = await self._generate_llm_response(
                query_analysis, retrieval_result, grounding_result
            )

            # Post-process answer (remove markdown artifacts)
            answer = post_process_answer(answer)

            # Add low confidence indicator if needed
            if grounding_result.confidence < LOW_CONFIDENCE_THRESHOLD:
                answer = self._add_low_confidence_note(answer)

            return AgenticRAGResponse(
                answer=answer,
                confidence=grounding_result.confidence,
                citations=grounding_result.citations,
                intent=query_analysis.intent,
                is_grounded=grounding_result.is_grounded,
                sources_used=retrieval_result.sources_used,
                query_analysis=query_analysis,
                retrieval_result=retrieval_result,
                grounding_result=grounding_result
            )

        except Exception as e:
            logger.error(f"ResponseGeneratorAgent error: {e}", exc_info=True)
            return AgenticRAGResponse(
                answer="‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞",
                confidence=0.0,
                citations=[],
                intent=query_analysis.intent,
                is_grounded=False,
                sources_used=[]
            )

    async def _generate_llm_response(
        self,
        query_analysis: QueryAnalysis,
        retrieval_result: RetrievalResult,
        grounding_result: GroundingResult
    ) -> str:
        """Generate formatted response using LLM with verified product data"""

        if not self.openai_client:
            return self._build_fallback_answer(retrieval_result, grounding_result)

        # Build product data context from retrieval results
        product_context_parts = []
        for i, doc in enumerate(retrieval_result.documents[:5], 1):
            meta = doc.metadata
            part = f"[‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ {i}] {meta.get('product_name', doc.title)}"
            if meta.get('active_ingredient'):
                part += f" (‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {meta['active_ingredient']})"
            part += "\n"
            if meta.get('category'):
                part += f"  ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {meta['category']}\n"
            if meta.get('target_pest'):
                part += f"  ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏à‡∏±‡∏î: {str(meta['target_pest'])[:200]}\n"
            if meta.get('applicable_crops'):
                part += f"  ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ: {str(meta['applicable_crops'])[:200]}\n"
            if meta.get('usage_rate'):
                part += f"  ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {meta['usage_rate']}\n"
            if meta.get('how_to_use'):
                part += f"  ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ: {str(meta['how_to_use'])[:200]}\n"
            if meta.get('usage_period'):
                part += f"  ‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {str(meta['usage_period'])[:150]}\n"
            product_context_parts.append(part)

        product_context = "\n".join(product_context_parts)

        # Relevant products from grounding
        relevant = grounding_result.relevant_products
        relevant_str = ", ".join(relevant) if relevant else "(‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö)"

        prompt = f"""‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "{query_analysis.original_query}"
Intent: {query_analysis.intent.value}
Entities: {json.dumps(query_analysis.entities, ensure_ascii=False)}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß:
{product_context}

‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: [{relevant_str}]

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""

        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤" ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Ç‡∏≠‡∏á ICP Ladda

‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:
1. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î - ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
2. ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
3. ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á
4. ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)" ‡πÄ‡∏ä‡πà‡∏ô "‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏ô 50 (‡πÇ‡∏õ‡∏£‡∏ü‡∏µ‡πÇ‡∏ô‡∏ü‡∏≠‡∏™)"

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ "‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤" + ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
- ‡πÉ‡∏ä‡πâ emoji ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: ü¶† üåø üíä üìã ‚öñÔ∏è üìÖ ‚ö†Ô∏è üí°
- ‡πÉ‡∏ä‡πâ ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‡∏Ñ‡∏±‡πà‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÜ
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ** ‡∏´‡∏£‡∏∑‡∏≠ ##
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä ‚Üí ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°: ‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏á‡∏≠‡∏Å / ‡∏´‡∏•‡∏±‡∏á‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏á‡∏≠‡∏Å
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á/‡πÇ‡∏£‡∏Ñ ‚Üí ‡∏£‡∏∞‡∏ö‡∏∏: ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ, ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ, ‡∏ä‡πà‡∏ß‡∏á‡πÉ‡∏ä‡πâ
- ‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ "‡∏ñ‡πâ‡∏≤‡∏ö‡∏≠‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏ñ‡∏±‡∏á‡∏û‡πà‡∏ô ‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞" (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤)
- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=700
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"LLM response generation failed: {e}")
            return self._build_fallback_answer(retrieval_result, grounding_result)

    def _build_fallback_answer(
        self,
        retrieval_result: RetrievalResult,
        grounding_result: GroundingResult
    ) -> str:
        """Build answer without LLM from raw product data"""
        if not retrieval_result.documents:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏∞"

        parts = ["‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n"]
        for i, doc in enumerate(retrieval_result.documents[:3], 1):
            meta = doc.metadata
            name = meta.get('product_name') or doc.title
            ingredient = meta.get('active_ingredient')
            if ingredient:
                name = f"{name} ({ingredient})"
            parts.append(f"{i}. {name}")
            if meta.get('target_pest'):
                parts.append(f"   - ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏à‡∏±‡∏î: {str(meta['target_pest'])[:100]}")
            if meta.get('usage_rate'):
                parts.append(f"   - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ: {meta['usage_rate']}")
            parts.append("")

        parts.append("\n‡∏ñ‡πâ‡∏≤‡∏ö‡∏≠‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏ñ‡∏±‡∏á‡∏û‡πà‡∏ô ‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞")
        return "\n".join(parts)

    def _generate_greeting_response(self, query_analysis: QueryAnalysis) -> AgenticRAGResponse:
        """Generate response for greeting intent"""
        greetings = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞",
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏∞ ‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞",
            "‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏°‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞",
        ]

        import random
        answer = random.choice(greetings)

        return AgenticRAGResponse(
            answer=answer,
            confidence=1.0,
            citations=[],
            intent=IntentType.GREETING,
            is_grounded=True,
            sources_used=[]
        )

    def _generate_no_data_response(self, query_analysis: QueryAnalysis) -> AgenticRAGResponse:
        """Generate response when no relevant data found"""

        # Customize response based on intent
        if query_analysis.intent == IntentType.PRODUCT_INQUIRY:
            product = query_analysis.entities.get('product_name', '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ')
            answer = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö \"{product}\" ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏∞\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞"

        elif query_analysis.intent == IntentType.DISEASE_TREATMENT:
            disease = query_analysis.entities.get('disease_name', '‡πÇ‡∏£‡∏Ñ‡∏ô‡∏µ‡πâ')
            plant = query_analysis.entities.get('plant_type', '')
            plant_text = f"‡πÉ‡∏ô{plant}" if plant else ""
            answer = f"‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏Ç‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö \"{disease}\" {plant_text}‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ñ‡πà‡∏∞\n\n‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡∏ä‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ä‡πà‡∏ß‡∏á‡πÑ‡∏´‡∏ô (‡πÅ‡∏ï‡∏Å‡πÉ‡∏ö‡∏≠‡πà‡∏≠‡∏ô/‡∏≠‡∏≠‡∏Å‡∏î‡∏≠‡∏Å/‡∏ï‡∏¥‡∏î‡∏ú‡∏•) ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞"

        elif query_analysis.intent == IntentType.PEST_CONTROL:
            pest = query_analysis.entities.get('pest_name', '‡πÅ‡∏°‡∏•‡∏á‡∏ô‡∏µ‡πâ')
            answer = f"‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏Ç‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏à‡∏±‡∏î \"{pest}\" ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ñ‡πà‡∏∞\n\n‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡∏ä‡∏≠‡∏∞‡πÑ‡∏£ ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞"

        elif query_analysis.intent == IntentType.WEED_CONTROL:
            plant = query_analysis.entities.get('plant_type', '')
            if plant:
                answer = f"‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏Ç‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö{plant}‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ñ‡πà‡∏∞\n\n‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏´‡∏ô (‡πÉ‡∏ö‡πÅ‡∏Ñ‡∏ö/‡πÉ‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á/‡∏Å‡∏Å) ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Ñ‡πà‡∏∞"
            else:
                answer = "‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏Ç‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞\n\n‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤:\n- ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡∏ä‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß, ‡∏Ç‡πâ‡∏≤‡∏ß‡πÇ‡∏û‡∏î)\n- ‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏´‡∏ô (‡πÉ‡∏ö‡πÅ‡∏Ñ‡∏ö/‡πÉ‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á/‡∏Å‡∏Å)\n\n‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ñ‡πà‡∏∞"

        elif query_analysis.intent == IntentType.USAGE_INSTRUCTION:
            answer = "‡∏Ç‡∏≠‡∏ó‡∏£‡∏≤‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏∞\n- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡∏Ñ‡∏∞?\n- ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∞?\n\n‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡πà‡∏∞"

        else:
            answer = "‡∏ô‡πâ‡∏≠‡∏á‡∏•‡∏±‡∏î‡∏î‡∏≤‡∏Ç‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ñ‡πà‡∏∞\n\n‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤:\n- ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡∏ä‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß, ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô, ‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á)\n- ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö (‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏£‡∏Ñ, ‡πÅ‡∏°‡∏•‡∏á, ‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä)\n\n‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞"

        return AgenticRAGResponse(
            answer=answer,
            confidence=0.0,
            citations=[],
            intent=query_analysis.intent,
            is_grounded=False,
            sources_used=[]
        )

    def _add_low_confidence_note(self, answer: str) -> str:
        """Add note when confidence is low"""
        note = "\n\n(‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏∞)"
        if note not in answer:
            answer += note
        return answer

