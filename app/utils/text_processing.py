import re
from typing import List, Dict

# Thai diacritics (tone marks + special marks) used for fuzzy matching
_THAI_DIACRITICS = re.compile(r'[\u0E48\u0E49\u0E4A\u0E4B\u0E47\u0E4C]')
# ‡πà (0E48) ‡πâ (0E49) ‡πä (0E4A) ‡πã (0E4B) ‡πá (0E47) ‡πå (0E4C)


def strip_thai_diacritics(text: str) -> str:
    """
    Remove Thai tone marks and diacritics for fuzzy matching.
    ‡πà ‡πâ ‡πä ‡πã ‡πá ‡πå  ‚Äî only for matching, never change the original query.
    """
    return _THAI_DIACRITICS.sub('', text)


def diacritics_match(text: str, pattern: str) -> bool:
    """
    Check if *pattern* appears in *text* after stripping Thai diacritics from both.
    Use this instead of ``pattern in text`` when user may type extra tone marks.
    """
    return strip_thai_diacritics(pattern) in strip_thai_diacritics(text)

# Allowed emojis: üòä (U+1F60A) and üå± (U+1F331)
_ALLOWED_EMOJIS = {'\U0001F60A', '\U0001F331'}

# Unicode ranges covering most emoji (simplified but effective)
_EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # Emoticons
    "\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
    "\U0001F680-\U0001F6FF"  # Transport and Map
    "\U0001F1E0-\U0001F1FF"  # Flags
    "\U0001F900-\U0001F9FF"  # Supplemental Symbols
    "\U0001FA00-\U0001FA6F"  # Chess Symbols
    "\U0001FA70-\U0001FAFF"  # Symbols Extended-A
    "\U00002702-\U000027B0"  # Dingbats
    "\U000024C2-\U0001F251"  # Enclosed characters
    "\U0000FE0F"             # Variation Selector
    "\U00002600-\U000026FF"  # Misc Symbols (‚òÄÔ∏è‚ö†Ô∏è‚ö° etc.)
    "\U00002700-\U000027BF"  # Dingbats
    "\U0000200D"             # ZWJ
    "\U00002B50"             # Star
    "\U0000203C\U00002049"   # Exclamation marks
    "\U000023E9-\U000023F3"  # Media symbols
    "\U000023F8-\U000023FA"  # Media symbols
    "]+",
    flags=re.UNICODE
)

def _strip_banned_emojis(text: str) -> str:
    """Remove all emojis except üòä and üå±"""
    def _replace(match):
        chars = match.group()
        # Keep only allowed emojis from the matched span
        kept = ''.join(c for c in chars if c in _ALLOWED_EMOJIS)
        return kept
    return _EMOJI_PATTERN.sub(_replace, text)


def post_process_answer(answer: str) -> str:
    """Post-process Gemini answer for better quality"""
    if not answer:
        return ""
    
    # 1. Remove markdown formatting
    answer = answer.replace("```", "")
    answer = answer.replace("**", "")
    answer = answer.replace("##", "")
    answer = answer.replace("###", "")
    answer = re.sub(r'\*\*([^*]+)\*\*', r'\1', answer)  # **text** ‚Üí text
    answer = re.sub(r'\*([^*]+)\*', r'\1', answer)  # *text* ‚Üí text
    
    # 2. Fix Thai encoding issues
    answer = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥‡∏¥‡∏µ‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πà‡πâ‡πä‡πã])', r'\1\2', answer)
    answer = answer.replace('ƒû', '')
    answer = answer.replace('', '')
    answer = answer.replace('\x00', '')
    
    # 3. Fix spacing issues (preserve newlines!)
    # Only collapse multiple spaces within lines, preserve newlines
    answer = re.sub(r'[ \t]+', ' ', answer)  # Multiple spaces/tabs ‚Üí single space (preserve \n)
    answer = answer.replace(' ,', ',')
    answer = answer.replace(' .', '.')
    answer = answer.replace(' :', ':')
    answer = answer.replace('( ', '(')
    answer = answer.replace(' )', ')')
    
    # 4. Fix bullet points (convert markdown to Thai style)
    answer = re.sub(r'^\s*[-*]\s+', '‚Ä¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'\n\s*[-*]\s+', '\n‚Ä¢ ', answer)
    
    # 5. Ensure proper line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)  # Max 2 line breaks
    
    # 6. Remove leading/trailing whitespace
    answer = answer.strip()
    
    # 7. Fix common Thai typos
    answer = answer.replace('‡∏ï‡πâ', '‡∏ï‡πâ')
    answer = answer.replace('‡∏ï', '‡∏ï')
    
    # 8. Strip all emojis except üòä and üå±
    answer = _strip_banned_emojis(answer)

    # 9. Remove divider/separator lines entirely (‚îÄ, ‚îÅ, ‚ïê, -, =, etc.)
    answer = re.sub(r'^[\s]*[-=‚îÄ‚îÅ‚ïê‚Äî‚Äì_]{3,}[\s]*$', '', answer, flags=re.MULTILINE)

    # 10. Remove old [‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠] bracket format (fallback cleanup)
    answer = re.sub(r'^\[([^\]]+)\]\s*$', r'\1', answer, flags=re.MULTILINE)

    # 11. Strip leading whitespace from each line (left after emoji removal)
    answer = '\n'.join(line.lstrip() for line in answer.split('\n'))

    # 12. Collapse excessive blank lines
    answer = re.sub(r'\n{3,}', '\n\n', answer)

    return answer

def clean_knowledge_text(text: str) -> str:
    """Clean and format knowledge text for better readability"""
    if not text:
        return ""
    
    # Fix encoding issues - remove corrupted characters
    # Common patterns: ‡∏àƒû‡∏≥, ‡∏•ƒû‡∏≥, ‡∏óƒû‡∏≥, ‡∏ôƒû‡πâ‡∏≥, ‡∏Åƒû‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥])', r'\1\2', text)  # ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏ôƒû‡πâ ‚Üí ‡∏ô‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏¥])', r'\1\2', text)  # ‡∏Åƒû‡∏¥ ‚Üí ‡∏Å‡∏¥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏µ])', r'\1\2', text)  # ‡∏Åƒû‡∏µ ‚Üí ‡∏Å‡∏µ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏∏])', r'\1\2', text)  # ‡∏Åƒû‡∏∏ ‚Üí ‡∏Å‡∏∏
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏π])', r'\1\2', text)  # ‡∏Åƒû‡∏π ‚Üí ‡∏Å‡∏π
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πà])', r'\1\2', text)  # ‡∏Åƒû‡πà ‚Üí ‡∏Å‡πà
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏Åƒû‡πâ ‚Üí ‡∏Å‡πâ
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πä])', r'\1\2', text)  # ‡∏Åƒû‡πä ‚Üí ‡∏Å‡πä
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πã])', r'\1\2', text)  # ‡∏Åƒû‡πã ‚Üí ‡∏Å‡πã
    text = re.sub(r'ƒû', '', text)  # Remove remaining ƒû
    
    # Fix other corrupted characters
    text = text.replace('‡∏ï‡πâ', '‡∏ï‡πâ')  # Fix tone marks
    text = text.replace('‡∏ï', '‡∏ï')
    text = text.replace('', '')  # Remove replacement character
    text = text.replace('\x00', '')  # Remove null character
    
    # Fix common Thai encoding issues
    text = text.replace('√†¬∏', '')  # Remove Thai encoding prefix
    text = text.replace('√†¬π', '')  # Remove Thai encoding prefix
    
    # Remove excessive whitespace
    text = ' '.join(text.split())
    
    # Fix common issues
    text = text.replace('  ', ' ')  # Double spaces
    text = text.replace(' ,', ',')  # Space before comma
    text = text.replace(' .', '.')  # Space before period
    text = text.replace('( ', '(')  # Space after opening parenthesis
    text = text.replace(' )', ')')  # Space before closing parenthesis
    text = text.replace(' :', ':')  # Space before colon
    
    # Fix Thai-specific issues (keep important marks)
    # text = text.replace('‡∏∫', '')  # Keep Thai character above
    # text = text.replace('‡πå', '')  # Keep Thai character above
    
    # Remove multiple consecutive spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence spacing
    text = re.sub(r'([.!?])\s*([A-Za-z‡∏Å-‡πô])', r'\1 \2', text)
    
    # Remove leading/trailing whitespace
    text = text.strip()
    
    # Remove lines with only special characters
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if line.strip() and not re.match(r'^[^\w\s]+$', line.strip())]
    text = '\n'.join(cleaned_lines)
    
    return text

def extract_keywords_from_question(question: str) -> dict:
    """Extract main keywords from question with categories"""
    question_lower = question.lower()
    # Normalize simple punctuation to spaces for better matching
    question_norm = re.sub(r'[\.,\?\!\:\;\(\)\/]',' ', question_lower)
    
    # Pest/Disease keywords (expanded)
    pest_keywords = [
        # Thai - ‡πÅ‡∏°‡∏•‡∏á
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏≠‡πà‡∏≠‡∏ô", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢", "‡∏´‡∏ô‡∏≠‡∏ô", "‡πÅ‡∏°‡∏•‡∏á", "‡∏î‡πâ‡∏ß‡∏á‡∏á‡∏ß‡∏á",
        "‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡∏´‡∏ô‡∏≠‡∏ô‡πÄ‡∏à‡∏≤‡∏∞", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏≠", "‡∏´‡∏ô‡∏≠‡∏ô‡πÉ‡∏¢", "‡∏î‡πâ‡∏ß‡∏á", "‡∏°‡∏î", "‡∏õ‡∏•‡∏ß‡∏Å",
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏à‡∏±‡∏Å‡∏à‡∏±‡πà‡∏ô", "‡πÅ‡∏°‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ú‡∏•", "‡πÅ‡∏°‡∏•‡∏á‡∏´‡∏ß‡∏µ‡πà‡∏Ç‡∏≤‡∏ß", "‡∏ó‡∏£‡∏¥‡∏õ‡∏™‡πå",
        "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä", "‡πÑ‡∏£", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÅ‡∏õ‡πâ‡∏á", "‡∏´‡∏ô‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏Ç‡πâ‡∏≤‡∏ß",
        # Thai - ‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤
        "‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á", "‡∏£‡∏≤‡πÅ‡∏õ‡πâ‡∏á", "‡∏£‡∏≤‡∏™‡∏ô‡∏¥‡∏°", "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡∏£‡∏≤", "‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™",
        "‡πÇ‡∏£‡∏Ñ‡∏ú‡∏•‡πÄ‡∏ô‡πà‡∏≤", "‡∏ú‡∏•‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ", "‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ", "‡πÇ‡∏£‡∏Ñ‡∏£‡∏≤‡∏î‡∏≥", "‡∏£‡∏≤‡∏î‡∏≥",
        "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏à‡∏∏‡∏î", "‡πÉ‡∏ö‡∏à‡∏∏‡∏î", "‡πÇ‡∏£‡∏Ñ‡∏Å‡∏¥‡πà‡∏á‡πÅ‡∏´‡πâ‡∏á", "‡∏Å‡∏¥‡πà‡∏á‡πÅ‡∏´‡πâ‡∏á", "‡πÇ‡∏£‡∏Ñ‡∏£‡∏≤‡∏Å‡πÄ‡∏ô‡πà‡∏≤", "‡∏£‡∏≤‡∏Å‡πÄ‡∏ô‡πà‡∏≤",
        "‡πÇ‡∏£‡∏Ñ‡∏•‡∏≥‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡∏•‡∏≥‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏£‡∏Ñ‡πÇ‡∏Ñ‡∏ô‡πÄ‡∏ô‡πà‡∏≤", "‡πÇ‡∏Ñ‡∏ô‡πÄ‡∏ô‡πà‡∏≤",
        # Thai - ‡πÑ‡∏ß‡∏£‡∏±‡∏™
        "‡πÑ‡∏ß‡∏£‡∏±‡∏™", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å", "‡πÇ‡∏£‡∏Ñ‡∏à‡∏π‡πã", "‡πÇ‡∏£‡∏Ñ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á",
        # Thai - ‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä
        "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤", "‡∏ú‡∏±‡∏Å‡∏ö‡∏∏‡πâ‡∏á", "‡∏´‡∏ç‡πâ‡∏≤‡∏Ñ‡∏≤",
        # Thai - ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä", "‡πÇ‡∏£‡∏Ñ",
        # English
        "aphid", "thrips", "whitefly", "moth", "caterpillar", "worm", "beetle",
        "mildew", "powdery mildew", "rust", "fungus", "fungal", "anthracnose",
        "virus", "viral", "disease", "weed", "grass", "mite", "borer", "leaf miner",
        "insect", "pest", "armyworm", "thrips", "fruit rot", "root rot", "stem rot"
    ]
    
    # Crop keywords (expanded)
    crop_keywords = [
        # Thai
        "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏û‡∏∑‡∏ä‡∏ú‡∏±‡∏Å", "‡∏ú‡∏±‡∏Å", "‡∏ú‡∏•‡πÑ‡∏°‡πâ",
        "‡∏°‡∏∞‡∏ô‡∏≤‡∏ß", "‡∏™‡πâ‡∏°", "‡∏Å‡∏•‡πâ‡∏ß‡∏¢", "‡∏°‡∏∞‡∏û‡∏£‡πâ‡∏≤‡∏ß", "‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤", "‡∏õ‡∏≤‡∏•‡πå‡∏°",
        "‡∏Ç‡πâ‡∏≤‡∏ß‡πÇ‡∏û‡∏î", "‡∏≠‡πâ‡∏≠‡∏¢", "‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á", "‡∏ñ‡∏±‡πà‡∏ß", "‡∏û‡∏£‡∏¥‡∏Å", "‡∏°‡∏∞‡πÄ‡∏Ç‡∏∑‡∏≠‡πÄ‡∏ó‡∏®",
        "‡∏•‡∏≥‡πÑ‡∏¢", "‡∏•‡∏¥‡πâ‡∏ô‡∏à‡∏µ‡πà", "‡πÄ‡∏á‡∏≤‡∏∞", "‡∏°‡∏±‡∏á‡∏Ñ‡∏∏‡∏î", "‡∏ù‡∏£‡∏±‡πà‡∏á", "‡∏ä‡∏°‡∏û‡∏π‡πà",
        # English
        "durian", "mango", "rice", "vegetable", "vegetables", "fruit",
        "lime", "orange", "banana", "coconut", "rubber", "palm",
        "corn", "sugarcane", "cassava", "peanut", "chilli", "tomato",
        "longan", "lychee", "rambutan", "mangosteen", "guava"
    ]
    
    # Product-related keywords
    product_keywords = [
        # Thai
        "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏™‡∏≤‡∏£", "‡∏õ‡∏∏‡πã‡∏¢", "‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏∏‡πã‡∏¢",
        "‡∏°‡πâ‡∏≤‡∏ö‡∏¥‡∏ô", "mahbin",
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡πÉ‡∏ä‡πâ", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô",
        # English
        "product", "products", "fertilizer", "recommend"
    ]

    # Fertilizer-specific keywords (NEW)
    fertilizer_keywords = [
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏∏‡πã‡∏¢/‡∏™‡∏≤‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á
        "‡∏õ‡∏∏‡πã‡∏¢", "‡∏™‡∏≤‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á", "‡∏ò‡∏≤‡∏ï‡∏∏‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô", "‡∏™‡∏≤‡∏£‡πÄ‡∏£‡πà‡∏á",
        "‡∏õ‡∏∏‡πã‡∏¢‡πÄ‡∏Ñ‡∏°‡∏µ", "‡∏õ‡∏∏‡πã‡∏¢‡∏≠‡∏¥‡∏ô‡∏ó‡∏£‡∏µ‡∏¢‡πå", "‡∏õ‡∏∏‡πã‡∏¢‡∏ä‡∏µ‡∏ß‡∏†‡∏≤‡∏û",
        "‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏∏‡πã‡∏¢", "npk", "‡πÑ‡∏ô‡πÇ‡∏ï‡∏£‡πÄ‡∏à‡∏ô", "‡∏ü‡∏≠‡∏™‡∏ü‡∏≠‡∏£‡∏±‡∏™", "‡πÇ‡∏û‡πÅ‡∏ó‡∏™‡πÄ‡∏ã‡∏µ‡∏¢‡∏°",
        # ‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï
        "‡πÄ‡∏£‡πà‡∏á‡∏ï‡πâ‡∏ô", "‡πÅ‡∏ï‡∏Å‡∏Å‡∏≠", "‡∏£‡∏±‡∏ö‡∏£‡∏ß‡∏á", "‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡πâ‡∏≠‡∏á", "‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏ï‡πâ‡∏ô",
        # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏∏‡πã‡∏¢
        "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÉ‡∏ä‡πâ", "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏™‡∏°", "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏õ‡∏∏‡πã‡∏¢", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢",
        # English
        "fertilizer", "nutrient", "hormone", "chemical"
    ]
    
    # Intent keywords (NEW)
    intent_keywords = {
        "increase_yield": [
            # Thai
            "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏™‡∏π‡∏á", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏°‡∏≤‡∏Å", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏¢‡∏≠‡∏∞", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏°",
            # English
            "increase yield", "higher yield", "more yield", "increase production", "boost yield", "increase harvest"
        ],
        "solve_problem": [
            # Thai
            "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°", "‡πÅ‡∏Å‡πâ‡πÇ‡∏£‡∏Ñ", "‡πÅ‡∏Å‡πâ",
            # English
            "solve problem", "control", "kill", "manage pest", "prevent", "control pest", "treat"
        ],
        "general_care": [
            # Thai
            "‡∏î‡∏π‡πÅ‡∏•", "‡∏ö‡∏≥‡∏£‡∏∏‡∏á", "‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á", "‡∏õ‡∏•‡∏π‡∏Å", "‡πÉ‡∏™‡πà‡∏õ‡∏∏‡πã‡∏¢",
            # English
            "care", "fertilize", "general care", "maintenance", "nurture"
        ],
        "product_inquiry": [
            # Thai - ‡πÄ‡∏û‡∏¥‡πà‡∏° patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
            "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏°‡∏µ‡πÑ‡∏´‡∏°", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ã‡∏∑‡πâ‡∏≠",
            "‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡∏û‡πà‡∏ô‡∏≠‡∏∞‡πÑ‡∏£", "‡∏â‡∏µ‡∏î‡∏≠‡∏∞‡πÑ‡∏£",
            "‡∏™‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ", "‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡∏î‡∏µ", "‡∏¢‡∏≤‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô",
            "‡∏°‡∏µ‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£", "‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô", "‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á", "‡∏ö‡πâ‡∏≤‡∏á",
            # English
            "what products", "what is available", "recommend product", "recommend", "what to use", "is there"
        ]
    }
    
    found = {
        "pests": [],
        "crops": [],
        "products": [],
        "fertilizers": [],  # NEW: fertilizer keywords
        "intent": None,  # NEW: detect user intent
        "is_product_query": False,
        "is_fertilizer_query": False  # NEW: flag for fertilizer questions
    }
    
    # Extract pests
    for keyword in pest_keywords:
        if keyword in question_norm:
            found["pests"].append(keyword)
    
    # Extract crops
    for keyword in crop_keywords:
        if keyword in question_norm:
            found["crops"].append(keyword)
    
    # Extract product-related
    for keyword in product_keywords:
        if keyword in question_norm:
            found["products"].append(keyword)
            found["is_product_query"] = True

    # Extract fertilizer-related (NEW)
    for keyword in fertilizer_keywords:
        if keyword in question_norm:
            found["fertilizers"].append(keyword)
            found["is_fertilizer_query"] = True
            found["is_product_query"] = True

    # Detect intent (NEW)
    # Detect intent (MATCH ON NORMALIZED TEXT)
    for intent, keywords in intent_keywords.items():
        for keyword in keywords:
            if keyword in question_norm:
                found["intent"] = intent
                found["is_product_query"] = True
                break
        if found["intent"]:
            break
    
    return found


def validate_numbers_against_source(answer: str, source_docs: list) -> dict:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç+‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô source documents ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    Phase 1: logging only ‚Äî ‡πÄ‡∏Å‡πá‡∏ö data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå

    Args:
        answer: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å LLM
        source_docs: list ‡∏Ç‡∏≠‡∏á documents (‡∏°‡∏µ .metadata)

    Returns:
        {"valid": bool, "mismatches": [{"number": str, "unit": str, "found_in_source": bool}]}
    """
    import logging as _log
    _logger = _log.getLogger(__name__)

    # 1. ‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç+‡∏´‡∏ô‡πà‡∏ß‡∏¢ ‡∏à‡∏≤‡∏Å source docs
    _UNIT_PATTERN = r'(\d+(?:[.,]\d+)?)\s*(‡∏°‡∏•\.|‡∏°‡∏¥‡∏•‡∏•‡∏¥‡∏•‡∏¥‡∏ï‡∏£|‡∏Å‡∏£‡∏±‡∏°|‡∏•‡∏¥‡∏ï‡∏£|‡πÑ‡∏£‡πà|%|‡∏Å‡∏Å\.|‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°|‡∏ß‡∏±‡∏ô|‡∏ã‡∏µ‡∏ã‡∏µ|‡∏ä‡πâ‡∏≠‡∏ô|‡πÅ‡∏Å‡πâ‡∏ß|‡∏Ç‡∏ß‡∏î|‡∏ñ‡∏∏‡∏á|‡∏Å‡∏£‡∏∞‡∏™‡∏≠‡∏ö)'
    source_numbers = set()
    fields_to_check = ['usage_rate', 'how_to_use', 'package_size', 'active_ingredient', 'selling_point']
    for doc in source_docs:
        meta = doc.metadata if hasattr(doc, 'metadata') else (doc if isinstance(doc, dict) else {})
        for field in fields_to_check:
            text = str(meta.get(field, ''))
            if text:
                for m in re.finditer(_UNIT_PATTERN, text):
                    # Normalize: remove commas
                    num = m.group(1).replace(',', '')
                    source_numbers.add(num)

    # 2. ‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç+‡∏´‡∏ô‡πà‡∏ß‡∏¢ ‡∏à‡∏≤‡∏Å answer
    answer_matches = list(re.finditer(_UNIT_PATTERN, answer))
    mismatches = []
    for m in answer_matches:
        num = m.group(1).replace(',', '')
        unit = m.group(2)
        found = num in source_numbers
        mismatches.append({
            "number": num,
            "unit": unit,
            "found_in_source": found
        })

    valid = all(item["found_in_source"] for item in mismatches) if mismatches else True

    if not valid:
        bad = [f'{item["number"]}{item["unit"]}' for item in mismatches if not item["found_in_source"]]
        _logger.warning(f"[NumberCheck] Mismatched numbers in answer: {bad}")
    else:
        _logger.info(f"[NumberCheck] All {len(mismatches)} numbers validated OK")

    return {"valid": valid, "mismatches": mismatches}


