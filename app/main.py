"""
LINE Plant Disease Detection Bot with Google Gemini Vision and Supabase RAG
Production-grade FastAPI implementation with Multi-Agent System
"""

import os
import logging
import time
from typing import Dict, List, Optional, Any
from fastapi import FastAPI, Request, HTTPException, Header
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import httpx
import base64
import hashlib
import hmac
import json
from dotenv import load_dotenv
from supabase import create_client, Client
import google.generativeai as genai
from PIL import Image
import io
from sentence_transformers import SentenceTransformer

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# LightRAG - removed (not used)

# Initialize FastAPI app
app = FastAPI(
    title="LINE Plant Disease Detection Bot",
    description="AI-powered plant disease detection with Multi-Agent System",
    version="1.0.0"
)

# ============================================================================#
# ENVIRONMENT / SERVICES
# ============================================================================#
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

required_env_vars = {
    "LINE_CHANNEL_ACCESS_TOKEN": LINE_CHANNEL_ACCESS_TOKEN,
    "LINE_CHANNEL_SECRET": LINE_CHANNEL_SECRET,
    "GEMINI_API_KEY": GEMINI_API_KEY,
    "SUPABASE_URL": SUPABASE_URL,
    "SUPABASE_KEY": SUPABASE_KEY,
}
for var_name, var_value in required_env_vars.items():
    if not var_value:
        logger.error(f"Missing required environment variable: {var_name}")

# Initialize Gemini (for Vision)
gemini_model = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')
    logger.info("Gemini initialized successfully (gemini-2.5-flash)")

# Initialize E5 model for embeddings (768 dimensions)
e5_model = None
try:
    e5_model = SentenceTransformer('intfloat/multilingual-e5-base')
    logger.info("E5 model initialized successfully (768 dimensions)")
except Exception as e:
    logger.warning(f"E5 model initialization failed: {e}")

# Initialize Supabase (fallback)
supabase_client: Client = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)
        logger.info("Supabase initialized successfully (fallback)")
    except Exception as e:
        logger.error(f"Failed to initialize Supabase: {e}")

# Using Supabase Vector Search with Gemini filtering
logger.info("Using Supabase Vector Search + Gemini Filtering")

# ============================================================================#
# Memory System (Supabase-based)
# ============================================================================#
# In-memory store for pending image contexts awaiting user symptom input
# Keyed by user_id -> dict with image_bytes and reply_token (optional)
pending_image_contexts: Dict[str, Dict[str, Any]] = {}

# Memory configuration
MAX_MEMORY_MESSAGES = 10  # Keep last 10 messages for context
MEMORY_CONTEXT_WINDOW = 5  # Use last 5 messages for context

async def add_to_memory(user_id: str, role: str, content: str, metadata: dict = None):
    """Add message to conversation memory in Supabase"""
    try:
        if not supabase_client:
            logger.warning("Supabase not available, skipping memory storage")
            return
        
        # Truncate very long messages
        truncated_content = content[:2000] if len(content) > 2000 else content
        
        data = {
            "user_id": user_id,
            "role": role,  # "user" or "assistant"
            "content": truncated_content,
            "metadata": metadata or {}
        }
        
        result = supabase_client.table('conversation_memory').insert(data).execute()
        logger.info(f"‚úì Added to memory: {role} message for user {user_id[:8]}...")
        
        # Clean up old messages (keep last N per user)
        await cleanup_old_memory(user_id)
        
    except Exception as e:
        logger.error(f"Failed to add to memory: {e}")

async def get_conversation_context(user_id: str, limit: int = MEMORY_CONTEXT_WINDOW) -> str:
    """Get conversation history as context string from Supabase"""
    try:
        if not supabase_client:
            return ""
        
        # Get last N messages for this user
        result = supabase_client.table('conversation_memory')\
            .select('role, content, created_at')\
            .eq('user_id', user_id)\
            .order('created_at', desc=True)\
            .limit(limit)\
            .execute()
        
        if not result.data:
            return ""
        
        # Reverse to get chronological order
        messages = list(reversed(result.data))
        
        context_parts = []
        for msg in messages:
            role = "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ" if msg["role"] == "user" else "‡∏â‡∏±‡∏ô"
            content = msg["content"][:150]  # Truncate for context
            context_parts.append(f"{role}: {content}")
        
        logger.info(f"‚úì Retrieved {len(messages)} messages from memory")
        return "\n".join(context_parts)
        
    except Exception as e:
        logger.error(f"Failed to get conversation context: {e}")
        return ""

async def cleanup_old_memory(user_id: str):
    """Keep only last N messages per user"""
    try:
        if not supabase_client:
            return
        
        # Get all message IDs for this user, ordered by created_at desc
        result = supabase_client.table('conversation_memory')\
            .select('id')\
            .eq('user_id', user_id)\
            .order('created_at', desc=True)\
            .execute()
        
        if not result.data or len(result.data) <= MAX_MEMORY_MESSAGES:
            return
        
        # Get IDs to delete (keep only last MAX_MEMORY_MESSAGES)
        ids_to_keep = [msg['id'] for msg in result.data[:MAX_MEMORY_MESSAGES]]
        ids_to_delete = [msg['id'] for msg in result.data[MAX_MEMORY_MESSAGES:]]
        
        if ids_to_delete:
            # Delete old messages
            supabase_client.table('conversation_memory')\
                .delete()\
                .in_('id', ids_to_delete)\
                .execute()
            logger.info(f"‚úì Cleaned up {len(ids_to_delete)} old messages for user {user_id[:8]}...")
            
    except Exception as e:
        logger.error(f"Failed to cleanup old memory: {e}")

async def clear_memory(user_id: str):
    """Clear all conversation memory for user"""
    try:
        if not supabase_client:
            logger.warning("Supabase not available")
            return
        
        result = supabase_client.table('conversation_memory')\
            .delete()\
            .eq('user_id', user_id)\
            .execute()
        
        logger.info(f"‚úì Cleared memory for user {user_id[:8]}...")
        
    except Exception as e:
        logger.error(f"Failed to clear memory: {e}")

async def get_memory_stats(user_id: str) -> dict:
    """Get memory statistics for user"""
    try:
        if not supabase_client:
            return {"total": 0, "user_messages": 0, "assistant_messages": 0}
        
        result = supabase_client.table('conversation_memory')\
            .select('role')\
            .eq('user_id', user_id)\
            .execute()
        
        if not result.data:
            return {"total": 0, "user_messages": 0, "assistant_messages": 0}
        
        user_count = sum(1 for msg in result.data if msg['role'] == 'user')
        assistant_count = sum(1 for msg in result.data if msg['role'] == 'assistant')
        
        return {
            "total": len(result.data),
            "user_messages": user_count,
            "assistant_messages": assistant_count
        }
        
    except Exception as e:
        logger.error(f"Failed to get memory stats: {e}")
        return {"total": 0, "user_messages": 0, "assistant_messages": 0}

# ============================================================================#
# Pydantic Models
# ============================================================================#
class DiseaseDetectionResult(BaseModel):
    disease_name: str
    confidence: str
    symptoms: str
    severity: str
    raw_analysis: str

class ProductRecommendation(BaseModel):
    product_name: str
    active_ingredient: Optional[str] = ""
    target_pest: Optional[str] = ""
    applicable_crops: Optional[str] = ""
    how_to_use: Optional[str] = ""
    usage_rate: Optional[str] = ""
    score: float = 0.0

# ============================================================================#
# Helpers
# ============================================================================#
def clean_knowledge_text(text: str) -> str:
    """Clean and format knowledge text for better readability"""
    if not text:
        return ""
    
    import re
    
    # Fix encoding issues - remove corrupted characters
    # Common patterns: ‡∏àƒû‡∏≥, ‡∏•ƒû‡∏≥, ‡∏óƒû‡∏≥, ‡∏ôƒû‡πâ‡∏≥, ‡∏Åƒû‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡∏≥])', r'\1\2', text)  # ‡∏àƒû‡∏≥ ‚Üí ‡∏à‡∏≥
    text = re.sub(r'([‡∏Å-‡∏Æ])ƒû([‡πâ])', r'\1\2', text)  # ‡∏ôƒû‡πâ ‚Üí ‡∏ô‡πâ
    text = re.sub(r'ƒû', '', text)  # Remove remaining ƒû
    
    # Fix other corrupted characters
    text = text.replace('‡∏ï‡πâ', '‡∏ï‡πâ')  # Fix tone marks
    text = text.replace('‡∏ï', '‡∏ï')
    
    # Remove excessive whitespace
    text = ' '.join(text.split())
    
    # Fix common issues
    text = text.replace('  ', ' ')  # Double spaces
    text = text.replace(' ,', ',')  # Space before comma
    text = text.replace(' .', '.')  # Space before period
    text = text.replace('( ', '(')  # Space after opening parenthesis
    text = text.replace(' )', ')')  # Space before closing parenthesis
    
    # Fix Thai-specific issues
    text = text.replace('‡∏∫', '')  # Remove Thai character above
    text = text.replace('‡πå', '')  # Remove Thai character above (optional - keep for now)
    
    # Remove multiple consecutive spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence spacing
    text = re.sub(r'([.!?])\s*([A-Za-z‡∏Å-‡πô])', r'\1 \2', text)
    
    # Remove leading/trailing whitespace
    text = text.strip()
    
    return text

def verify_line_signature(body: bytes, signature: str) -> bool:
    if not LINE_CHANNEL_SECRET:
        logger.warning("LINE_CHANNEL_SECRET not set, skipping signature verification")
        return True
    hash_digest = hmac.new(
        LINE_CHANNEL_SECRET.encode('utf-8'),
        body,
        hashlib.sha256
    ).digest()
    expected_signature = base64.b64encode(hash_digest).decode('utf-8')
    return hmac.compare_digest(signature, expected_signature)

async def get_image_content_from_line(message_id: str) -> bytes:
    url = f"https://api-data.line.me/v2/bot/message/{message_id}/content"
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"}
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.get(url, headers=headers)
        response.raise_for_status()
        return response.content



# ============================================================================#
# Core: Detect disease (Gemini Vision)
# ============================================================================#
async def detect_disease(image_bytes: bytes, extra_user_info: Optional[str] = None) -> DiseaseDetectionResult:
    logger.info("Starting pest/disease detection with Gemini Vision")
    try:
        # Convert bytes to PIL Image for Gemini
        image = Image.open(io.BytesIO(image_bytes))
        
        prompt = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡πÑ‡∏ó‡∏¢ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå 20 ‡∏õ‡∏µ

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

üìã **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå**:
1. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ö‡∏ô‡πÉ‡∏ö/‡∏•‡∏≥‡∏ï‡πâ‡∏ô/‡∏ú‡∏• ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏µ ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
3. ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏°‡∏•‡∏á ‡πÑ‡∏Ç‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä
4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢

üîç **‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó**:
- **‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤ (Fungus)**: ‡∏à‡∏∏‡∏î‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•/‡∏î‡∏≥, ‡πÅ‡∏ú‡∏•‡πÄ‡∏õ‡∏µ‡∏¢‡∏Å, ‡∏£‡∏≤‡∏Ç‡∏≤‡∏ß, ‡πÉ‡∏ö‡πÑ‡∏´‡∏°‡πâ
- **‡πÑ‡∏ß‡∏£‡∏±‡∏™ (Virus)**: ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á, ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å, ‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏ö‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡πÅ‡∏Ñ‡∏£‡∏∞‡πÅ‡∏Å‡∏£‡πá‡∏ô
- **‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä (Pest)**: ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á, ‡∏£‡∏≠‡∏¢‡∏Å‡∏±‡∏î, ‡πÉ‡∏ö‡∏°‡πâ‡∏ß‡∏ô, ‡∏°‡∏µ‡πÄ‡∏¢‡∏∑‡πà‡∏≠‡πÉ‡∏¢
- **‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä (Weed)**: ‡∏û‡∏∑‡∏ä‡πÅ‡∏õ‡∏•‡∏Å‡∏õ‡∏•‡∏≠‡∏°‡πÉ‡∏ô‡πÅ‡∏õ‡∏•‡∏á

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏†‡∏≤‡∏û
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏ confidence ‡∏ï‡πà‡∏≥
- ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"

üì§ **‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):

{
  "disease_name": "‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü, ‡πÅ‡∏≠‡∏ô‡πÅ‡∏ó‡∏£‡∏Ñ‡πÇ‡∏ô‡∏™, ‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á",
  "pest_type": "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤/‡πÑ‡∏ß‡∏£‡∏±‡∏™/‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä/‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä",
  "confidence_level_percent": 0-100,
  "confidence": "‡∏™‡∏π‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏ï‡πà‡∏≥",
  "symptoms_in_image": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏û (‡∏™‡∏±‡πâ‡∏ô‡πÜ)",
  "symptoms": "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏ß‡∏°‡∏™‡∏µ ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡∏Ç‡∏ô‡∏≤‡∏î",
  "possible_cause": "‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á",
  "severity_level": "‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢",
  "severity": "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•",
  "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô",
  "affected_area": "‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö",
  "spread_risk": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (‡∏™‡∏π‡∏á/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏ï‡πà‡∏≥)"
}

‚úÖ ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤: disease_name = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤", confidence = "‡∏™‡∏π‡∏á" """

        # If user provided extra observation text, include it as additional context
        if extra_user_info:
            prompt += f"\n\n‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {extra_user_info}"

        # Call Gemini with image
        response = gemini_model.generate_content([prompt, image])
        raw_text = response.text
        logger.info(f"Gemini raw response: {raw_text}")

        # Extract JSON flexibly
        try:
            json_str = raw_text.strip()
            if json_str.startswith("```"):
                # remove code fences and find JSON part
                parts = json_str.split("```")
                for p in parts:
                    p_s = p.strip()
                    if p_s.startswith("{") and p_s.endswith("}"):
                        json_str = p_s
                        break
            # find first { ... } block if extra text present
            if "{" in json_str and "}" in json_str:
                start = json_str.find("{")
                end = json_str.rfind("}") + 1
                json_str = json_str[start:end]
            data = json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to parse JSON from Gemini response: {e}", exc_info=True)
            data = {"disease_name": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ", "confidence": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "symptoms": "", "severity": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "description": raw_text}

        # Map many possible keys to canonical fields
        disease_name = data.get("disease_name") or data.get("disease") or data.get("‡πÇ‡∏£‡∏Ñ") or "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ"
        # confidence prefer numeric percent if provided
        confidence = ""
        if "confidence_level_percent" in data:
            confidence = str(data.get("confidence_level_percent"))
        elif "confidence" in data:
            confidence = str(data.get("confidence"))
        elif "confidence_percent" in data:
            confidence = str(data.get("confidence_percent"))
        else:
            confidence = "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"
        # symptoms
        symptoms = data.get("symptoms_in_image") or data.get("symptoms") or data.get("‡∏≠‡∏≤‡∏Å‡∏≤‡∏£") or ""
        # severity
        severity = data.get("severity_level") or data.get("severity") or data.get("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á") or "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"
        # description / raw
        description = data.get("description") or data.get("possible_cause") or raw_text

        # Extract pest_type
        pest_type = data.get("pest_type") or "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä"
        
        # Extract additional fields for better analysis
        affected_area = data.get("affected_area") or ""
        spread_risk = data.get("spread_risk") or ""
        
        # Build comprehensive raw_analysis
        raw_parts = [f"{pest_type}: {description}"]
        if affected_area:
            raw_parts.append(f"‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö: {affected_area}")
        if spread_risk:
            raw_parts.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà: {spread_risk}")
        
        result = DiseaseDetectionResult(
            disease_name=str(disease_name),
            confidence=str(confidence),
            symptoms=str(symptoms),
            severity=str(severity),
            raw_analysis=" | ".join(raw_parts)
        )
        
        # Check confidence level and warn if low
        confidence_num = 0
        try:
            if confidence.replace("%", "").replace("‡∏™‡∏π‡∏á", "90").replace("‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "60").replace("‡∏ï‡πà‡∏≥", "30").isdigit():
                confidence_num = int(confidence.replace("%", "").replace("‡∏™‡∏π‡∏á", "90").replace("‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "60").replace("‡∏ï‡πà‡∏≥", "30"))
        except:
            pass
        
        if confidence_num < 50 or "‡∏ï‡πà‡∏≥" in confidence:
            logger.warning(f"Low confidence detection: {result.disease_name} ({confidence})")
        
        logger.info(f"Pest/Disease detected: {result.disease_name} (Type: {pest_type}, Confidence: {confidence})")
        
        # Log detection for analysis (optional - can be used to improve accuracy)
        try:
            import datetime
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "disease_name": result.disease_name,
                "pest_type": pest_type,
                "confidence": confidence,
                "severity": result.severity,
                "has_user_input": bool(extra_user_info)
            }
            # Could save to file or database for later analysis
            logger.debug(f"Detection log: {log_entry}")
        except Exception as e:
            logger.warning(f"Failed to log detection: {e}")
        
        return result

    except Exception as e:
        logger.error(f"Error in pest/disease detection: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Detection failed: {str(e)}")

# ============================================================================#
# Core: Retrieve product recommendations (LightRAG or Supabase fallback)
# ============================================================================#
async def retrieve_product_recommendation(disease_info: DiseaseDetectionResult) -> List[ProductRecommendation]:
    """
    Query products using Vector Search + Gemini filtering
    Returns top 3-5 most relevant products
    """
    try:
        logger.info("üîç Retrieving products with Vector Search + Gemini Filter")

        if not supabase_client:
            logger.warning("Supabase not configured")
            return []

        disease_name = disease_info.disease_name
        logger.info(f"üìù Searching products for: {disease_name}")
        
        # Strategy 1: Vector search by disease name (most accurate)
        try:
            if e5_model:
                # Generate embedding for disease name
                query_text = f"query: {disease_name}"
                query_embedding = e5_model.encode(query_text, normalize_embeddings=True).tolist()
                logger.info("‚úì Product query embedding generated")
                
                # Vector search in products table
                result = supabase_client.rpc(
                    'match_products',
                    {
                        'query_embedding': query_embedding,
                        'match_threshold': 0.3,  # Lower threshold for more candidates
                        'match_count': 15  # Get more candidates for Gemini filtering
                    }
                ).execute()
                
                if result.data and len(result.data) > 0:
                    logger.info(f"‚úì Found {len(result.data)} product candidates via vector search")
                    
                    # Use Gemini to filter and rank products
                    filtered_products = await filter_products_with_gemini(
                        disease_name,
                        disease_info.raw_analysis,
                        result.data
                    )
                    
                    if filtered_products:
                        logger.info(f"‚úì Gemini filtered {len(filtered_products)} relevant products")
                        return filtered_products
                    else:
                        logger.warning("‚ö†Ô∏è Gemini filtering returned no products, using top vector results")
                        # Fallback: use top vector search results
                        return build_recommendations_from_data(result.data[:6])
                else:
                    logger.info("No products found via vector search, trying keyword search")
            else:
                logger.warning("E5 model not available, using keyword search")
        except Exception as e:
            logger.warning(f"Vector search failed: {e}, trying keyword search")
        
        # Strategy 2: Keyword search fallback
        matches_data = []
        
        # Search in target_pest field
        try:
            result = supabase_client.table('products')\
                .select('*')\
                .ilike('target_pest', f'%{disease_name}%')\
                .limit(10)\
                .execute()
            
            if result.data:
                matches_data.extend(result.data)
                logger.info(f"Found {len(result.data)} products in target_pest")
        except Exception as e:
            logger.warning(f"target_pest search failed: {e}")
        
        # If no results, search by pest type
        if not matches_data:
            try:
                pest_keywords = []
                if "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä"]
                elif "‡πÑ‡∏ß‡∏£‡∏±‡∏™" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÑ‡∏ß‡∏£‡∏±‡∏™"]
                elif "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä" in disease_info.raw_analysis or "‡πÅ‡∏°‡∏•‡∏á" in disease_info.raw_analysis:
                    pest_keywords = ["‡πÅ‡∏°‡∏•‡∏á", "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢"]
                elif "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä" in disease_info.raw_analysis:
                    pest_keywords = ["‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤"]
                
                for keyword in pest_keywords:
                    result = supabase_client.table('products')\
                        .select('*')\
                        .ilike('target_pest', f'%{keyword}%')\
                        .limit(5)\
                        .execute()
                    
                    if result.data:
                        matches_data.extend(result.data)
                        logger.info(f"Found {len(result.data)} products for keyword: {keyword}")
                        break
                        
            except Exception as e:
                logger.warning(f"Keyword search failed: {e}")
        
        if not matches_data:
            logger.warning("No products found with any search strategy")
            return []
        
        logger.info(f"Total products found: {len(matches_data)}")
        return build_recommendations_from_data(matches_data[:6])

    except Exception as e:
        logger.error(f"Product search failed: {e}", exc_info=True)
        return []

async def filter_products_with_gemini(disease_name: str, raw_analysis: str, product_candidates: List[Dict]) -> List[ProductRecommendation]:
    """Use Gemini to filter and rank the most relevant products"""
    try:
        if not gemini_model:
            return []
        
        # Build product list for Gemini
        products_text = ""
        for idx, p in enumerate(product_candidates[:10], 1):  # Top 10 candidates
            products_text += f"\n[{idx}] {p.get('product_name', 'N/A')}\n"
            products_text += f"   ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {p.get('active_ingredient', 'N/A')}\n"
            products_text += f"   ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä: {p.get('target_pest', 'N/A')[:100]}\n"
            products_text += f"   Similarity: {p.get('similarity', 0):.0%}\n"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "{disease_name}"

üìä **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏£‡∏Ñ**:
{raw_analysis}

üì¶ **‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö**:
{products_text}

üìã **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**:
1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏£‡∏Ñ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 3-5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
3. ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
4. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡∏Ç‡∏≠‡∏á product index ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‚ö†Ô∏è **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å**:
- ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
- ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏£‡∏Ñ (‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤/‡πÅ‡∏°‡∏•‡∏á/‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä)
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):
[1, 3, 5]"""

        response = gemini_model.generate_content(prompt)
        result_text = response.text.strip()
        
        # Parse JSON response
        try:
            # Extract JSON array
            if "[" in result_text and "]" in result_text:
                start = result_text.find("[")
                end = result_text.rfind("]") + 1
                json_str = result_text[start:end]
                selected_indices = json.loads(json_str)
                
                if selected_indices and len(selected_indices) > 0:
                    # Build recommendations from selected products
                    recommendations = []
                    for idx in selected_indices[:5]:  # Max 5
                        if 1 <= idx <= len(product_candidates):
                            product = product_candidates[idx - 1]
                            rec = ProductRecommendation(
                                product_name=product.get('product_name', 'N/A'),
                                active_ingredient=product.get('active_ingredient', ''),
                                target_pest=product.get('target_pest', ''),
                                applicable_crops=product.get('applicable_crops', ''),
                                how_to_use=product.get('how_to_use', ''),
                                usage_rate=product.get('usage_rate', ''),
                                score=product.get('similarity', 0.8)
                            )
                            recommendations.append(rec)
                    
                    logger.info(f"‚úì Gemini selected {len(recommendations)} products")
                    return recommendations
                else:
                    logger.warning("Gemini returned empty selection")
                    return []
            else:
                logger.warning(f"Invalid Gemini response format: {result_text[:100]}")
                return []
                
        except Exception as e:
            logger.error(f"Failed to parse Gemini response: {e}")
            return []
        
    except Exception as e:
        logger.error(f"Gemini product filtering failed: {e}")
        return []

def build_recommendations_from_data(products_data: List[Dict]) -> List[ProductRecommendation]:
    """Build ProductRecommendation list from raw data"""
    recommendations = []
    seen_products = set()
    
    for product in products_data:
        pname = product.get("product_name", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠")
        
        if pname in seen_products:
            continue
        seen_products.add(pname)
        
        pest = product.get("target_pest", "")
        if not pest or pest.strip() == "":
            continue
        
        rec = ProductRecommendation(
            product_name=pname,
            active_ingredient=product.get("active_ingredient", ""),
            target_pest=pest,
            applicable_crops=product.get("applicable_crops", ""),
            how_to_use=product.get("how_to_use", ""),
            usage_rate=product.get("usage_rate", ""),
            score=product.get("similarity", 0.7)
        )
        recommendations.append(rec)
    
    return recommendations



# ============================================================================#
# Core: Smart Q&A - Answer questions using Knowledge Base
# ============================================================================#
async def answer_question_with_knowledge(question: str) -> str:
    """Answer user questions using knowledge base and Gemini"""
    try:
        logger.info(f"Answering question: {question[:50]}...")
        
        if not supabase_client or not gemini_model:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ"
        
        # 1. Generate embedding for the question using E5 model (768 dimensions)
        try:
            if e5_model:
                # E5 requires "query: " prefix for queries
                query_text = f"query: {question}"
                query_embedding = e5_model.encode(query_text, normalize_embeddings=True).tolist()
                logger.info("‚úì Question embedding generated (E5, 768 dim)")
            else:
                logger.warning("E5 model not available, using keyword search")
                return await answer_with_keyword_search(question)
        except Exception as e:
            logger.warning(f"Failed to generate E5 embedding: {e}")
            # Fallback to keyword search
            return await answer_with_keyword_search(question)
        
        # 2. Search knowledge base using vector search
        try:
            result = supabase_client.rpc(
                'match_knowledge',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.3,  # Lower threshold for more results
                    'match_count': 10  # Get more candidates
                }
            ).execute()
            
            if result.data:
                logger.info(f"‚úì Found {len(result.data)} relevant knowledge entries")
                # Combine knowledge content
                knowledge_texts = []
                for item in result.data:
                    content = item.get('content', '').strip()
                    similarity = item.get('similarity', 0)
                    # Lower filter threshold to get more results
                    if content and similarity > 0.3:
                        # Clean the text before adding
                        cleaned_content = clean_knowledge_text(content)
                        if cleaned_content:
                            knowledge_texts.append(cleaned_content)
                
                if knowledge_texts:
                    combined_knowledge = "\n\n".join(knowledge_texts[:5])  # Top 5 for better context
                else:
                    # Fallback to keyword search
                    return await answer_with_keyword_search(question)
            else:
                # Fallback to keyword search
                return await answer_with_keyword_search(question)
                
        except Exception as e:
            logger.warning(f"Vector search failed: {e}")
            return await answer_with_keyword_search(question)
        
        # 3. Search for relevant products
        products_info = ""
        try:
            # Extract keywords for product search
            keywords = extract_keywords_from_question(question)
            if keywords:
                product_result = supabase_client.table('products')\
                    .select('product_name, active_ingredient, target_pest, how_to_use , usage_rate')\
                    .ilike('target_pest', f'%{keywords[0]}%')\
                    .limit(3)\
                    .execute()
                
                if product_result.data:
                    products_list = []
                    for p in product_result.data:
                        products_list.append(
                            f"- {p.get('product_name')}: {p.get('active_ingredient', 'N/A')}"
                        )
                    products_info = "\n".join(products_list)
        except Exception as e:
            logger.warning(f"Product search failed: {e}")
        
        # 4. Use Gemini to generate natural answer
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡πÑ‡∏ó‡∏¢ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå 20 ‡∏õ‡∏µ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£: {question}

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
{combined_knowledge}

‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{products_info if products_info else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå"}

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. **‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
2. **‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö** ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
   - ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏/‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
   - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏à‡∏±‡∏î/‡∏£‡∏±‡∏Å‡∏©‡∏≤
   - ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
3. **‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á** ‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
4. **‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
5. **‡πÄ‡∏û‡∏¥‡πà‡∏° emoji** ‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô (üå± üêõ üçÑ üíä ‚ö†Ô∏è)
6. **‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°** ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á
7. **‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ markdown

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:"""

        try:
            response = gemini_model.generate_content(prompt)
            answer = response.text.strip()
            
            # Clean up markdown if any
            answer = answer.replace("```", "").replace("**", "")
            
            logger.info("‚úì Answer generated successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            # Return knowledge directly
            return f"üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n\n{combined_knowledge[:500]}...\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡πà‡∏∞"
        
    except Exception as e:
        logger.error(f"Error in Q&A: {e}", exc_info=True)
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"

async def answer_with_keyword_search(question: str) -> str:
    """Fallback: Answer using keyword search"""
    try:
        # Extract main keywords
        keywords = extract_keywords_from_question(question)
        
        if not keywords:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"
        
        # Search in knowledge table
        result = supabase_client.table('knowledge')\
            .select('content')\
            .ilike('content', f'%{keywords[0]}%')\
            .limit(2)\
            .execute()
        
        if result.data:
            # Clean and format knowledge
            cleaned_items = []
            for item in result.data:
                content = item.get('content', '')
                cleaned = clean_knowledge_text(content)
                if cleaned:
                    cleaned_items.append(cleaned[:300])
            
            knowledge = "\n\n".join(cleaned_items)
            return f"üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n\n{knowledge}\n\nüí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡πà‡∏∞"
        else:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞ üå±"
            
    except Exception as e:
        logger.error(f"Keyword search failed: {e}")
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏∞"

def extract_keywords_from_question(question: str) -> list:
    """Extract main keywords from question"""
    # Common disease/pest keywords
    keywords = [
        "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡∏≠‡πà‡∏≠‡∏ô", "‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢", "‡∏´‡∏ô‡∏≠‡∏ô", "‡πÅ‡∏°‡∏•‡∏á",
        "‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á", "‡∏£‡∏≤‡πÅ‡∏õ‡πâ‡∏á", "‡∏£‡∏≤‡∏™‡∏ô‡∏¥‡∏°", "‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤", "‡∏£‡∏≤",
        "‡πÑ‡∏ß‡∏£‡∏±‡∏™", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏î‡πà‡∏≤‡∏á", "‡πÇ‡∏£‡∏Ñ‡πÉ‡∏ö‡∏´‡∏á‡∏¥‡∏Å",
        "‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä", "‡∏´‡∏ç‡πâ‡∏≤",
        "‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä", "‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä",
        "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏û‡∏∑‡∏ä‡∏ú‡∏±‡∏Å"
    ]
    
    found_keywords = []
    question_lower = question.lower()
    
    for keyword in keywords:
        if keyword in question_lower:
            found_keywords.append(keyword)
    
    return found_keywords[:3]  # Return top 3

# ============================================================================#
# Core: Retrieve knowledge from knowledge table (Vector Search)
# ============================================================================#
async def retrieve_knowledge_from_knowledge_table(disease_info: DiseaseDetectionResult) -> str:
    """Query knowledge table using vector search + Gemini filtering for disease information"""
    try:
        if not supabase_client:
            return ""
        
        logger.info(f"üîç Searching knowledge for: {disease_info.disease_name}")
        
        # Strategy 1: Search by exact disease name first (most accurate)
        query_text = disease_info.disease_name
        logger.info(f"üìù Primary query: {query_text}")
        
        # Generate embedding using E5 model (768 dimensions)
        try:
            if e5_model:
                query_with_prefix = f"query: {query_text}"
                query_embedding = e5_model.encode(query_with_prefix, normalize_embeddings=True).tolist()
                logger.info("‚úì Embedding generated (E5, 768 dim)")
            else:
                logger.warning("E5 model not available, using keyword search")
                return await retrieve_knowledge_keyword_search(disease_info)
        except Exception as e:
            logger.warning(f"Failed to generate E5 embedding: {e}")
            return await retrieve_knowledge_keyword_search(disease_info)
        
        # Vector similarity search
        try:
            result = supabase_client.rpc(
                'match_knowledge',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.4,  # Lower threshold to get more candidates
                    'match_count': 10  # Get more results for Gemini to filter
                }
            ).execute()
            
            if result.data and len(result.data) > 0:
                logger.info(f"‚úì Found {len(result.data)} knowledge candidates")
                
                # Collect all knowledge content for Gemini filtering
                knowledge_candidates = []
                for idx, item in enumerate(result.data, 1):
                    content = item.get('content', '').strip()
                    similarity = item.get('similarity', 0)
                    if content and len(content) > 20:
                        cleaned_content = clean_knowledge_text(content)
                        knowledge_candidates.append({
                            'index': idx,
                            'content': cleaned_content,
                            'similarity': similarity
                        })
                
                if knowledge_candidates:
                    # Use Gemini to filter and synthesize the most relevant knowledge
                    filtered_knowledge = await filter_knowledge_with_gemini(
                        disease_info.disease_name,
                        knowledge_candidates
                    )
                    
                    if filtered_knowledge:
                        logger.info(f"‚úì Gemini filtered knowledge successfully")
                        return filtered_knowledge
                    else:
                        # Fallback: return top 2 by similarity
                        logger.info("‚ö†Ô∏è Gemini filtering failed, using top results")
                        top_results = sorted(knowledge_candidates, key=lambda x: x['similarity'], reverse=True)[:2]
                        return "\n\n".join([k['content'][:300] + "..." if len(k['content']) > 300 else k['content'] for k in top_results])
            else:
                logger.info("No knowledge found via vector search, trying keyword search")
                return await retrieve_knowledge_keyword_search(disease_info)
                
        except Exception as e:
            logger.warning(f"Vector search failed: {e}, trying keyword search")
            return await retrieve_knowledge_keyword_search(disease_info)
        
        return ""
        
    except Exception as e:
        logger.warning(f"Failed to retrieve knowledge: {e}")
        return ""

async def filter_knowledge_with_gemini(disease_name: str, knowledge_candidates: List[Dict]) -> str:
    """Use Gemini to filter and synthesize the most relevant knowledge"""
    try:
        if not gemini_model:
            return ""
        
        # Build prompt with all candidates
        candidates_text = ""
        for k in knowledge_candidates[:5]:  # Top 5 candidates
            candidates_text += f"\n[{k['index']}] (Similarity: {k['similarity']:.0%})\n{k['content'][:400]}\n"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä

üéØ **‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à**: ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö "{disease_name}"

üìö **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏û‡∏ö**:
{candidates_text}

üìã **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**:
1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö "{disease_name}"
3. ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 250 ‡∏Ñ‡∏≥)
4. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞, ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏à‡∏±‡∏î
5. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
6. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ markdown ‡∏´‡∏£‡∏∑‡∏≠ bullet points

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:"""

        response = gemini_model.generate_content(prompt)
        filtered_text = response.text.strip()
        
        # Check if Gemini found relevant info
        if "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in filtered_text or "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠" in filtered_text or len(filtered_text) < 50:
            logger.warning("Gemini: No relevant knowledge found")
            return ""
        
        # Clean up markdown if any
        filtered_text = filtered_text.replace("```", "").replace("**", "").replace("##", "")
        
        logger.info(f"‚úì Gemini filtered knowledge: {len(filtered_text)} chars")
        return filtered_text
        
    except Exception as e:
        logger.error(f"Gemini filtering failed: {e}")
        return ""

async def retrieve_knowledge_keyword_search(disease_info: DiseaseDetectionResult) -> str:
    """Fallback: keyword search in knowledge table"""
    try:
        result = supabase_client.table('knowledge')\
            .select('content')\
            .ilike('content', f'%{disease_info.disease_name}%')\
            .limit(2)\
            .execute()
        
        if result.data:
            logger.info(f"‚úì Found {len(result.data)} knowledge entries via keyword search")
            knowledge_parts = []
            for item in result.data:
                content = item.get('content', '').strip()
                if content and len(content) > 20:
                    # Clean the text first
                    cleaned_content = clean_knowledge_text(content)
                    preview = cleaned_content[:250] + "..." if len(cleaned_content) > 250 else cleaned_content
                    knowledge_parts.append(preview)
            
            if knowledge_parts:
                return "\n\n".join(knowledge_parts)
        
        return ""
    except Exception as e:
        logger.warning(f"Keyword search failed: {e}")
        return ""

# ============================================================================#
# Core: Generate final response (single long text block, friendly Thai)
# ============================================================================#
async def generate_final_response(
    disease_info: DiseaseDetectionResult,
    recommendations: List[ProductRecommendation]
) -> str:
    try:
        logger.info("Generating final response (Thai friendly minimal RAG)")

        # Header: disease summary with confidence warning
        header = f"üîç ‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û: {disease_info.disease_name}\n\n"
        
        # Add confidence indicator
        confidence_str = str(disease_info.confidence)
        confidence_emoji = "üü¢" if "‡∏™‡∏π‡∏á" in confidence_str or any(str(x) in confidence_str for x in range(70, 101)) else \
                          "üü°" if "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á" in confidence_str or any(str(x) in confidence_str for x in range(50, 70)) else "üî¥"
        
        header += f"{confidence_emoji} ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {disease_info.confidence}\n"
        
        # Add warning for low confidence
        if "‡∏ï‡πà‡∏≥" in confidence_str or confidence_emoji == "üî¥":
            header += "‚ö†Ô∏è **‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç\n\n"
        
        header += f"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á: {disease_info.severity}\n\n"
        header += f"üìù ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô: {disease_info.symptoms}\n\n"
        
        # Retrieve additional knowledge from knowledge table (Vector Search)
        knowledge = await retrieve_knowledge_from_knowledge_table(disease_info)
        if knowledge:
            header += f"üìö ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n{knowledge}\n\n"

        if not recommendations:
            body = "‚ö†Ô∏è ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n\n"
            body += "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î\n\n"
            body += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà:\n"
            body += "https://www.icpladda.com/about/\n\n"
            body += "‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏ñ‡πà‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö üòä"
            return header + body

        # Build product blocks (minimal fields)
        body = "üíä ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n"
        for idx, rec in enumerate(recommendations, 1):
            body += f"\n{idx}. {rec.product_name}\n"
            if rec.active_ingredient:
                body += f"   ‚Ä¢ ‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {rec.active_ingredient}\n"
            if rec.usage_rate:
                body += f"   ‚Ä¢ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ: {rec.usage_rate}\n"
            if rec.target_pest:
                # Truncate long text
                pest_text = rec.target_pest[:80] + "..." if len(rec.target_pest) > 80 else rec.target_pest
                body += f"   ‚Ä¢ ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {pest_text}\n"
            if rec.applicable_crops:
                crops_text = rec.applicable_crops[:60] + "..." if len(rec.applicable_crops) > 60 else rec.applicable_crops
                body += f"   ‚Ä¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä: {crops_text}\n"
            # friendly short how-to: if long, take first line
            if rec.how_to_use:
                short_how = rec.how_to_use.split("\n")[0].strip()
                if len(short_how) > 80:
                    short_how = short_how[:80] + "..."
                body += f"   ‚Ä¢ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ: {short_how}\n"
            body += f"   ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {int(rec.score * 100)}%\n"

        footer = "\n" + "="*40 + "\n"
        footer += "üìã **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:\n"
        footer += "‚Ä¢ ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡∏£‡∏≤/‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ï‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏Å‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n"
        footer += "‚Ä¢ ‚úÖ ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ\n"
        footer += "‚Ä¢ ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏û‡πà‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏õ‡∏•‡∏á\n\n"
        
        footer += "ÔøΩ  **‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô**:\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏Å‡∏•‡πâ‡πÜ ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏° (‡πÉ‡∏ö ‡∏•‡∏≥‡∏ï‡πâ‡∏ô ‡∏ú‡∏•)\n"
        footer += "‚Ä¢ ‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏á‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠\n"
        footer += "‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏ô‡∏¥‡∏î‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n\n"
        
        footer += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n"
        footer += "üîó https://www.icpladda.com/about/\n\n"
        footer += "üí¨ ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ üòä"
        
        return header + body + footer

    except Exception as e:
        logger.error(f"Error generating final response: {e}", exc_info=True)
        # fallback simple template
        products_text = "\n".join([f"‚Ä¢ {p.product_name}" for p in (recommendations or [])[:3]])
        fallback = f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {disease_info.disease_name}\n\n"
        fallback += f"‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n{products_text}\n\n"
        fallback += "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: https://www.icpladda.com/about/\n"
        fallback += "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏â‡∏•‡∏≤‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ"
        return fallback

# ============================================================================#
# LINE reply helper
# ============================================================================#
async def handle_natural_conversation(user_id: str, text: str, reply_token: str) -> None:
    """Handle natural conversation using Gemini AI"""
    try:
        logger.info(f"Natural conversation: {text[:50]}...")
        
        # Get conversation context
        context = await get_conversation_context(user_id)
        
        # Build prompt for Gemini
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡∏∞‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡∏ä‡∏∑‡πà‡∏≠ "‡∏î‡∏≠‡∏Å‡πÑ‡∏°‡πâ" üå∏

üéØ **‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó**:
- ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä ‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå
- ‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡πÄ‡∏û‡∏¥‡πà‡∏° emoji ‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å üå± üêõ üçÑ üíä

üìù **‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤**:
{context if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà)"}

üí¨ **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ**: {text}

üìã **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö**:
1. **‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏à‡∏ï‡∏ô‡∏≤**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£
   - ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢ ‚Üí ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö
   - ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‚Üí ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
   - ‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‚Üí ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
   - ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‚Üí ‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥

2. **‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö**: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3-4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß)

3. **‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£**: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡πà‡∏∞", "‡∏ô‡∏∞‡∏Ñ‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö" ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó

4. **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**: ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ:
   - ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
   - ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
   - ‡∏û‡∏¥‡∏°‡∏û‡πå "‡∏ä‡πà‡∏ß‡∏¢" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

5. **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ markdown**: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°**:
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏©‡∏ï‡∏£/‡∏û‡∏∑‡∏ä
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à"

‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):"""

        # Call Gemini
        response = gemini_model.generate_content(prompt)
        answer = response.text.strip()
        
        # Clean up markdown if any
        answer = answer.replace("```", "").replace("**", "").replace("##", "")
        
        # Add to memory
        await add_to_memory(user_id, "user", text)
        await add_to_memory(user_id, "assistant", answer)
        
        # Reply
        await reply_line(reply_token, answer)
        
    except Exception as e:
        logger.error(f"Natural conversation error: {e}", exc_info=True)
        fallback = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° üòÖ\n\nüí° ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏ä‡πà‡∏ß‡∏¢' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô üå±"
        await reply_line(reply_token, fallback)

async def reply_line(reply_token: str, message: str, with_sticker: bool = False) -> None:
    """Reply to LINE with text message and optionally a sticker"""
    try:
        logger.info(f"Replying to LINE token: {reply_token[:10]}...")
        url = "https://api.line.me/v2/bot/message/reply"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"
        }
        
        # Build messages array
        messages = [{"type": "text", "text": message}]
        
        # Add sticker if requested
        if with_sticker:
            # Use LINE's free sticker packages
            # Package 446: Brown & Cony's Friendly Stickers
            sticker_message = {
                "type": "sticker",
                "packageId": "446",
                "stickerId": "1988"  # Thumbs up sticker
            }
            messages.append(sticker_message)
        
        payload = {"replyToken": reply_token, "messages": messages}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
        logger.info("Reply sent to LINE")
    except Exception as e:
        logger.error(f"Error sending LINE reply: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to send LINE reply: {str(e)}")

# ============================================================================#
# API endpoints (unchanged flow)
# ============================================================================#
@app.get("/")
async def root():
    return {"status": "ok", "service": "LINE Plant Disease Detection Bot", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "services": {
            "gemini": "ok" if GEMINI_API_KEY else "not_configured",
            "supabase": "ok" if supabase_client else "not_configured",
            "line": "ok" if LINE_CHANNEL_ACCESS_TOKEN else "not_configured"
        }
    }

@app.post("/webhook")
async def webhook(
    request: Request,
    x_line_signature: str = Header(None, alias="X-Line-Signature")
):
    try:
        body = await request.body()
        if x_line_signature and not verify_line_signature(body, x_line_signature):
            logger.warning("Invalid LINE signature")
            raise HTTPException(status_code=403, detail="Invalid signature")
        webhook_data = json.loads(body.decode("utf-8"))
        events = webhook_data.get("events", [])
        logger.info(f"Received {len(events)} events from LINE")
        for event in events:
            event_type = event.get("type")
            reply_token = event.get("replyToken")
            if event_type == "message":
                message = event.get("message", {})
                message_type = message.get("type")
                if message_type == "image":
                    # When receiving an image, store it and ask the user for additional symptoms
                    message_id = message.get("id")
                    try:
                        image_bytes = await get_image_content_from_line(message_id)
                        user_id = event.get("source", {}).get("userId") or event.get("source", {}).get("userId")
                        if user_id:
                            # store pending context
                            pending_image_contexts[user_id] = {
                                "image_bytes": image_bytes,
                                "reply_token": reply_token
                            }
                        ask_message = (
                            "‚úÖ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n"
                            "üìù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:\n\n"
                            "1Ô∏è‚É£ **‡∏ä‡∏ô‡∏¥‡∏î‡∏û‡∏∑‡∏ä**: ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á/‡∏Ç‡πâ‡∏≤‡∏ß/‡∏≠‡∏∑‡πà‡∏ô‡πÜ?\n"
                            "2Ô∏è‚É£ **‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£**: ‡πÉ‡∏ö/‡∏•‡∏≥‡∏ï‡πâ‡∏ô/‡∏ú‡∏•/‡∏£‡∏≤‡∏Å?\n"
                            "3Ô∏è‚É£ **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£**:\n"
                            "   ‚Ä¢ ‡∏™‡∏µ‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î/‡πÅ‡∏ú‡∏• (‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•/‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á/‡∏î‡∏≥/‡∏Ç‡∏≤‡∏ß)\n"
                            "   ‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢/‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏°‡∏≤‡∏Å)\n"
                            "   ‚Ä¢ ‡∏°‡∏µ‡πÅ‡∏°‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î)\n\n"
                            "4Ô∏è‚É£ **‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤**: ‡πÄ‡∏Å‡∏¥‡∏î‡∏°‡∏≤‡∏ô‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n\n"
                            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö:\n"
                            "\"‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÉ‡∏ö‡∏°‡πâ‡∏ß‡∏ô ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏• ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏°‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏µ‡∏î‡∏≥ ‡πÄ‡∏Å‡∏¥‡∏î‡∏°‡∏≤ 3 ‡∏ß‡∏±‡∏ô\"\n\n"
                            "‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å ‡∏¢‡∏¥‡πà‡∏á‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ñ‡πà‡∏∞ üéØ"
                        )
                        await reply_line(reply_token, ask_message)
                    except Exception as e:
                        logger.error(f"Error fetching image content: {e}", exc_info=True)
                        error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å LINE ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
                        await reply_line(reply_token, error_message)
                elif message_type == "text":
                    text = message.get("text", "").strip()
                    user_id = event.get("source", {}).get("userId")
                    # If user has a pending image, treat this text as symptom input
                    if user_id and user_id in pending_image_contexts:
                        ctx = pending_image_contexts.pop(user_id)
                        image_bytes = ctx.get("image_bytes")
                        try:
                            # Run detection with extra user-provided observations
                            disease_result = await detect_disease(image_bytes, extra_user_info=text)
                            recommendations = await retrieve_product_recommendation(disease_result)
                            final_message = await generate_final_response(disease_result, recommendations)
                            await reply_line(reply_token, final_message)
                        except Exception as e:
                            logger.error(f"Error processing combined image+text: {e}", exc_info=True)
                            error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏∞ üôè"
                            await reply_line(reply_token, error_message)
                    else:
                        # Natural conversation with memory
                        text_lower = text.lower()
                        
                        # Check for memory clear command
                        if any(keyword in text_lower for keyword in ["‡∏•‡∏∑‡∏°", "‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà", "clear", "reset"]):
                            await clear_memory(user_id)
                            clear_message = "‚úÖ ‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ üå±"
                            await reply_line(reply_token, clear_message)
                        
                        # Check for specific commands that need exact responses
                        elif any(keyword in text_lower for keyword in ["‡∏ä‡πà‡∏ß‡∏¢", "help", "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á"]):
                            help_message = (
                                "üå± ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Plant Disease Bot\n\n"
                                "ÔøΩ ‡∏ï‡∏£‡∏ß‡∏≥‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä:\n"
                                "1. ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n"
                                "2. ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô\n"
                                "3. ‡∏£‡∏≠ 5-10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n"
                                "4. ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå\n\n"
                                "ÔøΩ ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤/‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô:\n"
                                "‚Ä¢ \"‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢‡πÑ‡∏ü‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏¢‡∏±‡∏á‡πÑ‡∏á?\"\n"
                                "‚Ä¢ \"‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏≠‡∏∞‡πÑ‡∏£?\"\n"
                                "‚Ä¢ \"‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏ô 50 ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?\"\n\n"
                                "üéØ ‡∏â‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:\n"
                                "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä/‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä\n"
                                "‚Ä¢ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä\n"
                                "‚Ä¢ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ\n"
                                "‚Ä¢ ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥\n\n"
                                "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞! üòä"
                            )
                            await add_to_memory(user_id, "user", text)
                            await add_to_memory(user_id, "assistant", help_message)
                            await reply_line(reply_token, help_message)
                        
                        elif any(keyword in text_lower for keyword in ["‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "product"]):
                            help_message = (
                                "üì¶ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n\n"
                                "‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡∏∑‡∏ä 43 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:\n\n"
                                "üêõ ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÅ‡∏°‡∏•‡∏á (Insecticide)\n"
                                "üçÑ ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤ (Fungicide)\n"
                                "üåø ‡∏¢‡∏≤‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ß‡∏±‡∏ä‡∏û‡∏∑‡∏ä (Herbicide)\n"
                                "üå± ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï (PGR)\n\n"
                                "üìö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà:\n"
                                "üîó https://www.icpladda.com/about/\n\n"
                                "üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:\n"
                                "‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏∞ üòä"
                            )
                            await add_to_memory(user_id, "user", text)
                            await add_to_memory(user_id, "assistant", help_message)
                            await reply_line(reply_token, help_message)
                        
                        # Check if it's a specific question that needs knowledge base
                        elif any(q in text_lower for q in ["?", "‡∏¢‡∏±‡∏á‡πÑ‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏Ñ‡∏∑‡∏≠", "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á", "‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°", "‡πÉ‡∏ä‡πâ", "‡∏Å‡∏≥‡∏à‡∏±‡∏î", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏£‡∏±‡∏Å‡∏©‡∏≤"]):
                            # This is a question - use Smart Q&A with knowledge base
                            logger.info(f"Processing Q&A with knowledge: {text[:50]}...")
                            try:
                                answer = await answer_question_with_knowledge(text)
                                await add_to_memory(user_id, "user", text)
                                await add_to_memory(user_id, "assistant", answer)
                                await reply_line(reply_token, answer)
                            except Exception as e:
                                logger.error(f"Q&A error: {e}", exc_info=True)
                                # Fallback to natural conversation
                                await handle_natural_conversation(user_id, text, reply_token)
                        
                        else:
                            # Natural conversation for everything else
                            await handle_natural_conversation(user_id, text, reply_token)
                
                elif message_type == "sticker":
                    # Handle sticker messages
                    sticker_id = message.get("stickerId")
                    package_id = message.get("packageId")
                    logger.info(f"Received sticker: packageId={package_id}, stickerId={sticker_id}")
                    
                    # Reply with a friendly sticker response
                    sticker_response = (
                        "üòä ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞!\n\n"
                        "üå± ‡∏â‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞\n\n"
                        "üì∏ ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô\n"
                        "‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏∞\n\n"
                        "üí° ‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏ä‡πà‡∏ß‡∏¢' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
                    )
                    # Reply with text and sticker
                    await reply_line(reply_token, sticker_response, with_sticker=True)
                
                else:
                    # Handle other message types (video, audio, location, etc.)
                    logger.info(f"Received unsupported message type: {message_type}")
                    unsupported_message = (
                        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞:\n\n"
                        "üì∏ ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡∏û‡∏∑‡∏ä\n"
                        "üí¨ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
                        "üòä ‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢\n\n"
                        "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏∑‡∏ä‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏Ñ‡πà‡∏∞ üå±"
                    )
                    await reply_line(reply_token, unsupported_message)
        
        return JSONResponse(content={"status": "ok"})
    except Exception as e:
        logger.error(f"Webhook error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================#
# Startup
# ============================================================================#
@app.on_event("startup")
async def startup_event():
    logger.info("=" * 60)
    logger.info("Starting LINE Plant Pest & Disease Detection Bot")
    logger.info(f"Gemini API: {'‚úì' if GEMINI_API_KEY else '‚úó'}")
    logger.info(f"Supabase: {'‚úì' if supabase_client else '‚úó'}")
    logger.info(f"LINE Bot: {'‚úì' if LINE_CHANNEL_ACCESS_TOKEN else '‚úó'}")
    logger.info("Vision: Google Gemini 2.5 Flash")
    logger.info("RAG Method: Keyword Search (Fast & Reliable)")
    logger.info("=" * 60)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
